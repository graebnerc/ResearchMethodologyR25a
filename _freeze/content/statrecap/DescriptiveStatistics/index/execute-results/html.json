{
  "hash": "e16b730a0647d86b6ee31296b8c9d654",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"3: Descriptive Statistics\"\ndate: '2025-05-12'\n---\n\n# Descriptive Statistics - An Overview\n\nDescriptive statistics help us understand and summarize the key characteristics of our dataset. Think of descriptive statistics as a set of tools that allow us to paint a clear picture of what our data looks like without getting lost in the individual data points. Just as a painter might capture the essence of a landscape rather than documenting every single blade of grass, descriptive statistics capture the essential features of our data.\n\nTo this end we can use a variety of different measures, which are often put in one of the following categories:\n\n- Measures of centrality: tell us where the \"center\" of our data lies\n- Measures of dispersion: tell us how spread out our data is\n- Measures of association: tell us how different variables in our data relate to each other\n\nWe will now go through these categories one by one.\n\n# Measures of Centrality\n\nMeasures of central tendency tell us where the \"center\" of our data lies. Imagine you're trying to describe the typical height of students in your class - you're looking for a single number that best represents the whole group.\n\n**The Mean** is what most people think of as the average. You add up all values and divide by the number of observations. The mean is like the balancing point of your data - if you imagine your data points as weights on a seesaw, the mean is where you'd place the fulcrum to balance it perfectly.\n\n> The mean is particularly useful when your data is normally distributed (bell-shaped), but it can be heavily influenced by extreme values (outliers). For example, if most employees in a company earn around €45,000 annually, but the CEO earns €500,000, the mean salary will be pulled upward by this outlier.\n\n**The Median** is the middle value when your data is arranged in order. Think of it as the value that splits your data into two equal halves. Unlike the mean, the median is not affected by extreme values, making it more robust when dealing with skewed data or outliers.\n\n> If you have employee satisfaction scores of 3.2, 3.5, 3.8, 4.1, and 4.9 (on a 5-point scale), the median is 3.8. Even if that last score were 1.0 instead of 4.9, the median would still be 3.8.\n\n**The Mode** is the value that appears most frequently in your dataset. In some datasets, there might be no mode (if all values appear equally often) or multiple modes (if several values tie for most frequent).\n\n> In a dataset of customer purchase categories where \"electronics\" appears 150 times, \"clothing\" appears 89 times, and \"books\" appears 112 times, \"electronics\" is the mode.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computing measures of central tendency in R\n# Using business-relevant sample data: employee salaries in thousands of euros\nsalaries <- c(35, 38, 42, 45, 48, 52, 55, 58, 62, 120)\n\n# Mean - notice how the outlier (120k) affects it\nmean_salary <- mean(salaries)\n\n# Median - more robust to outliers\nmedian_salary <- median(salaries)\n\n# Mode (R doesn't have a built-in mode function, so you can create one yourself)\nget_mode <- function(x) {\n unique_x <- unique(x)\n unique_x[which.max(tabulate(match(x, unique_x)))]\n}\n\n# Use synthetic department data\ndepartments <- c(\n  \"Sales\", \"IT\", \"Sales\", \"Marketing\", \"IT\", \"Sales\", \"HR\", \"IT\", \"Sales\")\nmode_dept <- get_mode(departments) # Gives most common department\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using dplyr for grouped calculations \nlibrary(dplyr)\n\n# Example with employee data by department\nset.seed(123) # for reproducibility\nemployee_data <- data.frame(\n department = rep(c(\"Sales\", \"IT\", \"Marketing\"), each = 10),\n salary = c(\n   rnorm(10, mean = 45, sd = 5),  # Sales\n   rnorm(10, mean = 55, sd = 8),  # IT\n   rnorm(10, mean = 48, sd = 6)   # Marketing\n )\n)\n\n# Calculate measures by department\ndept_summary <- employee_data %>%\n group_by(department) %>%\n summarise(\n   mean_salary = round(mean(salary), 2),\n   median_salary = round(median(salary), 2),\n   .groups = 'drop'\n )\n```\n:::\n\n\n\n# Measures of Dispersion\nWhile measures of central tendency tell us where our data is centered, measures of dispersion tell us how spread out our data is. Two datasets can have the same mean but very different patterns of spread.\n\n**Range** is the simplest measure of dispersion - it's just the difference between the maximum and minimum values. While easy to calculate, the range only considers the two extreme values and ignores everything in between.\n\n> If customer satisfaction scores range from 2.1 to 4.8 on a 5-point scale, the range is 2.7 points. However, this doesn't tell us whether most scores are clustered around the mean or spread evenly throughout this range.\n\n**Variance** measures how much individual data points deviate from the mean, on average. Think of it as the average squared distance from the mean. We square the differences to ensure positive and negative deviations don't cancel each other out.\n\n> If a company's monthly sales figures are all very close to the average, the variance will be small, indicating consistent performance. If sales are widely scattered, the variance will be large, suggesting high volatility.\n\n**Standard Deviation** is simply the square root of the variance. The great advantage of standard deviation over variance is that it's expressed in the same units as your original data, making it more interpretable than variance.\n\n> If the mean monthly revenue is €100,000 and the standard deviation is €15,000, you can think of most months generating revenue within about €15,000 of the average (between €85,000 and €115,000).\n\nWhile standard deviation is an excellent measure of how spread out a single variable your data is, the fact that it is expressed in the same units as this data can become an important limitation comparing different business metrics. \n\nThe **Coefficient of Variation (CV)** solves this problem by standardizing the variability of a variable relative to its mean. Think of it as asking the question: \"How large is the standard deviation relative to the average value?\" The CV is calculated as the standard deviation divided by the mean, often expressed as a percentage.\n\n> Imagine you're a business analyst comparing the consistency of two different metrics: monthly sales revenue (measured in thousands of euros) and customer satisfaction scores (measured on a 1-5 scale). Sales might have a standard deviation of €15,000 with a mean of €100,000, while satisfaction scores might have a standard deviation of 0.3 with a mean of 3.8. Without the coefficient of variation, these numbers are difficult to compare directly.\n\nGenerally speaking, a CV below 10% suggests relatively low variability (high consistency), while a CV above 30% indicates high variability (less predictability). However, these thresholds can vary significantly depending on your industry and the specific metric being measured.\n\n\n[Consider including a figure showing two normal distributions with the same mean but different standard deviations to illustrate this concept visually]\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R code example for range, variance, and mean\"}\n# Creating artificial business data: monthly sales figures in euros\nmonthly_sales <- c(\n  85000, 92000, 78000, 105000, 88000, 94000, 110000, 87000, 96000, 150000)\n\nsales_range <- range(monthly_sales)\n\nsales_variance <- var(monthly_sales)\n\nsales_std <- sd(monthly_sales) # Standard deviation \n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R code example for coefficient of variation\"}\n# Dataset 1: Monthly sales in euros\nmonthly_sales <- c(\n  85000, 92000, 78000, 105000, 88000, 94000, 110000, 87000, 96000, 150000)\n\n# Dataset 2: Customer satisfaction scores (1-5 scale)\ncustomer_satisfaction <- c(3.8, 4.1, 3.9, 4.2, 3.7, 4.0, 4.3, 3.6, 4.1, 3.9)\n\n# Calculate coefficient of variation for monthly sales\nsales_mean <- mean(monthly_sales)\nsales_sd <- sd(monthly_sales)\nsales_cv <- (sales_sd / sales_mean) * 100  \n# CV ~ 21% -> moderate sales variability\n\n# Calculate coefficient of variation for customer satisfaction\nsatisfaction_mean <- mean(customer_satisfaction)\nsatisfaction_sd <- sd(customer_satisfaction)\nsatisfaction_cv <- (satisfaction_sd / satisfaction_mean) * 100  \n# CV ~ 5%, showing very consistent satisfaction\n\n# Key insight: while sales figures vary by thousands of euros, their \n#  coefficient of variation (around 21%) shows moderate business volatility. \n#  In contrast, customer satisfaction scores, though varying by just decimal \n#  points, have a very low coefficient of variation (around 5%), indicating \n#  remarkably consistent customer experience. \n# This comparison illustrates why coefficient of variation is so valuable in \n#  business analytics - it reveals that this company has achieved stable \n#  customer satisfaction despite fluctuating sales performance, suggesting \n#  strong service quality regardless of revenue variations.\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R code example for grouped computation\"}\nlibrary(dplyr)\n\n# Example with quarterly performance data\nquarterly_data <- data.frame(\n  quarter = rep(c(\"Q1\", \"Q2\", \"Q3\", \"Q4\"), each = 6),\n  revenue = c(\n    rnorm(6, mean = 180, sd = 20),  # Q1\n    rnorm(6, mean = 195, sd = 15),  # Q2\n    rnorm(6, mean = 210, sd = 25),  # Q3\n    rnorm(6, mean = 185, sd = 18)   # Q4\n  )\n)\n\nquarterly_summary <- quarterly_data %>%\n  group_by(quarter) %>%\n  summarise(\n    mean_revenue = round(mean(revenue), 2),\n    std_dev_revenue = round(sd(revenue), 2),\n    cv_percent = round((sd(revenue)/mean(revenue)) * 100, 1), \n    .groups = 'drop'\n  )\n```\n:::\n\n\n\n# Measures of association\nWhen we have two variables, we often want to understand whether they move together or independently. This is where correlation and covariance become essential tools in business analysis.\n\n**Covariance** measures whether two variables tend to move in the same direction. If both variables tend to be above their respective means together, or below their means together, the covariance will be positive. If one tends to be high when the other is low, covariance will be negative.\n\n> Consider the relationship between advertising spending and sales revenue. If companies that spend more on advertising tend to have higher sales, we'd expect a positive covariance between these variables.\n\nHowever, covariance has a significant limitation: its magnitude depends on the scale of measurement, making it difficult to interpret. This is where correlation comes in.\n\n**Correlation** is essentially standardized covariance, ranging from -1 to +1. A correlation of +1 indicates a perfect positive relationship, -1 indicates a perfect negative relationship, and 0 indicates no linear relationship.\n\n> A correlation of 0.75 between marketing budget and quarterly sales suggests a strong positive relationship - as marketing investment increases, sales tend to increase as well.\n\nNote, however, there there are different calculation methods for measures of\ncorrelation. The choice is dictated by the data type and the relationship you \nexpect. This are the two primary correlation coefficients used in business \nanalytics:\n\nThe **Pearson correlation** measures linear relationships and works best with normally distributed continuous data. It assesses how well your data points fit along a straight line. This is the standard \"correlation\" most people reference when analyzing financial metrics or other continuous business variables.\n\n> When advertising spending and sales revenue increase proportionally, Pearson correlation effectively captures this direct linear relationship.\n\n**Spearman correlation** measures monotonic relationships - whether variables consistently move in the same direction, regardless of whether that movement follows a straight line. This makes it ideal for ordinal data (like satisfaction ratings) and relationships that might be curved rather than linear.\n\n> If customer satisfaction increases with service quality but at a decreasing rate, Spearman correlation captures this curved relationship better than Pearson.\n\nIn practice, use Pearson for financial metrics that follow normal patterns and linear relationships. Choose Spearman for survey data, when you have outliers, or when you suspect non-linear relationships in your business processes. Spearman's rank-based approach makes it more robust to extreme values than Pearson's assumption of normal distribution.\n\n[Consider including a scatter plot showing different correlation strengths to help students visualize these relationships]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computing correlation and covariance in R\n# Creating realistic business sample data\nset.seed(123)  # For reproducible results\n\n# Relationship between advertising spend and sales\nadvertising_spend <- seq(10, 100, by = 5)  # In thousands of euros\nsales_revenue <- advertising_spend * 2.5 + rnorm(19, mean = 0, sd = 15) + 50\n\n# Covariance - note the units are hard to interpret\ncovariance_value <- cov(advertising_spend, sales_revenue)\n\n# Correlation - much easier to interpret\ncorrelation_value <- cor(advertising_spend, sales_revenue)\n\n# Different correlation methods for different data types\n# Pearson: for linear relationships with normally distributed data\n# Spearman: for monotonic relationships, robust to outliers\ncor_pearson <- cor(advertising_spend, sales_revenue, method = \"pearson\")\ncor_spearman <- cor(advertising_spend, sales_revenue, method = \"spearman\")\n```\n:::\n\n\n## Correlation Matrices\nWhen working with multiple variables simultaneously, correlation matrices become invaluable tools in business analysis. A correlation matrix shows the correlation between every pair of variables in your dataset. Reading a correlation matrix is like reading a multiplication table, but for relationships between business metrics.\n\nThe diagonal of a correlation matrix always contains 1s because every variable is perfectly correlated with itself. The matrix is symmetric because the correlation between X and Y is the same as the correlation between Y and X.\n\nIn a correlation matrix examining business performance metrics, you might find that \"customer satisfaction\" correlates strongly with \"repeat purchase rate\" (0.82) but weakly with \"marketing spend\" (0.15), while \"employee satisfaction\" might have a moderate positive correlation with \"customer satisfaction\" (0.58).\n\nWhen interpreting correlation matrices in business contexts, look for:\n\n- Strong positive correlations (close to +1) that suggest complementary metrics\n- Strong negative correlations (close to -1) that might indicate trade-offs\n- Weak correlations (close to 0) suggesting independent factors\n\n[Consider including a visual representation of a correlation matrix with color coding to show the strength of relationships]\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Create artificial data 'business_metrics'\"}\nlibrary(dplyr)\n# Creating a realistic business dataset\nset.seed(123)\nbusiness_metrics <- data.frame(\n  advertising_spend = rnorm(100, mean = 50, sd = 15),  # thousands of euros\n  customer_satisfaction = rnorm(100, mean = 3.8, sd = 0.5),  # 1-5 scale\n  employee_satisfaction = rnorm(100, mean = 3.5, sd = 0.6),  # 1-5 scale\n  training_hours = rnorm(100, mean = 25, sd = 8)  # hours per quarter\n)\n\n# Create interdependent business metrics\nbusiness_metrics <- business_metrics %>%\n  mutate(\n    # Sales influenced by advertising and customer satisfaction\n    sales_revenue = 100 + \n      1.2 * advertising_spend + \n      30 * customer_satisfaction + \n      rnorm(100, 0, 20),\n    \n    # Customer satisfaction influenced by employee satisfaction\n    customer_satisfaction = customer_satisfaction + \n      0.3 * employee_satisfaction + \n      rnorm(100, 0, 0.2),\n    \n    # Employee satisfaction influenced by training\n    employee_satisfaction = employee_satisfaction + \n      0.02 * training_hours + \n      rnorm(100, 0, 0.15),\n    \n    # Profit margin influenced by efficiency (inverse of spending per revenue)\n    profit_margin = 15 + \n      sales_revenue * 0.1 - \n      advertising_spend * 0.5 + \n      customer_satisfaction * 2 + \n      rnorm(100, 0, 5)\n  )\n\n# Clean up the correlations by ensuring realistic ranges\nbusiness_metrics$customer_satisfaction <- pmax(\n  1, pmin(5, business_metrics$customer_satisfaction))\nbusiness_metrics$employee_satisfaction <- pmax(\n  1, pmin(5, business_metrics$employee_satisfaction))\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Create correlation matrix in R\"}\ncorrelation_matrix <- cor(business_metrics)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Visualize correlation matrix\"}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Assuming your correlation matrix is stored in 'correlation_matrix'\n# First, let's understand what we're working with\ncorrelation_visualization <- correlation_matrix %>%\n  # Convert the matrix to a data frame if it isn't already\n  as.data.frame() %>%\n  # Add row names as a column (these are our first variable names)\n  mutate(var1 = rownames(.)) %>%\n  # Reshape from wide to long format\n  pivot_longer(cols = -var1,           \n               names_to = \"var2\", \n               values_to = \"correlation\") %>%\n  # Create a more readable format for variable names\n  mutate(\n    var1_clean = case_when(\n      var1 == \"advertising_spend\" ~ \"Advertising Spend\",\n      var1 == \"customer_satisfaction\" ~ \"Customer Satisfaction\", \n      var1 == \"employee_satisfaction\" ~ \"Employee Satisfaction\",\n      var1 == \"training_hours\" ~ \"Training Hours\",\n      var1 == \"sales_revenue\" ~ \"Sales Revenue\",\n      var1 == \"profit_margin\" ~ \"Profit Margin\",\n      TRUE ~ var1\n    ),\n    var2_clean = case_when(\n      var2 == \"advertising_spend\" ~ \"Advertising Spend\",\n      var2 == \"customer_satisfaction\" ~ \"Customer Satisfaction\",\n      var2 == \"employee_satisfaction\" ~ \"Employee Satisfaction\", \n      var2 == \"training_hours\" ~ \"Training Hours\",\n      var2 == \"sales_revenue\" ~ \"Sales Revenue\",\n      var2 == \"profit_margin\" ~ \"Profit Margin\",\n      TRUE ~ var2\n    )\n  )\n\n# Create the visualization\ncorrelation_heatmap <- correlation_visualization %>%\n  ggplot(aes(x = var2_clean, y = var1_clean, fill = correlation)) +\n  geom_tile(color = \"white\", linewidth = 0.3) +\n  geom_text(aes(label = round(correlation, 2)), \n            color = \"black\", size = 3) +\n  scale_fill_gradient2(low = \"darkred\", mid = \"white\", high = \"darkblue\",\n                       midpoint = 0, limit = c(-1, 1),\n                       name = \"Correlation\\nCoefficient\") +\n  # Rotate x-axis labels for better readability\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.title = element_blank(),\n        panel.grid = element_blank()) +\n  labs(title = \"Business Metrics Correlation Matrix\")\n```\n:::\n\n\n\n# A Note on Visualizing Data\n\nADD THIS FIGURE EXAMPLE\n\nThe power of descriptive statistics is greatly enhanced when combined with appropriate visualizations. Charts and graphs can reveal patterns that numbers alone might hide, making them essential tools for understanding your business data.\nA famous example is \"Anscombe’s quartet\", a compilation of four data sets that have nearly identical  descriptive statistics but are in fact very different, something that is easy to show visually:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R code for the visualization\"}\n# Load required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggpubr)  # For ggarrange function\n\n# Step 1: Transform the Anscombe data into a more workable format\n# The original data has separate columns for each dataset, we need to reshape it\nanscombe_long <- anscombe %>%\n  # Add row numbers to track observations\n  mutate(observation = row_number()) %>%\n  # Reshape to long format - this is where the magic happens\n  pivot_longer(cols = -observation,\n               names_to = \"variable\", \n               values_to = \"value\") %>%\n  # Extract dataset number and variable type from column names\n  mutate(\n    dataset = str_extract(variable, \"\\\\d+\"),  # Extract number (1,2,3,4)\n    axis = str_extract(variable, \"[xy]\"),     # Extract x or y\n    dataset = paste(\"Dataset\", dataset)        # Make it more readable\n  ) %>%\n  select(-variable) |> \n  # Reshape wider to have x and y as separate columns\n  pivot_wider(names_from = axis, values_from = value)\n\n# Step 2: Create individual scatter plots for each dataset\n# We'll create a function to ensure consistency across all plots\ncreate_anscombe_plot <- function(data_subset, title) {\n  # Calculate correlation for the subtitle\n  correlation <- cor(data_subset$x, data_subset$y)\n  \n  # Create the scatter plot with regression line\n  ggplot(data_subset, aes(x = x, y = y)) +\n    geom_point(size = 3, alpha = 0.7, color = \"steelblue\") +\n    # Add regression line with confidence interval\n    geom_smooth(method = \"lm\", se = FALSE, color = \"coral2\", alpha = 0.2) +\n    # Ensure consistent scales across all plots for fair comparison\n    scale_x_continuous(limits = c(3, 20), breaks = seq(4, 18, 2)) +\n    scale_y_continuous(limits = c(3, 13), breaks = seq(4, 12, 2)) +\n    labs(\n      title = title,\n      subtitle = paste(\"r =\", round(correlation, 3)),\n      x = \"X Values\",\n      y = \"Y Values\"\n    ) +\n    # Clean minimal theme\n    theme_minimal() +\n    theme(\n      plot.title = element_text(size = 12, face = \"bold\"),\n      plot.subtitle = element_text(size = 10),\n      axis.title = element_text(size = 10)\n    )\n}\n\n# Step 3: Create individual plots for each dataset\nplot1 <- anscombe_long %>%\n  filter(dataset == \"Dataset 1\") %>%\n  create_anscombe_plot(\"Dataset I: Linear Relationship\")\n\nplot2 <- anscombe_long %>%\n  filter(dataset == \"Dataset 2\") %>%\n  create_anscombe_plot(\"Dataset II: Curved Relationship\")\n\nplot3 <- anscombe_long %>%\n  filter(dataset == \"Dataset 3\") %>%\n  create_anscombe_plot(\"Dataset III: Linear with Outlier\")\n\nplot4 <- anscombe_long %>%\n  filter(dataset == \"Dataset 4\") %>%\n  create_anscombe_plot(\"Dataset IV: Vertical Outlier\")\n\n# Step 4: Arrange all plots in a 2x2 grid\nfinal_plot <- ggarrange(plot1, plot2, plot3, plot4,\n                       ncol = 2, nrow = 2,\n                       common.legend = FALSE)\n\n# Add an overall title to tie everything together\nfinal_plot <- annotate_figure(final_plot,\n                             top = text_grob(\"Anscombe's Quartet: Why Visualization Matters\",\n                                           face = \"bold\", size = 16))\n\n# Display the final visualization\nprint(final_plot)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nAnd here are the summary statistics:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R code \"}\nlibrary(knitr)\nsummary_stats <- anscombe_long %>%\n  group_by(dataset) %>%\n  summarise(\n    mean_x = round(mean(x), 2),\n    mean_y = round(mean(y), 2),\n    sd_x = round(sd(x), 2),\n    sd_y = round(sd(y), 2),\n    correlation = round(cor(x, y), 3),\n    .groups = 'drop'\n  )\nkable(summary_stats)\n```\n\n::: {.cell-output-display}\n\n\n|dataset   | mean_x| mean_y| sd_x| sd_y| correlation|\n|:---------|------:|------:|----:|----:|-----------:|\n|Dataset 1 |      9|    7.5| 3.32| 2.03|       0.816|\n|Dataset 2 |      9|    7.5| 3.32| 2.03|       0.816|\n|Dataset 3 |      9|    7.5| 3.32| 2.03|       0.816|\n|Dataset 4 |      9|    7.5| 3.32| 2.03|       0.817|\n\n\n:::\n:::\n\n\n\nFor detailed guidance on creating effective data visualizations using `R` and `ggplot2`, please refer to the comprehensive tutorial available on our course homepage. \n\nRemember that the goal of descriptive statistics is not just to calculate numbers, but to gain genuine insight into your data. Each measure tells part of the story - the mean tells you where your data centers, the standard deviation tells you how spread out it is, and correlation tells you how variables relate to each other. Together, these tools provide a comprehensive picture that guides further analysis and decision-making.\n\nAs you practice using these concepts with business data, remember that no single statistic tells the complete story. Always consider multiple measures together, visualize your data when possible, and think critically about what the numbers are revealing about the underlying business phenomena you're studying. For instance, when analyzing customer satisfaction scores, look at both the average satisfaction and the variability - consistent high satisfaction is very different from highly variable satisfaction that averages to the same number.\n\nThink of descriptive statistics as the foundation of all business analytics. Just as you wouldn't build a house without first understanding the characteristics of your building materials, you shouldn't make business decisions without first understanding the basic characteristics of your data through these fundamental statistical measures.\n\n\n# Next steps\n- [Back to the recap overview](content/statrecap.qmd)\n- [Next topic: Essentials in Probability Theory](content/statrecap/ProbabilityTheory.qmd)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}