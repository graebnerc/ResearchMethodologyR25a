{
  "hash": "6389f107e33d42890a86470f49a1cf0f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"3: Descriptive Statistics\"\ndate: '2025-05-12'\n---\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Packages used in R examples\"}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(knitr)\n```\n:::\n\n\n\n# Descriptive Statistics - An Overview\n\nDescriptive statistics help us understand and summarize the key characteristics of our dataset. Think of descriptive statistics as a set of tools that allow us to paint a clear picture of what our data looks like without getting lost in the individual data points. Just as a painter might capture the essence of a landscape rather than documenting every single blade of grass, descriptive statistics capture the essential features of our data.\n\nTo this end we can use a variety of different measures, which are often put in one of the following categories:\n\n- Measures of centrality: tell us where the \"center\" of our data lies\n- Measures of dispersion: tell us how spread out our data is\n- Measures of association: tell us how different variables in our data relate to each other\n\nWe will now go through these categories one by one.\n\n# Measures of Centrality\n\nMeasures of central tendency tell us where the \"center\" of our data lies. Imagine you're trying to describe the typical height of students in your class - you're looking for a single number that best represents the whole group. The left panel in @fig-stats visualizes the following examples.\n\n**The Mean** is what most people think of as the average. You add up all values and divide by the number of observations. The mean is like the balancing point of your data - if you imagine your data points as weights on a seesaw, the mean is where you'd place the fulcrum to balance it perfectly.\n\n> The mean is particularly useful when your data is normally distributed (bell-shaped), but it can be heavily influenced by extreme values (outliers). For example, if most employees in a company earn around €45,000 annually, but the CEO earns €500,000, the mean salary will be pulled upward by this outlier.\n\n**The Median** is the middle value when your data is arranged in order. Think of it as the value that splits your data into two equal halves. Unlike the mean, the median is not affected by extreme values, making it more robust when dealing with skewed data or outliers.\n\n> If you have employee satisfaction scores of 3.2, 3.5, 3.8, 4.1, and 4.9 (on a 5-point scale), the median is 3.8. Even if that last score were 1.0 instead of 4.9, the median would still be 3.8.\n\n**The Mode** is the value that appears most frequently in your dataset. In some datasets, there might be no mode (if all values appear equally often) or multiple modes (if several values tie for most frequent).\n\n> In a dataset of customer purchase categories where \"electronics\" appears 150 times, \"clothing\" appears 89 times, and \"books\" appears 112 times, \"electronics\" is the mode.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computing measures of central tendency in R\n# Using business-relevant sample data: employee salaries in thousands of euros\nsalaries <- c(35, 38, 42, 45, 48, 52, 55, 58, 62, 120)\n\n# Mean - notice how the outlier (120k) affects it\nmean_salary <- mean(salaries)\n\n# Median - more robust to outliers\nmedian_salary <- median(salaries)\n\n# Mode (R doesn't have a built-in mode function, so you can create one yourself)\nget_mode <- function(x) {\n unique_x <- unique(x)\n unique_x[which.max(tabulate(match(x, unique_x)))]\n}\n\n# Use synthetic department data\ndepartments <- c(\n  \"Sales\", \"IT\", \"Sales\", \"Marketing\", \"IT\", \"Sales\", \"HR\", \"IT\", \"Sales\")\nmode_dept <- get_mode(departments) # Gives most common department\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using dplyr for grouped calculations \nlibrary(dplyr)\n\n# Example with employee data by department\nset.seed(123) # for reproducibility\nemployee_data <- data.frame(\n department = rep(c(\"Sales\", \"IT\", \"Marketing\"), each = 10),\n salary = c(\n   rnorm(10, mean = 45, sd = 5),  # Sales\n   rnorm(10, mean = 55, sd = 8),  # IT\n   rnorm(10, mean = 48, sd = 6)   # Marketing\n )\n)\n\n# Calculate measures by department\ndept_summary <- employee_data %>%\n group_by(department) %>%\n summarise(\n   mean_salary = round(mean(salary), 2),\n   median_salary = round(median(salary), 2),\n   .groups = 'drop'\n )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a slightly right-skewed dataset (more realistic for business data like salaries)\nset.seed(123)\n# Create a mixture of two normal distributions for a slightly skewed result\ndata1 <- rnorm(800, mean = 50, sd = 10)\ndata2 <- rnorm(200, mean = 75, sd = 15)\nvalues <- c(data1, data2)\n\n# Create a data frame\ndf <- data.frame(values = values)\n\n# Calculate key statistics\ndata_mean <- mean(values)\ndata_median <- median(values)\ndata_sd <- sd(values)\n\n# Find the mode (bin with highest frequency)\nhist_data <- hist(values, plot = FALSE, breaks = 30)\nmode_bin <- hist_data$mids[which.max(hist_data$counts)]\n\n# Create ranges for standard deviation visualization\nlower_sd <- data_mean - data_sd\nupper_sd <- data_mean + data_sd\n\n# Create common plot elements to ensure consistency\ncommon_theme <- theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    axis.title = element_text(size = 10)\n  )\n\n# PLOT 1: Central Tendency (Mean, Median, Mode)\nnote_distance <- 15\ncentral_tendency_plot <- ggplot(df, aes(x = values)) +\n  # Create histogram\n  geom_histogram(binwidth = 2, fill = \"lightblue\", color = \"white\", alpha = 0.7) +\n  \n  # Add vertical lines for mean, median and mode\n  geom_vline(xintercept = data_mean, color = \"red\", linewidth = 1.0, linetype = \"solid\") +\n  geom_vline(xintercept = data_median, color = \"darkgreen\", linewidth = 1.0, linetype = \"dashed\") +\n  geom_vline(xintercept = mode_bin, color = \"purple\", linewidth = 1.0, linetype = \"dotdash\") +\n  \n  # Add annotations\n  annotate(\"text\", x = data_mean + note_distance, y = 80, label = \"Mean\", \n           color = \"red\", fontface = \"bold\", hjust = 0) +\n  annotate(\"text\", x = data_median - note_distance, y = 65, label = \"Median\", \n           color = \"darkgreen\", fontface = \"bold\", hjust = 1) +\n  annotate(\"text\", x = mode_bin + note_distance, y = 50, label = \"Mode\", \n           color = \"purple\", fontface = \"bold\", hjust = 0) +\n  \n  # Add arrows to point at key features\n  annotate(\"segment\", x = data_mean + note_distance, xend = data_mean, y = 80, yend = 80, \n           arrow = arrow(length = unit(0.3, \"cm\")), color = \"red\") +\n  annotate(\"segment\", x = data_median - note_distance, xend = data_median, y = 65, yend = 65, \n           arrow = arrow(length = unit(0.3, \"cm\")), color = \"darkgreen\") +\n  annotate(\"segment\", x = mode_bin + note_distance, xend = mode_bin, y = 50, yend = 50, \n           arrow = arrow(length = unit(0.3, \"cm\")), color = \"purple\") +\n  \n  # Labels\n  labs(\n    title = \"Measures of Central Tendency\",\n    subtitle = \"Employee Salaries (thousands €)\",\n    x = \"Salary (thousands €)\",\n    y = \"Frequency (Number of Employees)\",\n    caption = \"Mean: Average (sum of all values ÷ count)\\nMedian: Middle value (50th percentile)\\nMode: Most common value (highest peak)\"\n  ) +\n  common_theme +\n  theme(plot.caption = element_text(hjust = 0, size = 9))\n\n# PLOT 2: Standard Deviation\ndispersion_plot <- ggplot(df, aes(x = values)) +\n  # Create histogram\n  geom_histogram(binwidth = 2, fill = \"lightblue\", color = \"white\", alpha = 0.7) +\n  \n  # Add vertical line for mean\n  geom_vline(xintercept = data_mean, color = \"red\", linewidth = 1.0, linetype = \"solid\") +\n  \n  # Add shaded area for standard deviation\n  annotate(\"rect\", xmin = lower_sd, xmax = upper_sd, \n           ymin = 0, ymax = Inf, alpha = 0.2, fill = \"orange\") +\n  \n  # Add annotations for standard deviation\n  annotate(\"text\", x = data_mean, y = 80, label = paste(\"Mean =\", round(data_mean, 1)), \n           color = \"red\", fontface = \"bold\") +\n  annotate(\"text\", x = data_mean, y = 65, \n           label = paste(\"Standard Deviation =\", round(data_sd, 1)), \n           color = \"darkorange\", fontface = \"bold\") +\n  \n  # Add brackets to show standard deviation range\n  annotate(\"segment\", x = lower_sd, xend = upper_sd, y = 45, yend = 45, \n           linewidth = 1.0, color = \"darkorange\") +\n  annotate(\"segment\", x = lower_sd, xend = lower_sd, y = 42, yend = 48, \n           linewidth = 1.0, color = \"darkorange\") +\n  annotate(\"segment\", x = upper_sd, xend = upper_sd, y = 42, yend = 48, \n           linewidth = 1.0, color = \"darkorange\") +\n  \n  annotate(\"text\", x = data_mean, y = 35, \n           label = \"68% of all observations\\nfall within ±1 SD of the mean\", \n           color = \"darkorange\", fontface = \"bold\") +\n  \n  # Labels\n  labs(\n    title = \"Measure of Spread\",\n    subtitle = \"Employee Salaries (thousands €)\",\n    x = \"Salary (thousands €)\",\n    y = \"Frequency (Number of Employees)\",\n    caption = \"Standard Deviation shows the typical distance\\nfrom the average. Smaller SD = less variability.\"\n  ) +\n  common_theme +\n  theme(plot.caption = element_text(hjust = 0, size = 9))\n\n# Combine plots with ggarrange\ncombined_plot <- ggarrange(\n  central_tendency_plot, dispersion_plot,\n  ncol = 2, \n  labels = c(\"A\", \"B\")\n)\n\n# Add an overall title\nfinal_plot <- annotate_figure(combined_plot,\n                             top = text_grob(\"Key Descriptive Statistics\", \n                                           face = \"bold\", size = 16))\n\n# Display the final plot\nprint(final_plot)\n```\n\n::: {.cell-output-display}\n![An illustration of typical measures of centrality and spread.](index_files/figure-html/fig-stats-1.png){#fig-stats width=672}\n:::\n:::\n\n\n\n# Measures of Dispersion\nWhile measures of central tendency tell us where our data is centered, measures of dispersion tell us how spread out our data is (see the right panel in @fig-stats). Two datasets can have the same mean but very different patterns of spread.\n\n**Range** is the simplest measure of dispersion - it's just the difference between the maximum and minimum values. While easy to calculate, the range only considers the two extreme values and ignores everything in between.\n\n> If customer satisfaction scores range from 2.1 to 4.8 on a 5-point scale, the range is 2.7 points. However, this doesn't tell us whether most scores are clustered around the mean or spread evenly throughout this range.\n\n**Variance** measures how much individual data points deviate from the mean, on average. Think of it as the average squared distance from the mean. We square the differences to ensure positive and negative deviations don't cancel each other out.\n\n> If a company's monthly sales figures are all very close to the average, the variance will be small, indicating consistent performance. If sales are widely scattered, the variance will be large, suggesting high volatility.\n\n**Standard Deviation** is simply the square root of the variance. The great advantage of standard deviation over variance is that it's expressed in the same units as your original data, making it more interpretable than variance.\n\n> If the mean monthly revenue is €100,000 and the standard deviation is €15,000, you can think of most months generating revenue within about €15,000 of the average (between €85,000 and €115,000).\n\nWhile standard deviation is an excellent measure of how spread out a single variable your data is, the fact that it is expressed in the same units as this data can become an important limitation comparing different business metrics. \n\nThe **Coefficient of Variation (CV)** solves this problem by standardizing the variability of a variable relative to its mean. Think of it as asking the question: \"How large is the standard deviation relative to the average value?\" The CV is calculated as the standard deviation divided by the mean, often expressed as a percentage.\n\n> Imagine you're a business analyst comparing the consistency of two different metrics: monthly sales revenue (measured in thousands of euros) and customer satisfaction scores (measured on a 1-5 scale). Sales might have a standard deviation of €15,000 with a mean of €100,000, while satisfaction scores might have a standard deviation of 0.3 with a mean of 3.8. Without the coefficient of variation, these numbers are difficult to compare directly.\n\nGenerally speaking, a CV below 10% suggests relatively low variability (high consistency), while a CV above 30% indicates high variability (less predictability). However, these thresholds can vary significantly depending on your industry and the specific metric being measured.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R code example for range, variance, and mean\"}\n# Creating artificial business data: monthly sales figures in euros\nmonthly_sales <- c(\n  85000, 92000, 78000, 105000, 88000, 94000, 110000, 87000, 96000, 150000)\n\nsales_range <- range(monthly_sales)\n\nsales_variance <- var(monthly_sales)\n\nsales_std <- sd(monthly_sales) # Standard deviation \n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R code example for coefficient of variation\"}\n# Dataset 1: Monthly sales in euros\nmonthly_sales <- c(\n  85000, 92000, 78000, 105000, 88000, 94000, 110000, 87000, 96000, 150000)\n\n# Dataset 2: Customer satisfaction scores (1-5 scale)\ncustomer_satisfaction <- c(3.8, 4.1, 3.9, 4.2, 3.7, 4.0, 4.3, 3.6, 4.1, 3.9)\n\n# Calculate coefficient of variation for monthly sales\nsales_mean <- mean(monthly_sales)\nsales_sd <- sd(monthly_sales)\nsales_cv <- (sales_sd / sales_mean) * 100  \n# CV ~ 21% -> moderate sales variability\n\n# Calculate coefficient of variation for customer satisfaction\nsatisfaction_mean <- mean(customer_satisfaction)\nsatisfaction_sd <- sd(customer_satisfaction)\nsatisfaction_cv <- (satisfaction_sd / satisfaction_mean) * 100  \n# CV ~ 5%, showing very consistent satisfaction\n\n# Key insight: while sales figures vary by thousands of euros, their \n#  coefficient of variation (around 21%) shows moderate business volatility. \n#  In contrast, customer satisfaction scores, though varying by just decimal \n#  points, have a very low coefficient of variation (around 5%), indicating \n#  remarkably consistent customer experience. \n# This comparison illustrates why coefficient of variation is so valuable in \n#  business analytics - it reveals that this company has achieved stable \n#  customer satisfaction despite fluctuating sales performance, suggesting \n#  strong service quality regardless of revenue variations.\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R code example for grouped computation\"}\nlibrary(dplyr)\n\n# Example with quarterly performance data\nquarterly_data <- data.frame(\n  quarter = rep(c(\"Q1\", \"Q2\", \"Q3\", \"Q4\"), each = 6),\n  revenue = c(\n    rnorm(6, mean = 180, sd = 20),  # Q1\n    rnorm(6, mean = 195, sd = 15),  # Q2\n    rnorm(6, mean = 210, sd = 25),  # Q3\n    rnorm(6, mean = 185, sd = 18)   # Q4\n  )\n)\n\nquarterly_summary <- quarterly_data %>%\n  group_by(quarter) %>%\n  summarise(\n    mean_revenue = round(mean(revenue), 2),\n    std_dev_revenue = round(sd(revenue), 2),\n    cv_percent = round((sd(revenue)/mean(revenue)) * 100, 1), \n    .groups = 'drop'\n  )\n```\n:::\n\n\n\n# Measures of association\nWhen we have two variables, we often want to understand whether they move together or independently. This is where correlation and covariance become essential tools in business analysis.\n\n**Covariance** measures whether two variables tend to move in the same direction. If both variables tend to be above their respective means together, or below their means together, the covariance will be positive. If one tends to be high when the other is low, covariance will be negative.\n\n> Consider the relationship between advertising spending and sales revenue. If companies that spend more on advertising tend to have higher sales, we'd expect a positive covariance between these variables.\n\nHowever, covariance has a significant limitation: its magnitude depends on the scale of measurement, making it difficult to interpret. This is where correlation comes in.\n\n**Correlation** is essentially standardized covariance, ranging from -1 to +1. A correlation of +1 indicates a perfect positive relationship, -1 indicates a perfect negative relationship, and 0 indicates no linear relationship.\n\n> A correlation of 0.75 between marketing budget and quarterly sales suggests a strong positive relationship - as marketing investment increases, sales tend to increase as well.\n\nNote, however, there there are different calculation methods for measures of\ncorrelation. The choice is dictated by the data type and the relationship you \nexpect. This are the two primary correlation coefficients used in business \nanalytics:\n\nThe **Pearson correlation** measures linear relationships and works best with normally distributed continuous data. It assesses how well your data points fit along a straight line. This is the standard \"correlation\" most people reference when analyzing financial metrics or other continuous business variables.\n\n> When advertising spending and sales revenue increase proportionally, Pearson correlation effectively captures this direct linear relationship.\n\n**Spearman correlation** measures monotonic relationships - whether variables consistently move in the same direction, regardless of whether that movement follows a straight line. This makes it ideal for ordinal data (like satisfaction ratings) and relationships that might be curved rather than linear.\n\n> If customer satisfaction increases with service quality but at a decreasing rate, Spearman correlation captures this curved relationship better than Pearson.\n\nIn practice, use Pearson for financial metrics that follow normal patterns and linear relationships. Choose Spearman for survey data, when you have outliers, or when you suspect non-linear relationships in your business processes. Spearman's rank-based approach makes it more robust to extreme values than Pearson's assumption of normal distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computing correlation and covariance in R\n# Creating realistic business sample data\nset.seed(123)  # For reproducible results\n\n# Relationship between advertising spend and sales\nadvertising_spend <- seq(10, 100, by = 5)  # In thousands of euros\nsales_revenue <- advertising_spend * 2.5 + rnorm(19, mean = 0, sd = 15) + 50\n\n# Covariance - note the units are hard to interpret\ncovariance_value <- cov(advertising_spend, sales_revenue)\n\n# Correlation - much easier to interpret\ncorrelation_value <- cor(advertising_spend, sales_revenue)\n\n# Different correlation methods for different data types\n# Pearson: for linear relationships with normally distributed data\n# Spearman: for monotonic relationships, robust to outliers\ncor_pearson <- cor(advertising_spend, sales_revenue, method = \"pearson\")\ncor_spearman <- cor(advertising_spend, sales_revenue, method = \"spearman\")\n```\n:::\n\n\n## Correlation Matrices\nWhen working with multiple variables simultaneously, correlation matrices become invaluable tools in business analysis. A correlation matrix shows the correlation between every pair of variables in your dataset. Reading a correlation matrix is like reading a multiplication table, but for relationships between business metrics.\n\nThe diagonal of a correlation matrix always contains 1s because every variable is perfectly correlated with itself. The matrix is symmetric because the correlation between X and Y is the same as the correlation between Y and X.\n\nIn a correlation matrix examining business performance metrics, you might find that \"customer satisfaction\" correlates strongly with \"repeat purchase rate\" (0.82) but weakly with \"marketing spend\" (0.15), while \"employee satisfaction\" might have a moderate positive correlation with \"customer satisfaction\" (0.58).\n\nWhen interpreting correlation matrices in business contexts, look for:\n\n- Strong positive correlations (close to +1) that suggest complementary metrics\n- Strong negative correlations (close to -1) that might indicate trade-offs\n- Weak correlations (close to 0) suggesting independent factors\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Create artificial data 'business_metrics'\"}\nlibrary(dplyr)\n# Creating a realistic business dataset\nset.seed(123)\nbusiness_metrics <- data.frame(\n  advertising_spend = rnorm(100, mean = 50, sd = 15),  # thousands of euros\n  customer_satisfaction = rnorm(100, mean = 3.8, sd = 0.5),  # 1-5 scale\n  employee_satisfaction = rnorm(100, mean = 3.5, sd = 0.6),  # 1-5 scale\n  training_hours = rnorm(100, mean = 25, sd = 8)  # hours per quarter\n)\n\n# Create interdependent business metrics\nbusiness_metrics <- business_metrics %>%\n  mutate(\n    # Sales influenced by advertising and customer satisfaction\n    sales_revenue = 100 + \n      1.2 * advertising_spend + \n      30 * customer_satisfaction + \n      rnorm(100, 0, 20),\n    \n    # Customer satisfaction influenced by employee satisfaction\n    customer_satisfaction = customer_satisfaction + \n      0.3 * employee_satisfaction + \n      rnorm(100, 0, 0.2),\n    \n    # Employee satisfaction influenced by training\n    employee_satisfaction = employee_satisfaction + \n      0.02 * training_hours + \n      rnorm(100, 0, 0.15),\n    \n    # Profit margin influenced by efficiency (inverse of spending per revenue)\n    profit_margin = 15 + \n      sales_revenue * 0.1 - \n      advertising_spend * 0.5 + \n      customer_satisfaction * 2 + \n      rnorm(100, 0, 5)\n  )\n\n# Clean up the correlations by ensuring realistic ranges\nbusiness_metrics$customer_satisfaction <- pmax(\n  1, pmin(5, business_metrics$customer_satisfaction))\nbusiness_metrics$employee_satisfaction <- pmax(\n  1, pmin(5, business_metrics$employee_satisfaction))\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Create correlation matrix in R\"}\ncorrelation_matrix <- cor(business_metrics)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Visualize correlation matrix\"}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Assuming your correlation matrix is stored in 'correlation_matrix'\n# First, let's understand what we're working with\ncorrelation_visualization <- correlation_matrix %>%\n  # Convert the matrix to a data frame if it isn't already\n  as.data.frame() %>%\n  # Add row names as a column (these are our first variable names)\n  mutate(var1 = rownames(.)) %>%\n  # Reshape from wide to long format\n  pivot_longer(cols = -var1,           \n               names_to = \"var2\", \n               values_to = \"correlation\") %>%\n  # Create a more readable format for variable names\n  mutate(\n    var1_clean = case_when(\n      var1 == \"advertising_spend\" ~ \"Advertising Spend\",\n      var1 == \"customer_satisfaction\" ~ \"Customer Satisfaction\", \n      var1 == \"employee_satisfaction\" ~ \"Employee Satisfaction\",\n      var1 == \"training_hours\" ~ \"Training Hours\",\n      var1 == \"sales_revenue\" ~ \"Sales Revenue\",\n      var1 == \"profit_margin\" ~ \"Profit Margin\",\n      TRUE ~ var1\n    ),\n    var2_clean = case_when(\n      var2 == \"advertising_spend\" ~ \"Advertising Spend\",\n      var2 == \"customer_satisfaction\" ~ \"Customer Satisfaction\",\n      var2 == \"employee_satisfaction\" ~ \"Employee Satisfaction\", \n      var2 == \"training_hours\" ~ \"Training Hours\",\n      var2 == \"sales_revenue\" ~ \"Sales Revenue\",\n      var2 == \"profit_margin\" ~ \"Profit Margin\",\n      TRUE ~ var2\n    )\n  )\n\n# Create the visualization\ncorrelation_heatmap <- correlation_visualization %>%\n  ggplot(aes(x = var2_clean, y = var1_clean, fill = correlation)) +\n  geom_tile(color = \"white\", linewidth = 0.3) +\n  geom_text(aes(label = round(correlation, 2)), \n            color = \"black\", size = 3) +\n  scale_fill_gradient2(low = \"darkred\", mid = \"white\", high = \"darkblue\",\n                       midpoint = 0, limit = c(-1, 1),\n                       name = \"Correlation\\nCoefficient\") +\n  # Rotate x-axis labels for better readability\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.title = element_blank(),\n        panel.grid = element_blank()) +\n  labs(title = \"Business Metrics Correlation Matrix\")\n```\n:::\n\n\n## Regression\n\nWhile often associated with predictive modeling, regression also serves as a powerful descriptive tool that goes beyond correlation. Think of regression as drawing the \"best-fit line\" through scattered data points, providing a mathematical equation that summarizes the relationship between (two or more) variables. \nThe most common form, linear regression, is typically written using standardized notation: \n\n$$y=\\beta_0 + \\beta_1 \\boldsymbol{x}_1$$\n\nwhere $\\beta_0$ represents the y-intercept and $\\beta_1$ represents the slope coefficient. For multiple regression with several predictors, the formula extends to \n\n$$y=\\beta_0 + \\beta_1  \\boldsymbol{x}_1 + \\beta_2  \\boldsymbol{x}_2 ... + \\beta_k \\boldsymbol{x}_k $$\n\nwith each $\\beta_1$ coefficient describing how $\\boldsymbol{y}$ changes when the corresponding predictor variable changes by one unit, holding all other variables constant. Unlike correlation, which only indicates strength and direction, regression coefficients provide interpretable measures of how variables actually relate to each other in your data. \n\nFor instance, when $y$ is total sales in 1000 EUR, $\\boldsymbol{x}_2$ the total distance of your shop to the city centre in km, and $\\hat{\\beta}_2=-0.5$ this means that, according to your model, if your shop was moving away from the city centre by 1 km, this is associated, on average, by reducing sales by 500 EUR.\n\nWhen examining relationships between variables, a regression line offers both a visual and mathematical summary that complements other measures of association.\n\n> **Example:** \nConsider a marketing manager analyzing the relationship between monthly advertising spend and sales revenue. The manager collects data from the past 12 months to understand how these variables are related.\n@fig-regexpl shows a regression that is based on the equation \n$$sales = \\beta_0 + \\beta_1 \\cdot advertising$$\nThe coefficient $\\beta_1$ (approximately 2.5) provides a clear descriptive measure: \nin the data, for each additional thousand euros spent on advertising, monthly sales revenue increase on average by about 2500 EUR. \nThe intercept ($\\beta_0$) of around 50 represents the theoretical sales revenue when advertising spend is zero, though this value often has less practical interpretation.\n> \n> The visualization highlights both the overall trend and how individual data points deviate from it. The dotted lines — representing *residuals* — show the \"misses\" between what our regression line predicts (the 'fitted values') and the actual sales values. Some months performed better than the regression predicted (points above the line), while others performed worse (points below the line). These patterns themselves are descriptive insights that might prompt further investigation: What happened in months with large positive residuals that might explain their better-than-expected performance?\n> \n> As a descriptive tool, this regression cannot claim that advertising directly causes sales increases or predict future performance. It simply summarizes the historical relationship between these variables in a quantifiable way that goes beyond what correlation alone could tell us. Everything that goes beyond that is in the area of inferential statistics.\n\n\nas you can see in .\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Create data and conduct regression\"}\n# Create a simple business dataset\n\nset.seed(123) \n\n # Monthly ad expenditures in thousands of euros:\nadvertising_spend <- seq(15, 37, by = 2) \n# Sales in thousands with noise:\nsales_revenue <- 50 + 2.5 * advertising_spend + rnorm(12, 0, 5)  \n\n# Combine into a data frame\nmarketing_data <- data.frame(\n  month = month.abb[1:12],\n  advertising_spend = advertising_spend,\n  sales_revenue = sales_revenue\n)\n\n# Run a simple linear regression\nmodel <- lm(sales_revenue ~ advertising_spend, data = marketing_data)\n\n# Extract coefficients for our discussion\nbeta0 <- coef(model)[1]  # Intercept\nbeta1 <- coef(model)[2]  # Slope\n\n# Create data with regression information for plotting\nmarketing_data$fitted <- fitted(model)  # Predicted values from regression\nmarketing_data$residuals <- residuals(model)  # Differences between actual and predicted\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R code for the visualization\"}\nggplot(marketing_data, aes(x = advertising_spend, y = sales_revenue)) +\n  geom_point(size = 3, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\") +\n  geom_segment(aes(xend = advertising_spend, yend = fitted), \n               linetype = \"dotted\", color = \"gray30\") +\n  labs(\n    title = \"Advertising Expenditure and Sales Revenue\",\n    subtitle = paste0(\"y = \", round(beta0, 1), \" + \", round(beta1, 2), \"x\"),\n    x = \"Monthly Advertising Spend (thousands €)\",\n    y = \"Monthly Sales Revenue (thousands €)\",\n    caption = \"Dotted lines represent residuals - the difference between\\nactual sales and what the regression line predicts\"\n  ) +\n  annotate(\"text\", x = 30, y = 110, \n           label = \"Residuals show how much\\nactual sales differ from\\nwhat our line predicts\", \n           size = 3, hjust = 0) +\n  annotate(\"curve\", x = 29, y = 110, xend = 27, yend = 120,\n           arrow = arrow(length = unit(0.2, \"cm\")), curvature = -0.3) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(face = \"italic\")\n  )\n```\n\n::: {.cell-output-display}\n![The regression associated with the marketing example.](index_files/figure-html/fig-regexpl-1.png){#fig-regexpl width=672}\n:::\n:::\n\n\n\n# A Note on Visualizing Data\n\nThe power of descriptive statistics is greatly enhanced when combined with appropriate visualizations. Charts and graphs can reveal patterns that numbers alone might hide, making them essential tools for understanding your business data.\nA famous example is \"Anscombe's quartet\", as shown in @fig-anscombe. The four data sets have nearly identical descriptive statistics (see @tbl-anscombe), but are in fact very different.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"R code for the visualization\"}\n# Step 1: Transform the Anscombe data into a more workable format\n# The original data has separate columns for each dataset, we need to reshape it\nanscombe_long <- anscombe %>%\n  # Add row numbers to track observations\n  mutate(observation = row_number()) %>%\n  # Reshape to long format - this is where the magic happens\n  pivot_longer(cols = -observation,\n               names_to = \"variable\", \n               values_to = \"value\") %>%\n  # Extract dataset number and variable type from column names\n  mutate(\n    dataset = str_extract(variable, \"\\\\d+\"),  # Extract number (1,2,3,4)\n    axis = str_extract(variable, \"[xy]\"),     # Extract x or y\n    dataset = paste(\"Dataset\", dataset)        # Make it more readable\n  ) %>%\n  select(-variable) |> \n  # Reshape wider to have x and y as separate columns\n  pivot_wider(names_from = axis, values_from = value)\n\n# Step 2: Create individual scatter plots for each dataset\n# We'll create a function to ensure consistency across all plots\ncreate_anscombe_plot <- function(data_subset, title) {\n  # Calculate correlation for the subtitle\n  correlation <- cor(data_subset$x, data_subset$y)\n  \n  # Create the scatter plot with regression line\n  ggplot(data_subset, aes(x = x, y = y)) +\n    geom_point(size = 3, alpha = 0.7, color = \"steelblue\") +\n    # Add regression line with confidence interval\n    geom_smooth(method = \"lm\", se = FALSE, color = \"coral2\", alpha = 0.2) +\n    # Ensure consistent scales across all plots for fair comparison\n    scale_x_continuous(limits = c(3, 20), breaks = seq(4, 18, 2)) +\n    scale_y_continuous(limits = c(3, 13), breaks = seq(4, 12, 2)) +\n    labs(\n      title = title,\n      subtitle = paste(\"r =\", round(correlation, 3)),\n      x = \"X Values\",\n      y = \"Y Values\"\n    ) +\n    # Clean minimal theme\n    theme_minimal() +\n    theme(\n      plot.title = element_text(size = 12, face = \"bold\"),\n      plot.subtitle = element_text(size = 10),\n      axis.title = element_text(size = 10)\n    )\n}\n\n# Step 3: Create individual plots for each dataset\nplot1 <- anscombe_long %>%\n  filter(dataset == \"Dataset 1\") %>%\n  create_anscombe_plot(\"Dataset I: Linear Relationship\")\n\nplot2 <- anscombe_long %>%\n  filter(dataset == \"Dataset 2\") %>%\n  create_anscombe_plot(\"Dataset II: Curved Relationship\")\n\nplot3 <- anscombe_long %>%\n  filter(dataset == \"Dataset 3\") %>%\n  create_anscombe_plot(\"Dataset III: Linear with Outlier\")\n\nplot4 <- anscombe_long %>%\n  filter(dataset == \"Dataset 4\") %>%\n  create_anscombe_plot(\"Dataset IV: Vertical Outlier\")\n\n# Step 4: Arrange all plots in a 2x2 grid\nfinal_plot <- ggarrange(plot1, plot2, plot3, plot4,\n                       ncol = 2, nrow = 2,\n                       common.legend = FALSE)\n\n# Add an overall title to tie everything together\nfinal_plot <- annotate_figure(final_plot,\n                             top = text_grob(\"Anscombe's Quartet: Why Visualization Matters\",\n                                           face = \"bold\", size = 16))\n\n# Display the final visualization\nprint(final_plot)\n```\n\n::: {.cell-output-display}\n![The four data sets making up 'Anscombe's quartet'.](index_files/figure-html/fig-anscombe-1.png){#fig-anscombe width=672}\n:::\n:::\n\n\nAnd here are the summary statistics:\n\n\n::: {#tbl-anscombe .cell}\n\n```{.r .cell-code  code-summary=\"R code \"}\nsummary_stats <- anscombe_long %>%\n  group_by(dataset) %>%\n  summarise(\n    mean_x = round(mean(x), 2),\n    mean_y = round(mean(y), 2),\n    sd_x = round(sd(x), 2),\n    sd_y = round(sd(y), 2),\n    correlation = round(cor(x, y), 3),\n    .groups = 'drop'\n  )\nkable(summary_stats)\n```\n\n::: {.cell-output-display}\n\n\nTable: The descriptive statistics underlying 'Anscombe's quartet'.\n\n|dataset   | mean_x| mean_y| sd_x| sd_y| correlation|\n|:---------|------:|------:|----:|----:|-----------:|\n|Dataset 1 |      9|    7.5| 3.32| 2.03|       0.816|\n|Dataset 2 |      9|    7.5| 3.32| 2.03|       0.816|\n|Dataset 3 |      9|    7.5| 3.32| 2.03|       0.816|\n|Dataset 4 |      9|    7.5| 3.32| 2.03|       0.817|\n\n\n:::\n:::\n\n\n\nFor detailed guidance on creating effective data visualizations using `R` and `ggplot2`, please refer to the comprehensive tutorial available on our course homepage. \n\nRemember that the goal of descriptive statistics is not just to calculate numbers, but to gain genuine insight into your data. Each measure tells part of the story - the mean tells you where your data centers, the standard deviation tells you how spread out it is, and correlation tells you how variables relate to each other. Together, these tools provide a comprehensive picture that guides further analysis and decision-making.\n\nAs you practice using these concepts with business data, remember that no single statistic tells the complete story. Always consider multiple measures together, visualize your data when possible, and think critically about what the numbers are revealing about the underlying business phenomena you're studying. For instance, when analyzing customer satisfaction scores, look at both the average satisfaction and the variability - consistent high satisfaction is very different from highly variable satisfaction that averages to the same number.\n\nThink of descriptive statistics as the foundation of all business analytics. Just as you wouldn't build a house without first understanding the characteristics of your building materials, you shouldn't make business decisions without first understanding the basic characteristics of your data through these fundamental statistical measures.\n\n\n# Next steps\n- [Back to the recap overview](content/statrecap.qmd)\n- [Next topic: Essentials in Probability Theory](content/statrecap/ProbabilityTheory.qmd)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}