{
  "hash": "e68a51d28eaac1dd724c4631e8abe0de",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Linear Regression # Also check out line 43\nauthor: Claudius Gräbner-Radkowitsch\ndate: '2025-06-06'\ndocumentclass: scrartcl\nexecute: \n  freeze: auto\n  message: false\n  warning: false\nformat: \n  html: # All options are in metadata\n    code-fold: true \n  pdf:\n    include-in-header: \n      text: |\n        \\usepackage{lmodern} \n        \\usepackage{graphicx}\n        \\usepackage{hyperref}\n        \\usepackage{url}                \n        \\usepackage{xcolor}\n        \\usepackage{booktabs}\n        \\usepackage{listings}\n        \\lstloadlanguages{R}\n        \n        \\definecolor{eufblue}{RGB}{0,57,91}\n        \\definecolor{eufgrey}{RGB}{111,111,111}\n        \\definecolor{euflightblue}{RGB}{105,170,205}\n        \n        \\hypersetup{\n        pdfauthor={Claudius Graebner-Radkowitsch}\n        colorlinks=true,\n        linkcolor=euflightblue,\n        urlcolor=euflightblue\n        }\n        \\usepackage[includehead,includefoot,top=2cm, bottom=1.5cm]{geometry}\n        \\usepackage[headsepline, footsepline]{scrlayer-scrpage}\n        \\pagestyle{scrheadings}\n        \\clearpairofpagestyles\n        \\ihead{Tutorial: Linear Regression}\n        %\\chead{Kopfzeile Mitte}\n        \\ohead{\\pagemark} %\n        \\ifoot{}\n        \\cfoot{\\href{https://researchmethodology25spring.netlify.app}{Research Methodology - Spring Semester 2025}} % Fußzeile Mitte\n        \\ofoot{} \n        \\setkomafont{disposition}{\\color{eufblue}\\bfseries}\n---\n\n# Introduction\n\nIn this lab, we will focus on the practical aspects of implementing linear regression models in R.\nThe lab complements the respective lecture, but extends it by going from simple linear regression to multiple regression. \nWe'll also explore how to handle non-linear relationships through data transformation practically and discuss the important concept of omitted variable bias, something that was not part of the lecture itself.\n\n## Learning Objectives\n\nBy the end of this lab, you will be able to:\n\n- Implement simple and multiple linear regression in R\n- Calculate and interpret $R^2$ manually and using `R` functions\n- Understand and demonstrate omitted variable bias\n- Perform data transformations to linearize relationships\n- Create effective visualizations using `geom_smooth()`\n- Conduct basic model diagnostics\n\n## Prerequisites\n\n- Basic understanding of linear regression concepts from lecture\n- Familiarity with R and RStudio, as developed in previous labs\n- Understanding of basic statistical concepts (mean, variance, correlation), as provided by the statistics recap tutorials\n\n# Setup and Data\n\n## Loading Required Packages\n\nIn this lab we will use the following libraries:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages explicitly\nlibrary(ggplot2)     # For data visualization\nlibrary(dplyr)       # For data manipulation\nlibrary(readr)       # For reading CSV files\nlibrary(moderndive)  # For regression tables\nlibrary(broom)       # For model summaries\nlibrary(here)        # For file paths\nlibrary(kableExtra)  # For nice HTML tables, optional for you\n```\n:::\n\n\n## Loading the Datasets\n\nFor this lab, we'll work with the following business data sets that demonstrate different aspects of regression analysis.\nThey are available via the lab webpage.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the three datasets using here package\nmarketing_data <- read_csv(\"marketing_roi.csv\")\npricing_data <- read_csv(\"pricing_strategy.csv\")\nhr_data <- read_csv(\"hr_salaries.csv\")\nfirm_growth_data <- read_csv(\"firm_growth.csv\")\nmarketing_efficiency_data <- read_csv(\"marketing_efficiency.csv\")\n```\n:::\n\n\n# Simple Linear Regression\n\nLet's start with the fundamentals of simple linear regression using the marketing dataset.\nIt is always a good idea to first inspect the data set you are using:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(marketing_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 36\nColumns: 3\n$ ad_spend        <dbl> 10751.550, 20766.103, 13179.538, 22660.348, 23809.346,~\n$ website_traffic <dbl> 11003.952, 16667.299, 9407.983, 18692.329, 17995.468, ~\n$ sales_revenue   <dbl> 46252.00, 82831.26, 44909.07, 83920.10, 84140.37, 3299~\n```\n\n\n:::\n:::\n\n\n\n### Basic Implementation\n\nTo fit a linear regression model we use the function `lm()` (which stands for \"linear model\").\nHere we conduct a regression with sales revenue as the dependent, and ad spending as the independent variable:\n\n$$SALES = \\beta_0 + \\beta_1 EXP_{Ads} + \\epsilon $$\n\nTo this end we specify the LHS and RHS of the regression equation, separated by a `~`, through the argument `formula`.\nThe variable names must be the same as in the data set we use, and which we specify through the argument `data`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple regression: Sales Revenue ~ Ad Spend\nmodel_simple <- lm(formula = sales_revenue ~ ad_spend, data = marketing_data)\nmodel_simple\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = sales_revenue ~ ad_spend, data = marketing_data)\n\nCoefficients:\n(Intercept)     ad_spend  \n  14969.808        2.945  \n```\n\n\n:::\n:::\n\n\nYou can get additional information by using the function `summary()` on the\nresulting object, or the function `get_regression_table` from the package `moderndive`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Display results using moderndive for clean output\nget_regression_table(model_simple) %>%\n  kable(caption = \"Simple Linear Regression: Sales Revenue ~ Ad Spend\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n\n\\begin{longtable}[t]{lrrrrrr}\n\\caption{\\label{tab:unnamed-chunk-2}Simple Linear Regression: Sales Revenue ~ Ad Spend}\\\\\n\\toprule\nterm & estimate & std\\_error & statistic & p\\_value & lower\\_ci & upper\\_ci\\\\\n\\midrule\nintercept & 14969.808 & 2351.591 & 6.366 & 0 & 10190.801 & 19748.815\\\\\nad\\_spend & 2.945 & 0.133 & 22.139 & 0 & 2.674 & 3.215\\\\\n\\bottomrule\n\\end{longtable}\n\n\n:::\n:::\n\n\n\n### Interpreting the basic regression model\n\nWe estimated two main parameters of interest, the intercept $\\beta_0$ and the slope $\\beta_1$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract key values for interpretation\nintercept <- round(coef(model_simple)[1], 0)\nslope <- round(coef(model_simple)[2], 3)\n```\n:::\n\n\nIn our fitted model we have:\n\n- $\\hat{\\beta_0}=$ \\ensuremath{1.497\\times 10^{4}}\n- $\\hat{\\beta_1}=$ 2.945\n\nThe means that:\n\n- **Base Revenue**: Even without advertising, we can expect a baseline revenue of 14,970 EUR (although intercepts must be interpreted with great care)\n- **Return on Investment**: Every EUR spent on advertising is associated with on average approximately 2.94 EUR in additional sales revenue\n\nFor more details, see the accompanying lecture.\n\n### Visualizing the Regression\n\nOne of the most useful features of `ggplot2` is the `geom_smooth()` function, which can automatically fit and display regression lines:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create scatter plot with regression line\nggplot(marketing_data, aes(x = ad_spend, y = sales_revenue)) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1) +\n  labs(\n    title = \"Sales Revenue vs Ad Spend\",\n    x = \"Ad Spend (EUR)\",\n    y = \"Sales Revenue (EUR)\",\n    caption = \"Blue line shows linear regression fit\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/simple-viz-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.callout-tip}\n## Understanding geom_smooth()\n\n- `method = \"lm\"` fits a linear model\n- `se = FALSE` removes the confidence interval bands\n- `se = TRUE` (default) shows 95% confidence intervals\n- You can also use `method = \"loess\"` for flexible, non-parametric fits\n:::\n\n# Multiple Linear Regression\n\nReal-world relationships often involve multiple variables. Let's extend our model to include website traffic.\n\n### Adding a Second Predictor\n\nConceptually, we are now estimating the following model:\n\n$$SALES = \\beta_0 + \\beta_1 EXP_{Ads} + \\beta_2 TRAFFIC + \\epsilon $$\n\nTo do this in `R`, we proceed exactly as before and just add the new independent variable to the `formula`:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Multiple regression: Sales ~ Ad Spend + Website Traffic\nmodel_multiple <- lm(sales_revenue ~ ad_spend + website_traffic, \n                    data = marketing_data)\n\nget_regression_table(model_multiple) %>%\n  kable(caption = \"Multiple Linear Regression: Sales Revenue ~ Ad Spend + Website Traffic\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n\n\\begin{longtable}[t]{lrrrrrr}\n\\caption{\\label{tab:multiple-regression}Multiple Linear Regression: Sales Revenue ~ Ad Spend + Website Traffic}\\\\\n\\toprule\nterm & estimate & std\\_error & statistic & p\\_value & lower\\_ci & upper\\_ci\\\\\n\\midrule\nintercept & 12965.099 & 2398.245 & 5.406 & 0.000 & 8085.833 & 17844.365\\\\\nad\\_spend & 2.216 & 0.349 & 6.341 & 0.000 & 1.505 & 2.927\\\\\nwebsite\\_traffic & 0.985 & 0.440 & 2.236 & 0.032 & 0.089 & 1.881\\\\\n\\bottomrule\n\\end{longtable}\n\n\n:::\n:::\n\n\n## Interpreting Both Predictors\n\n\nWe now estimated *three* main parameters of interest, the intercept $\\beta_0$ and two slope parameters $\\beta_1$, and $\\beta_2$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract coefficients for interpretation\nintercept_mult <- round(coef(model_multiple)[1], 0)\nad_coef_mult <- round(coef(model_multiple)[2], 3)\ntraffic_coef <- round(coef(model_multiple)[3], 3)\n```\n:::\n\n\nIn our fitted model we have:\n\n- $\\hat{\\beta_0}=$ \\ensuremath{1.2965\\times 10^{4}}\n- $\\hat{\\beta_1}=$ 2.216\n- $\\hat{\\beta_2}=$ 0.985\n\nIn the multiple regression framework the interpretation of the coefficients changes slightly, as we now estimate *ceteris paribus* effects, sometimes also called *direct effects*, in contrast to the *total association* that was our focus in the simple regression framework.\n\nMore precisely, the estimates in the multiple regression framework control for changes in the other variables. \nThe means that these are the changes we can expect in the dependent variable, if the independent variable changed by one unit, and all other variables stayed the same.\n\nMore precisely:\n\n- $\\hat{\\beta_1}=$ 2.216 means that for every increase of 1 EUR in ad spending, there is an associated increase of revenue of, on\naverage and ceteris paribus, 2.216 EUR.\n\n- $\\hat{\\beta_2}=$ 0.985 means that every increase of website traffic by 1 person is an associated with an increase of revenue of, on\naverage and ceteris paribus, 0.985 EUR.\n\nNote that the *indirect effect* of ad spending is different to the direct effect estimated!\nMore precisely, the direct effect of advertising (2.216 EUR) is smaller than the total effect (2.945 EUR) because some of advertising's impact works through increased website traffic, and the multiple regression framework helps us to identify these different channels.\n\nWe also say that the simple regression model was *confounding* the effects of ad spend and website traffic!\n\n## Omitted Variable Bias\n\nThis brings is to one of the most important concepts in regression analysis:\n*omitted variable bias*.\nIt occurs when both of the following conditions are met:\n\n- There is a variable that correlates with our dependent variable **and**...\n- also correlates with one of our independent variables.\n\n### The Theory\n\nWhen we omit a relevant variable from our regression, the coefficients of included variables become **biased**. The bias depends on:\n\n1. How strongly the omitted variable affects the outcome\n2. How correlated the omitted variable is with included variables\n\n### Example 1: Marketing Data - Coefficient Size Change\n\nIn our previous example, $\\hat{\\beta}_1$ for the simple model was \n2.94\nand for the multiple model it was \n2.22,\nmeaning that we have a total bias of \n0.73.\n\nSince website traffic is positively correlated with ad spend AND positively affects sales, the simple model **overestimates** the effect of ad spend.\n\n### Example 2: When Coefficients Change Sign Completely\n\nSometimes omitted variable bias is so severe that it completely changes the **direction** of the relationship. Let's look at the first example of the lecture, the analysis of beer consumption:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeer_data <- DataScienceExercises::beer %>% \n  dplyr::mutate(income = income/1000)\n\nmodel_1 <- lm(\n  formula = consumption ~ income, \n  data=beer_data)\nmodel_2 <- lm(\n  formula = consumption ~ income + price, \n  data=beer_data)\nmodel_3 <- lm(\n  formula = consumption ~ income + price + price_liquor, \n  data=beer_data)\nmodel_4 <- lm(\n  formula = consumption ~ income + price  + price_liquor + price_other, \n  data=beer_data)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelsummary::modelsummary(\n  models = list(\n    \"Simple\"=model_1, \n    \"Model 2\"=model_2, \n    \"Model 3\"=model_3, \n    \"Model 4\"=model_4), \n  gof_map = c(\"nobs\", \"r.squared\"), \n  stars = TRUE)\n```\n\n::: {.cell-output-display}\n\\begin{table}\n\\centering\n\\begin{talltblr}[         %% tabularray outer open\nentry=none,label=none,\nnote{}={+ p \\num{< 0.1}, * p \\num{< 0.05}, ** p \\num{< 0.01}, *** p \\num{< 0.001}},\n]                     %% tabularray outer close\n{                     %% tabularray inner open\ncolspec={Q[]Q[]Q[]Q[]Q[]},\ncolumn{2,3,4,5}={}{halign=c,},\ncolumn{1}={}{halign=l,},\nhline{12}={1,2,3,4,5}{solid, black, 0.05em},\n}                     %% tabularray inner close\n\\toprule\n& Simple & Model 2 & Model 3 & Model 4 \\\\ \\midrule %% TinyTableHeader\n(Intercept) & \\num{96.439}*** & \\num{57.160}*** & \\num{67.440}** & \\num{82.159}*** \\\\\n& (\\num{7.521}) & (\\num{9.468}) & (\\num{19.995}) & (\\num{17.962}) \\\\\nincome & \\num{-1.237}*** & \\num{2.580}** & \\num{2.776}** & \\num{1.995}* \\\\\n& (\\num{0.229}) & (\\num{0.769}) & (\\num{0.847}) & (\\num{0.776}) \\\\\nprice &  & \\num{-27.653}*** & \\num{-25.968}*** & \\num{-23.743}*** \\\\\n&  & (\\num{5.438}) & (\\num{6.212}) & (\\num{5.429}) \\\\\nprice\\_liquor &  &  & \\num{-2.611} & \\num{-4.077} \\\\\n&  &  & (\\num{4.457}) & (\\num{3.890}) \\\\\nprice\\_other &  &  &  & \\num{12.924}** \\\\\n&  &  &  & (\\num{4.164}) \\\\\nNum.Obs. & \\num{30} & \\num{30} & \\num{30} & \\num{30} \\\\\nR2 & \\num{0.511} & \\num{0.750} & \\num{0.754} & \\num{0.822} \\\\\n\\bottomrule\n\\end{talltblr}\n\\end{table}\n:::\n:::\n\n\n\nThis example shows how omitted variable bias can lead to completely **wrong business conclusions**:\n\n- **Wrong conclusion** (simple model): \"Higher-income customers drink less beer - target low-income segments\"\n- **Correct conclusion** (multiple model): \"Higher-income customers drink more beer, but they're price-sensitive - consider premium pricing strategies\"\n\nThe simple regression result could lead to disastrous policy decisions!\n\n# Model Diagnostics\n\nGood regression analysis doesn't stop at fitting the model. We need to check our assumptions and test how good the model explains the dependent variable.\n\n## The explanatory power: $R^2$\n\n$R^2$ measures the proportion of variation in the dependent variable explained by our model. \n\nIt is defined as the ratio between the total variation ('Total Sum of Squares') and\nthe explained variation. Since the residuals are our measure for *unexplained* variation (see the lecture),\nwe can compute $R^2$ by the following formula:\n\n$$R^2 = 1- \\frac{RSS}{TSS}$$\n\nLet's calculate it manually to understand what it means.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the actual and fitted values\ny_actual <- marketing_data$sales_revenue\ny_fitted <- predict(model_simple)\ny_mean <- mean(y_actual)\n\n# Calculate the components\nTSS <- sum((y_actual - y_mean)^2)      # Total Sum of Squares\nRSS <- sum((y_actual - y_fitted)^2)    # Residual Sum of Squares\n\n# Calculate R²\nr_squared_manual <- 1 - (RSS / TSS)\n\ncat(\"Manual R² calculation:\", round(r_squared_manual, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nManual R² calculation: 0.9351 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R's built-in R²:\", round(summary(model_simple)$r.squared, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR's built-in R²: 0.9351 \n```\n\n\n:::\n:::\n\n\nBut in practice there is no need to compute it manually, as it is stored in the \n`summary` of every regression object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare R² between models\nr2_simple <- summary(model_simple)$r.squared\nr2_multiple <- summary(model_multiple)$r.squared\n\ncat(\"Simple model R²:\", round(r2_simple, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple model R²: 0.9351 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Multiple model R²:\", round(r2_multiple, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMultiple model R²: 0.9437 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Improvement:\", round(r2_multiple - r2_simple, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImprovement: 0.0085 \n```\n\n\n:::\n:::\n\n\nIn our case:\n\n- The $R^2$ of the simple model is 0.9351. This means that this model explains 0.94 % of the variation in the dependent variable.\n- The $R^2$ of the multiple model is 0.9437. This means that this model explains 0.94 % of the variation in the dependent variable.\n\nBut be careful: a higher $R^2$ does not necessarily imply better prediction or inference capability of the model!\n\n## Key Diagnostic Plots\n\nThe most important assumptions of linear regression are about the nature of the error term $\\epsilon$.\nBut as discussed previously, the error term is located on the population level,\nmeaning it will always remain unobservable. Therefore, we usually inspect its sample equivalent, the residuals, to test the most important assumptions.\n\nThe two most important assumptions are:\n\n1. The error term is uncorrelated with the dependent variable.\n2. The error follows a normal distribution with mean zero.\n\nTo test assumption 1, we can look at the correlation between residuals and fitted values.\nIf our model is good, we should see no structure.\n\nTo test assumption 2, we can look at a so called QQ-plot.\nQQ stands for quantile-quantile. Such a plot plots the quantiles of the data\n(here: residuals) against the theoretical quantiles if the data was following a normal distribution.\nIf the residuals are normally distributed, they should follow a straight line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create augmented data with fitted values and residuals\nmarketing_augmented <- augment(model_multiple)\n\n# Tukey-Anscombe Plot (Residuals vs Fitted)\np1 <- ggplot(marketing_augmented, aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Residuals vs Fitted Values\",\n    x = \"Fitted Values\",\n    y = \"Residuals\",\n    subtitle = \"Should show no clear pattern\"\n  ) +\n  theme_minimal()\n\n# Normal Q-Q Plot\np2 <- ggplot(marketing_augmented, aes(sample = .resid)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(\n    title = \"Normal Q-Q Plot\",\n    subtitle = \"Points should follow the red line\"\n  ) +\n  theme_minimal()\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/diagnostics-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nIn this example we see that both assumptions seem to be satisfied, although some\nsmall structure in the residuals persist. In the end, there is no clear-cut rule when there is no structure, but it is usually difficult to do an even better job as in this example.\n\n# Data Transformation for Non-linear Relationships\n\nNot all relationships are linear!\nBut remember that for linear regression, we only need to assume \n*linearity in parameters*! \nThis means that sometimes we can transform our data to make initially non-linear relationships linear and still use linear regression effectively.\n\n## When to Transform Data\n\nCommon scenarios requiring transformation:\n\n- **Exponential relationships**: Often seen with salary/experience, population growth\n- **U-shaped relationships**: Common in economics (e.g., effort vs performance)\n- **Diminishing returns**: Sales response to advertising often follows this pattern\n\nIn practice, always visualize your data first and experiment with various transformation strategies - this is the best way to determine whether data transformation can help you.\n\nAlso, the diagnostic plots discussed above can also provide hints on necessary transformations:\nif the Tukey-Anscombe plot shows structure, think about transforming the data!\n\n## Log Transformation Example\n\nLet's examine a **company's revenue growth over time** - a classic example of exponential growth that appears linear after log transformation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First, visualize the raw exponential relationship\nggplot(firm_growth_data, aes(x = year, y = revenue)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\", linewidth = 1) +\n  labs(\n    title = \"Company Revenue Growth Over Time (Raw Data)\",\n    subtitle = \"Red = Linear fit (clearly inadequate), Blue = Flexible fit\",\n    x = \"Year\",\n    y = \"Revenue (EUR)\",\n    caption = \"Notice how the linear fit completely misses the exponential pattern\"\n  ) +\n  scale_y_continuous(labels = scales::comma) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/log-transformation-setup-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe exponential pattern is very clear! The red linear line completely fails to capture the relationship. Let's try a log transformation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create log-transformed variable\nfirm_growth_data_log <- firm_growth_data %>%\n  mutate(log_revenue = log(revenue))\n\n# Fit both models\nmodel_linear_exp <- lm(revenue ~ year, data = firm_growth_data)\nmodel_log_exp <- lm(log_revenue ~ year, data = firm_growth_data_log)\n\n# Visualize the log-transformed relationship\nggplot(firm_growth_data_log, aes(x = year, y = log_revenue)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1) +\n  labs(\n    title = \"Log(Revenue) vs Year\",\n    subtitle = \"Perfect linear relationship after transformation!\",\n    x = \"Year\",\n    y = \"Log(Revenue)\",\n    caption = \"The transformation successfully linearizes the exponential relationship\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/log-transformation-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelsummary::modelsummary(\n  models = list(\"Lin-Lin\"=model_linear_exp, \"Log-lin\"=model_log_exp), \n  gof_map = c(\"nobs\", \"r.squared\"), \n  stars = TRUE, output = \"kableExtra\") \n```\n\n::: {.cell-output-display}\n\\begin{table}\n\\centering\n\\begin{tabular}[t]{lcc}\n\\toprule\n  & Lin-Lin & Log-lin\\\\\n\\midrule\n(Intercept) & \\num{-25037.426}* & \\num{9.300}***\\\\\n & (\\num{10107.611}) & (\\num{0.078})\\\\\nyear & \\num{8991.437}*** & \\num{0.144}***\\\\\n & (\\num{843.767}) & (\\num{0.007})\\\\\n\\midrule\nNum.Obs. & \\num{20} & \\num{20}\\\\\nR2 & \\num{0.863} & \\num{0.964}\\\\\n\\bottomrule\n\\multicolumn{3}{l}{\\rule{0pt}{1em}+ p $<$ 0.1, * p $<$ 0.05, ** p $<$ 0.01, *** p $<$ 0.001}\\\\\n\\end{tabular}\n\\end{table}\n\n\n:::\n:::\n\n\n\n### Interpreting Log Models\n\nWhen you transform data, you need to interpret your results accordingly. \nFor instance, when the dependent variable is log-transformed, coefficients represent **percentage changes**:\n\n> Every year is, on average, associated with a  \n14.4\n% increase in revenue.\n\n\n::: {.callout-important}\n### Why Log Transformation Works for Exponential Data\n\nExponential relationships have the form: $Y = A \\cdot e^{Bx}$\n\nTaking the natural log: $\\ln(Y) = \\ln(A) + Bx$\n\nThis transforms the exponential curve into a straight line where:\n- The slope B represents the **growth rate**\n- $e^B - 1$ gives the percentage change per unit increase in x\n:::\n\n\nThe residual analysis discussed below can reveal how much better to log\nmodel performs in this context. But also by plotting the model on the raw\ndata already makes this very clear:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show predictions on original scale\nexp_predictions <- firm_growth_data %>%\n  mutate(\n    linear_pred = predict(model_linear_exp),\n    log_pred = exp(predict(model_log_exp, newdata = firm_growth_data_log))\n  )\n\np3 <- ggplot(exp_predictions, aes(x = year)) +\n  geom_point(aes(y = revenue), size = 2, alpha = 0.8) +\n  geom_line(aes(y = linear_pred), color = \"red\", linewidth = 1) +\n  geom_line(aes(y = log_pred), color = \"blue\", linewidth = 1) +\n  labs(title = \"Model Predictions Comparison\",\n       subtitle = \"Red = Linear model, Blue = Log model (back-transformed)\",\n       x = \"Year\", y = \"Revenue (EUR)\") +\n  scale_y_continuous(labels = scales::comma) +\n  theme_minimal()\np3\n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/logmodel-comparison-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Quadratic Relationships\n\nSometimes relationships are U-shaped or inverted-U shaped. A perfect example is **marketing efficiency**: too little spending is inefficient (high fixed costs), and too much spending leads to diminishing returns. Let's explore this dramatic pattern:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First, visualize the clear U-shaped relationship\nggplot(marketing_efficiency_data, aes(x = marketing_spend, y = cost_per_acquisition)) +\n  geom_point(size = 2, alpha = 0.8, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\", linewidth = 1) +\n  labs(\n    title = \"Marketing Efficiency: Cost per Acquisition vs Marketing Spend\",\n    subtitle = \"Red = Linear fit (completely wrong!), Blue = Flexible fit (captures U-shape)\",\n    x = \"Marketing Spend (thousands EUR)\",\n    y = \"Cost per Acquisition (EUR)\",\n    caption = \"Clear U-shaped pattern: optimal spending around 50k EUR\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/exercise-2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe U-shaped pattern is unmistakable! Now let's compare linear vs quadratic models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create squared term for quadratic model\nmarketing_efficiency_data <- marketing_efficiency_data %>%\n  mutate(marketing_spend_squared = marketing_spend^2)\n\n# Fit both models\nmarketing_linear <- lm(cost_per_acquisition ~ marketing_spend, data = marketing_efficiency_data)\nmarketing_quadratic <- lm(cost_per_acquisition ~ marketing_spend + marketing_spend_squared, \n                         data = marketing_efficiency_data)\n\n# Show regression results\ncat(\"=== LINEAR MODEL (completely inadequate) ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== LINEAR MODEL (completely inadequate) ===\n```\n\n\n:::\n\n```{.r .cell-code}\nget_regression_table(marketing_linear) %>%\n  kable(caption = \"Linear Model: Cost per Acquisition ~ Marketing Spend\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n\n\\begin{longtable}[t]{lrrrrrr}\n\\caption{\\label{tab:quadratic-models}Linear Model: Cost per Acquisition ~ Marketing Spend}\\\\\n\\toprule\nterm & estimate & std\\_error & statistic & p\\_value & lower\\_ci & upper\\_ci\\\\\n\\midrule\nintercept & 632.844 & 110.713 & 5.716 & 0.000 & 410.242 & 855.447\\\\\nmarketing\\_spend & 0.021 & 1.908 & 0.011 & 0.991 & -3.815 & 3.857\\\\\n\\bottomrule\n\\end{longtable}\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n=== QUADRATIC MODEL (captures the U-shape) ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== QUADRATIC MODEL (captures the U-shape) ===\n```\n\n\n:::\n\n```{.r .cell-code}\nget_regression_table(marketing_quadratic) %>%\n  kable(caption = \"Quadratic Model: Cost per Acquisition ~ Marketing Spend + Marketing Spend²\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n\n\\begin{longtable}[t]{lrrrrrr}\n\\caption{\\label{tab:quadratic-models}Quadratic Model: Cost per Acquisition ~ Marketing Spend + Marketing Spend²}\\\\\n\\toprule\nterm & estimate & std\\_error & statistic & p\\_value & lower\\_ci & upper\\_ci\\\\\n\\midrule\nintercept & 1452.080 & 5.857 & 247.914 & 0 & 1440.297 & 1463.863\\\\\nmarketing\\_spend & -50.157 & 0.271 & -185.159 & 0 & -50.702 & -49.612\\\\\nmarketing\\_spend\\_squared & 0.502 & 0.003 & 191.548 & 0 & 0.497 & 0.507\\\\\n\\bottomrule\n\\end{longtable}\n\n\n:::\n\n```{.r .cell-code}\n# Compare R² - dramatic difference!\nr2_linear_quad <- summary(marketing_linear)$r.squared\nr2_quadratic <- summary(marketing_quadratic)$r.squared\n\ncat(\"\\n=== MODEL COMPARISON ===\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== MODEL COMPARISON ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nLinear model R²:\", round(r2_linear_quad, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear model R²: 0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nQuadratic model R²:\", round(r2_quadratic, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nQuadratic model R²: 0.999\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nImprovement:\", round(r2_quadratic - r2_linear_quad, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nImprovement: 0.999\n```\n\n\n:::\n:::\n\n\nNow let's visualize both model fits to see the dramatic difference:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create predictions for smooth curves\nmarketing_range <- seq(0, 100, length.out = 100)\npred_data <- tibble(\n  marketing_spend = marketing_range,\n  marketing_spend_squared = marketing_range^2\n)\n\n# Get predictions\npred_data$linear_pred <- predict(marketing_linear, newdata = pred_data)\npred_data$quadratic_pred <- predict(marketing_quadratic, newdata = pred_data)\n\n# Create comprehensive comparison plot\nggplot(marketing_efficiency_data, aes(x = marketing_spend, y = cost_per_acquisition)) +\n  geom_point(size = 2, alpha = 0.8, color = \"black\") +\n  geom_line(data = pred_data, aes(y = linear_pred), \n            color = \"red\", linewidth = 1.2) +\n  geom_line(data = pred_data, aes(y = quadratic_pred), \n            color = \"blue\", linewidth = 1.2) +\n  labs(\n    title = \"Linear vs Quadratic Model Comparison\",\n    subtitle = \"Red = Linear model (R² = 0.001), Blue = Quadratic model (R² = 0.95+)\",\n    x = \"Marketing Spend (thousands EUR)\",\n    y = \"Cost per Acquisition (EUR)\",\n    caption = \"The quadratic model perfectly captures the optimal spending point\"\n  ) +\n  annotate(\"text\", x = 50, y = 180, \n           label = \"Optimal spending\\n(lowest cost per acquisition)\", \n           hjust = 0.5, color = \"blue\", size = 3) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/quadratic-visualization-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Understanding Quadratic Interpretation\n\nWith quadratic terms, interpretation becomes more nuanced because the effect of the variable **changes** depending on its current level:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract coefficients for interpretation\nlinear_coef <- coef(marketing_quadratic)[\"marketing_spend\"]\nquadratic_coef <- coef(marketing_quadratic)[\"marketing_spend_squared\"]\n\ncat(\"=== COEFFICIENT INTERPRETATION ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n=== COEFFICIENT INTERPRETATION ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Linear term coefficient:\", round(linear_coef, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear term coefficient: -50.157 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Quadratic term coefficient:\", round(quadratic_coef, 4), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQuadratic term coefficient: 0.5018 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Find the optimal point (minimum of the parabola)\noptimal_spend <- -linear_coef / (2 * quadratic_coef)\ncat(\"Optimal marketing spend:\", round(optimal_spend, 1), \"thousand EUR\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOptimal marketing spend: 50 thousand EUR\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate minimum cost per acquisition\noptimal_cost <- predict(marketing_quadratic, \n                       newdata = tibble(marketing_spend = optimal_spend,\n                                      marketing_spend_squared = optimal_spend^2))\ncat(\"Minimum cost per acquisition:\", round(optimal_cost, 0), \"EUR\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMinimum cost per acquisition: 199 EUR\n```\n\n\n:::\n:::\n\n\n### Marginal Effects in Quadratic Models\n\nThe **marginal effect** (slope) at any point is: Linear coefficient + 2 × Quadratic coefficient × X\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate marginal effects at different spending levels\nspending_levels <- c(20, 40, 50, 60, 80)\n\nmarginal_effects <- linear_coef + 2 * quadratic_coef * spending_levels\n\nmarginal_table <- tibble(\n  `Spending Level (k EUR)` = spending_levels,\n  `Marginal Effect` = round(marginal_effects, 3),\n  `Interpretation` = case_when(\n    marginal_effects < -0.1 ~ \"Strong efficiency gains from more spending\",\n    marginal_effects > 0.1 ~ \"Diminishing returns - reduce spending\", \n    TRUE ~ \"Near optimal - small changes have little effect\"\n  )\n)\n\nmarginal_table %>%\n  kable(caption = \"Marginal Effects at Different Spending Levels\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\")) \n```\n\n::: {.cell-output-display}\n\n\\begin{longtable}[t]{rrl}\n\\caption{\\label{tab:marginal-effects}Marginal Effects at Different Spending Levels}\\\\\n\\toprule\nSpending Level (k EUR) & Marginal Effect & Interpretation\\\\\n\\midrule\n20 & -30.086 & Strong efficiency gains from more spending\\\\\n40 & -10.015 & Strong efficiency gains from more spending\\\\\n50 & 0.021 & Near optimal - small changes have little effect\\\\\n60 & 10.057 & Diminishing returns - reduce spending\\\\\n80 & 30.128 & Diminishing returns - reduce spending\\\\\n\\bottomrule\n\\end{longtable}\n\n\n:::\n:::\n\n\n::: {.callout-tip}\nUnderstanding Quadratic Models\n\n**The quadratic model equation**: Cost = β₀ + β₁×Spend + β₂×Spend²\n\n**Key insights:**\n\n1. **Linear term** (β₁ = -50.157): The initial direction of the relationship\n2. **Quadratic term** (β₂ = 0.5018): How the slope changes\n   - Positive β₂ → U-shaped (costs increase at extremes)\n   - Negative β₂ → Inverted-U (optimal point in middle)\n3. **Optimal point**: Where marginal effect = 0\n4. **Business implication**: There's a \"sweet spot\" for efficiency\n\n:::\n\n### Practical Business Application\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a strategy table based on current spending\ncurrent_scenarios <- tibble(\n  `Current Spending` = c(\"10k EUR\", \"30k EUR\", \"50k EUR\", \"70k EUR\", \"90k EUR\"),\n  `Predicted Cost` = round(predict(marketing_quadratic, \n                                  newdata = tibble(marketing_spend = c(10, 30, 50, 70, 90),\n                                                 marketing_spend_squared = c(10, 30, 50, 70, 90)^2)), 0),\n  `Recommendation` = c(\n    \"Increase spending significantly\",\n    \"Increase spending moderately\", \n    \"Optimal level - maintain\",\n    \"Reduce spending moderately\",\n    \"Reduce spending significantly\"\n  ),\n  `Rationale` = c(\n    \"Far from optimum, high potential gains\",\n    \"Below optimum, efficiency improvements available\",\n    \"At optimal point for efficiency\",\n    \"Above optimum, diminishing returns setting in\",\n    \"Far above optimum, wasting resources\"\n  )\n)\n\ncurrent_scenarios %>%\n  kable(caption = \"Strategic Recommendations Based on Quadratic Model\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\")) %>%\n  row_spec(3, bold = TRUE, color = \"green\") # Highlight optimal\n```\n\n::: {.cell-output-display}\n\n\\begin{longtable}[t]{lrll}\n\\caption{\\label{tab:business-application}Strategic Recommendations Based on Quadratic Model}\\\\\n\\toprule\nCurrent Spending & Predicted Cost & Recommendation & Rationale\\\\\n\\midrule\n10k EUR & 1001 & Increase spending significantly & Far from optimum, high potential gains\\\\\n30k EUR & 399 & Increase spending moderately & Below optimum, efficiency improvements available\\\\\n\\textcolor{green}{\\textbf{50k EUR}} & \\textcolor{green}{\\textbf{199}} & \\textcolor{green}{\\textbf{Optimal level - maintain}} & \\textcolor{green}{\\textbf{At optimal point for efficiency}}\\\\\n70k EUR & 400 & Reduce spending moderately & Above optimum, diminishing returns setting in\\\\\n90k EUR & 1002 & Reduce spending significantly & Far above optimum, wasting resources\\\\\n\\bottomrule\n\\end{longtable}\n\n\n:::\n:::\n\n\n### Residual Analysis Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare residual patterns\nlibrary(gridExtra)\n\n# Linear model residuals - should show clear pattern\np1 <- marketing_efficiency_data %>%\n  mutate(fitted = fitted(marketing_linear), residuals = residuals(marketing_linear)) %>%\n  ggplot(aes(x = fitted, y = residuals)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(title = \"Linear Model Residuals\", \n       subtitle = \"Strong U-pattern = missing quadratic term\",\n       x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n\n# Quadratic model residuals - should be random\np2 <- marketing_efficiency_data %>%\n  mutate(fitted = fitted(marketing_quadratic), residuals = residuals(marketing_quadratic)) %>%\n  ggplot(aes(x = fitted, y = residuals)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(title = \"Quadratic Model Residuals\", \n       subtitle = \"Random scatter = good fit!\",\n       x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/quadratic-residuals-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.callout-note}\n## Business Interpretation\n\nThe quadratic model reveals important business insights:\n\n1. **Optimal spending**: Around 50k EUR gives the lowest cost per acquisition\n2. **Efficiency curve**: Both under-spending and over-spending are inefficient\n3. **Marginal effects**: The impact of additional spending depends on current spending level\n4. **Strategic implications**: There's a \"sweet spot\" for marketing efficiency\n:::\n\n::: {.callout-note}\n## Your Turn\n\n1. How would you interpret the linear and quadratic coefficients in business terms?\n2. At what spending levels is marketing particularly inefficient?\n3. What would happen if we used the linear model for business decisions?\n:::\n\n\n# Practical Exercises\n\n## Exercise 1: Complete HR Analysis\n\nAnalyze the relationship between experience and salary, controlling for education level:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your turn: Create dummy variables for education and run multiple regression\nhr_with_dummies <- hr_data %>%\n  mutate(\n    master = ifelse(education == \"Master\", 1, 0),\n    phd = ifelse(education == \"PhD\", 1, 0)\n    # Bachelor's degree is the reference category\n  )\n\n# Fit the multiple regression model\nhr_multiple <- lm(salary ~ experience + master + phd, data = hr_with_dummies)\nget_regression_table(hr_multiple) %>%\n  kable(caption = \"Multiple Regression: Salary ~ Experience + Education\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n\n\\begin{longtable}[t]{lrrrrrr}\n\\caption{\\label{tab:exercise-1}Multiple Regression: Salary ~ Experience + Education}\\\\\n\\toprule\nterm & estimate & std\\_error & statistic & p\\_value & lower\\_ci & upper\\_ci\\\\\n\\midrule\nintercept & 29729.90 & 829.309 & 35.849 & 0 & 28093.228 & 31366.568\\\\\nexperience & 1985.06 & 50.810 & 39.068 & 0 & 1884.784 & 2085.336\\\\\nmaster & 10217.58 & 798.832 & 12.791 & 0 & 8641.056 & 11794.102\\\\\nphd & 25514.67 & 1061.832 & 24.029 & 0 & 23419.112 & 27610.238\\\\\n\\bottomrule\n\\end{longtable}\n\n\n:::\n:::\n\n\n::: {.callout-note}\n## Interpretation Challenge\n\n1. How do you interpret the coefficient for `experience` now?\n2. What's the salary premium for having a Master's degree vs Bachelor's?\n3. How much omitted variable bias was there in the simple experience-only model?\n:::\n\n\n\n\n# Key Takeaways\n\n## Main Learning Points\n\n1. **Multiple regression** controls for confounding variables and reduces omitted variable bias\n2. **$R^2$** measures explained variance but isn't the only criterion for model quality\n3. **Data transformation** can help linearize non-linear relationships\n4. **geom_smooth(method = \"lm\")** provides easy and effective visualization of regression fits\n5. **Residual analysis** is crucial for checking model assumptions\n6. **Business context** should always guide model interpretation\n7. **Omitted variable bias** can completely reverse coefficient signs and lead to wrong business decisions\n8. **Quadratic models** capture non-linear relationships and reveal optimal points\n\n## Common Pitfalls to Avoid\n\n- Don't interpret correlation as causation\n- Don't ignore potential confounding variables\n- Don't rely solely on $R^2$ for model selection\n- Don't forget to check model assumptions through residual analysis\n- Don't use linear models for clearly non-linear relationships without considering transformations\n- Don't make business decisions based on biased models\n",
    "supporting": [
      "index_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{longtable}\n\\usepackage{array}\n\\usepackage{multirow}\n\\usepackage{wrapfig}\n\\usepackage{float}\n\\usepackage{colortbl}\n\\usepackage{pdflscape}\n\\usepackage{tabu}\n\\usepackage{threeparttable}\n\\usepackage{threeparttablex}\n\\usepackage[normalem]{ulem}\n\\usepackage{makecell}\n\\usepackage{xcolor}\n\\usepackage{tabularray}\n\\usepackage[normalem]{ulem}\n\\usepackage{graphicx}\n\\usepackage{rotating}\n\\UseTblrLibrary{booktabs}\n\\UseTblrLibrary{siunitx}\n\\NewTableCommand{\\tinytableDefineColor}[3]{\\definecolor{#1}{#2}{#3}}\n\\newcommand{\\tinytableTabularrayUnderline}[1]{\\underline{#1}}\n\\newcommand{\\tinytableTabularrayStrikeout}[1]{\\sout{#1}}\n\\usepackage{siunitx}\n\n    \\newcolumntype{d}{S[\n      table-align-text-before=false,\n      table-align-text-after=false,\n      input-symbols={-,\\*+()}\n    ]}\n  \n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}