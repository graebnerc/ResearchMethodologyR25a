[
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html",
    "href": "content/statrecap/ProbabilityTheory/index.html",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "",
    "text": "Packages used for R examples\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(ggpubr)\nlibrary(latex2exp)"
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#random-experiments-the-source-of-uncertainty",
    "href": "content/statrecap/ProbabilityTheory/index.html#random-experiments-the-source-of-uncertainty",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Random Experiments: The Source of Uncertainty",
    "text": "Random Experiments: The Source of Uncertainty\nA random experiment is any process or activity whose outcome cannot be predicted with certainty beforehand, even when we understand the factors involved. In business, almost every decision involves random experiments - from market research to product launches to employee performance.\nKey characteristics of random experiments:\n\nThe outcome is uncertain before the experiment occurs\nWe can usually identify all possible outcomes\nUnder similar conditions, different outcomes may occur\n\nHere are some examples of random experiments typical for the business world:\n\nExample 1: Product Launch Launching a new product in a regional market is a random experiment. Even with extensive market research, competitor analysis, and careful planning, you cannot know with certainty how many units will sell in the first quarter. Multiple factors - economic conditions, competitor responses, changing consumer preferences - combine to create uncertainty.\n\n\nExample 2: Job Interview Process Selecting a candidate through interviews is a random experiment. Despite standardized questions and evaluation criteria, the final hiring decision involves uncertainty about how well the candidate will actually perform on the job.\n\n\nExample 3: Marketing Campaign Running an advertising campaign across different channels represents a random experiment. Though you can estimate response rates based on historical data, the actual number of conversions remains uncertain until the campaign runs."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#events-what-we-care-about",
    "href": "content/statrecap/ProbabilityTheory/index.html#events-what-we-care-about",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Events: What We Care About",
    "text": "Events: What We Care About\nAn event is a specific outcome or collection of outcomes from a random experiment that we’re particularly interested in. Events represent the business questions we want to answer or the scenarios we want to evaluate.\nIf we consider the examples for random experiments from above we can also provide some examples for events. In the context of our product launch experiment, possible events include:\n\nEvent A: “Sales exceed €1 million in the first quarter”\nEvent B: “The product breaks even within six months”\n\nEvent C: “Customer satisfaction scores average above 8.0”\n\nIn the context of the the job interview experiment we could think of the following:\n\nEvent A: “The hired candidate receives a performance rating of ‘exceeds expectations’ in their first year”\nEvent B: “The candidate stays with the company for at least two years”\n\nNotice how events allow us to focus on specific business outcomes rather than all possible details of the experiment."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#probability-measuring-likelihood",
    "href": "content/statrecap/ProbabilityTheory/index.html#probability-measuring-likelihood",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Probability: Measuring Likelihood",
    "text": "Probability: Measuring Likelihood\nProbability quantifies how likely an event is to occur. It provides a numerical scale from 0 to 1 (or 0% to 100%) where:\n\nProbability = 0 means the event is impossible\nProbability = 1 means the event is certain\nProbability = 0.5 means the event is equally likely to occur or not occur\n\nIn business contexts, the concept of probability is essential when assessing risks and opportunities, making informed decisions under uncertainty, or communicating about likelihood in precise and transparent terms. In other words, making rational decisions requires thinking about probabilities.\nOften, we make statements about probabilities based on our previous knowledge or after inspecting relevant data. In fact, statistics is exactly about that: how to make smart statements about probabilities given what we know.\nHere is an example of how such statements could look and how they are often expressed more formally:\n\nExample: Product Launch Probabilities Based on market research and historical data, you might conclude that:\n\nThe probability that total sales exceed 1M EUR is 70%, i.e., there is a 70% chance that the event “Sales exceed 1M EUR” actually occurs in the future.\n\nMore formally: \\(\\mathbb{P}(R&gt;1M)=0.7\\), where \\(R\\) stands for ‘revenues’.\n\nThe probability that our project breaks even within 6 months is 85%, i.e., there is an 85% chance that the event “Break even within 6 months” actually occurs.\n\nMore formally: \\(\\mathbb{P}(BE)=0.85\\), where \\(BE\\) stands for “Break even within 6 months”.\n\nThe probability that the customer satisfaction score exceeds 8 is 60%, i.e., there is a 60% chance that the event “Customer satisfaction score is larger than 8” actually occurs.\n\nMore formally: \\(\\mathbb{P}(CSC&gt;8)=0.6\\), where \\(CSC\\) stands for “Customer Satisfaction Score”."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#conditional-probability-when-context-matters",
    "href": "content/statrecap/ProbabilityTheory/index.html#conditional-probability-when-context-matters",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Conditional Probability: When Context Matters",
    "text": "Conditional Probability: When Context Matters\nOften in business, the probability that one event occurs depends on the circumstances. Conditional probability is an important concept in this context as it helps answer the question:\n\n“What’s the likelihood of Event A happening, given that Event B has already occurred or is known to be true?”\n\nWe write this as \\(\\mathbb{P}(A|B)\\), read as “the probability of A given B.” (or, more verbosely: “The probability that event A occurs, given that event B has occurred.”)\nConditional probabilities are a key concept because most business decisions involve conditional thinking. Also, while you usually cannot predict the future with certainty, you are also rarely operating in a situation of complete uncertainty - you usually have some relevant information that should influence your probability assessments.\nExample: Marketing Campaign Success\nConsider the probability that a marketing campaign generates high conversion rates. This actually depends on factors such as the general economic situation. So while we can operate with the following baseline probability:\n\\[\\mathbb{P}(\\text{High conversions}) = 0.3\\]\nadditional information about the general economic situation and the market environment would allow us to make more precise statements (because we know these variables influence the likelihood for high conversions).\nFor example, if we knew that we were operating in a booming environment:\n\\[\\mathbb{P}(\\text{High conversions}|\\text{Economic boom}) = 0.5\\]\nSimilarly, if we were in a recession:\n\\[\\mathbb{P}(\\text{High conversions}|\\text{Economic recession}) = 0.15\\]\nNote that:\n\\[\\mathbb{P}(\\text{High conversions}|\\text{Economic boom}) &gt; \\mathbb{P}(\\text{High conversions})\\] and \\[\\mathbb{P}(\\text{High conversions}|\\text{Economic recession}) &lt; \\mathbb{P}(\\text{High conversions})\\]\nThe conditional probabilities differ significantly from the baseline probability, showing how context dramatically affects business outcomes. Conditional probabilities allow us to formalize our knowledge (or hypotheses) about relationships within the language of probabilities. As we will learn below, this is key for developing rational decision strategies and learning rationally from observations."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#short-recap",
    "href": "content/statrecap/ProbabilityTheory/index.html#short-recap",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Short recap",
    "text": "Short recap\nThese building blocks work together in every business analysis:\n\nIdentify the random experiment: What uncertain process are you analyzing?\nDefine relevant events: What specific outcomes matter for your decision?\nAssess probabilities: What’s the likelihood of each event?\nConsider conditional probabilities: How does available information change these likelihoods?\n\nUnderstanding these fundamentals prepares you to work with random variables, which provide a systematic way to assign numbers to the outcomes of random experiments. This numerical approach, which we’ll explore next, enables the powerful statistical methods you’ll use throughout your research and management career."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#detour-why-is-a-random-variable-called-a-function",
    "href": "content/statrecap/ProbabilityTheory/index.html#detour-why-is-a-random-variable-called-a-function",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Detour: Why is a random variable called a “function”?",
    "text": "Detour: Why is a random variable called a “function”?\nYou might find it confusing that we call a random variable a function - after all, we usually think of variables as containers that hold values, not as functions that produce them. But this terminology actually captures something important about how random variables work.\nA random variable is indeed a function, but with a specific purpose: it maps the possible outcomes of a random experiment to numerical values. Think of it as a systematic rule that converts whatever might happen into numbers we can analyze mathematically.\nConsider a concrete example. When flipping a coin twice, four outcomes are possible: HH, HT, TH, or TT. Now imagine we define a random variable \\(X\\) that counts the number of heads. This random variable works as a function by applying the same rule to each possible outcome:\n\n\\(X(HH)\\) = 2 (two heads)\n\\(X(HT)\\) = 1 (one head)\n\n\\(X(TH)\\) = 1 (one head)\n\\(X(TT)\\) = 0 (zero heads)\n\nNotice that \\(X\\) isn’t random in the sense of being unpredictable - it’s a fixed rule that always gives the same output for the same input. The randomness comes from not knowing which outcome will actually occur when we flip the coins. Once we know the outcome, the function X deterministically tells us what number to assign.\nThink of a random variable like a machine with a dial that can be set to different positions (representing possible outcomes). For each dial position, the machine displays a specific number according to its fixed programming. The machine’s function is predictable, but which position the dial lands on depends on the random process we’re studying.\nThis functional perspective explains why random variables are so powerful in business and management research. They allow us to transform complex, qualitative uncertain situations - like customer satisfaction, market conditions, or employee performance - into numerical values we can analyze using mathematical and statistical tools. The systematic nature of this transformation (the function) combined with uncertainty about outcomes (the randomness) gives us a rigorous way to study and make decisions about uncertain phenomena."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#examples-for-random-variables",
    "href": "content/statrecap/ProbabilityTheory/index.html#examples-for-random-variables",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Examples for random variables",
    "text": "Examples for random variables\nConsider these management scenarios as examples where random variables emerge naturally:\n\nExample 1: Customer satisfaction surveys represent a random process where each customer’s experience leads to a numerical rating. A random variable assigns values 1 through 10 to capture the phenomenon of satisfaction levels across your customer base.\n\n\nExample 2: Marketing campaign performance involves a random process where various factors (timing, message, audience, economic conditions) combine to produce a numerical outcome. A random variable might be the ROI percentage, which quantifies the phenomenon of campaign effectiveness.\n\n\nExample 3: Employee attendance involves a random process where personal, health, and motivational factors influence whether employees come to work. A random variable counts monthly sick days, capturing the phenomenon of workforce availability.\n\nNotice how each random variable transforms a complex, uncertain phenomenon into specific numbers we can analyze. This transformation is what makes statistical analysis possible."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#discrete-vs.-continuous-random-variables",
    "href": "content/statrecap/ProbabilityTheory/index.html#discrete-vs.-continuous-random-variables",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Discrete vs. Continuous Random Variables",
    "text": "Discrete vs. Continuous Random Variables\nRandom variables come in two main types:\nDiscrete random variables result from counting processes - they can only take specific, separated values. Customer satisfaction ratings (1, 2, 3, …, 10) and sick day counts (0, 1, 2, 3, …) are discrete because you cannot have fractional ratings or partial sick days.\nContinuous random variables result from measuring processes - they can take any value within a range. Marketing ROI could be 5.23%, 5.234%, or 5.2341%. These values represent points along a continuous spectrum. Continuous random variables are often used to represent quantities like time, weight, distance, or percentages.\nThe distinction between discrete and continuous random variables matters because discrete and continuous variables require different visualization techniques, different probability calculations, and different statistical tests."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#using-random-variables-in-r---a-first-glance",
    "href": "content/statrecap/ProbabilityTheory/index.html#using-random-variables-in-r---a-first-glance",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Using random variables in R - a first glance",
    "text": "Using random variables in R - a first glance\nOne way to use random variables in R is to make draws from a probability distribution. We will learn more about these distributions in the next section. Another way to use them is to use functions such as sample().\nThe function sample() allows you to draw random values from a specified vector of possible outcomes. This makes it particularly well-suited for discrete random variables.\n\n\nDiscrete RV in R using sample()\n# Simulating coin flips (discrete)\n# The possible outcomes, i.e. values the random variable can take:\ncoin_outcomes_possible &lt;- c(\"Heads\", \"Tails\") \n\ncoin_outcomes_actual &lt;- sample(\n  x = coin_outcomes_possible, # The vector from which to draw\n  size = 10, # The size of the sample you draw\n  replace = TRUE \n  # Draw with replacement (i.e. you can can draw \"Heads\" more than once)\n  )\n\n# Simulating dice rolls (discrete)\ndice_outcomes_possible &lt;- 1:6 # # The possible outcomes\n\ndice_outcomes_actual &lt;- sample(\n  x = dice_outcomes_possible, # The vector from which to draw\n  size = 5, # The size of the sample you draw\n  replace = TRUE # Draw with replacement\n  )\n\n# Sampling employees for a focus group (discrete, without replacement)\nemployee_ids &lt;- 1:50  # 50 employees in the department\nemployee_sample &lt;- sample(\n  x = employee_ids, # The vector from which to draw\n  size = 8, # The size of the sample you draw\n  replace = FALSE # Draw without replacement\n  )\n# Sampling without replacement here means once an employee is selected for \n#  the focus group, they cannot be selected again - just like in real life\n#  where you wouldn't invite the same person twice to the same meeting.\n\n\nIn the examples above, each element of the initial vector was equally likely to be drawn. But you can also specify different probabilities for each outcome using the argument prob. This allows you to model situations where outcomes are not equally likely:\n\n\nUsing different probabilities in sample()\n# Modeling customer purchase decisions with different probabilities:\n#   70% chance of \"No Purchase\", \n#   20% chance of \"Small Purchase\", \n#   10% chance of \"Large Purchase\"\npurchase_outcomes &lt;- c(\"No Purchase\", \"Small Purchase\", \"Large Purchase\")\npurchase_probabilities &lt;- c(0.7, 0.2, 0.1)\npurchases &lt;- sample(\n  x = purchase_outcomes, \n  size = 100, \n  replace = TRUE, \n  prob = purchase_probabilities\n  )\n\n\nThis weighted sampling reflects real business scenarios where some outcomes are naturally more common than others. For instance, in customer behavior analysis, you might observe that most visitors to your website don’t make a purchase, some make small purchases, and only a few make large purchases.\nWhile sample() technically works with discrete vectors, you can create the appearance of continuous sampling by providing a very fine-grained vector of values:\n\n\nUsing sample() for continuous RV in R\n# Approximating continuous values by sampling from many discrete points\nprices_possible &lt;- seq(10.00, 50.00, by = 0.01)  # Creates 4001 price points\nprices_actual &lt;- sample(\n  x = prices_possible, # The vector from which to draw, here almost continuous\n  size = 100, # The size of the sample you draw\n  replace = TRUE # Draw with replacement\n  )\n\n\nHowever, for true continuous random variables, R provides specialized functions for different probability distributions (like rnorm() for normal distributions, runif() for uniform distributions, etc.), which we’ll explore in detail when we discuss probability distributions."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#the-normal-distribution-natures-favorite-pattern",
    "href": "content/statrecap/ProbabilityTheory/index.html#the-normal-distribution-natures-favorite-pattern",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "The Normal Distribution: Nature’s Favorite Pattern",
    "text": "The Normal Distribution: Nature’s Favorite Pattern\nThere is one distribution that deserver special attention: The normal distribution This distribution appears remarkably often when many small, independent factors combine to influence an outcome. This isn’t mathematical coincidence - it’s a consequence of how complex systems work in the real world.\nThe theoretical normal distribution is characterized by: - Perfect symmetry around its center point - Most values clustering near the center - Probability decreasing smoothly toward the tails - A distinctive bell shape that appears throughout nature and business\nWhat do we mean by ‘theoretical’ normal distribution above? When we refer to a “theoretical” normal distribution, we mean the mathematically perfect, idealized version described by precise equations. This theoretical distribution has exact properties - perfect symmetry, infinite tails, and specific mathematical relationships between its parameters. Think of it as the mathematical blueprint or recipe for what a normal distribution should look like.\nIn contrast, when we collect real business data like our sales figures, we get an empirical distribution - actual observations from the real world. This empirical data can “approximate” the theoretical normal distribution, meaning it roughly follows the same bell-shaped pattern without being mathematically perfect. Real data might have slight asymmetries, finite ranges, or small irregularities due to measurement limitations, sample size, or the complex nature of business processes. The key insight is that even when real data isn’t perfectly normal, it often resembles the theoretical distribution closely enough that we can use normal distribution methods for analysis and prediction.\nTo illustrate this, in the following two examples we show both the empirical distribution using a histogram, as well as a close theoretical normal distribution, which was chosen to “fit” the data (we talk more about “fitting” a distributionlater).\n\nManagement Example: Employee performance ratings in large organizations often approximate normal distributions. This happens because performance results from many factors (skill, effort, training, luck, health, motivation) combining in complex ways. Most employees cluster around average performance, with fewer showing exceptional or poor performance.\n\n\nBusiness Example: Product defect rates in manufacturing often follow normal patterns when many small sources of variation (material quality, machine precision, worker attention, environmental conditions) combine to influence the final outcome.\n\n\n\nR code for the visualization\n# Example 1: Employee Performance Ratings\n# Generate realistic performance data that approximates normal distribution\nset.seed(123)  # For reproducible results\n\n# Create employee performance data (scale 1-100)\nperformance_data &lt;- tibble(\n  # Generate ratings with slight positive skew (more common in HR data)\n  # Most employees rated around 75-80, fewer at extremes\n  performance_rating = rnorm(n = 800, mean = 77, sd = 12) %&gt;%\n    # Bound the ratings between 1 and 100 (realistic HR scale)\n    pmax(1) %&gt;% pmin(100) %&gt;%\n    # Round to whole numbers (typical for performance reviews)\n    round()\n)\n\n# Calculate sample statistics to fit theoretical normal distribution\nsample_mean &lt;- mean(performance_data$performance_rating)\nsample_sd &lt;- sd(performance_data$performance_rating)\n\n# Create the visualization comparing empirical and theoretical distributions\np1 &lt;- ggplot(performance_data, aes(x = performance_rating)) +\n  # Empirical distribution (histogram)\n  geom_histogram(\n    aes(y = after_stat(density)),\n    bins = 20,\n    fill = \"#3498db\",\n    alpha = 0.7,\n    color = \"white\"\n  ) +\n  # Theoretical normal distribution overlay\n  stat_function(\n    fun = dnorm,\n    args = list(mean = sample_mean, sd = sample_sd),\n    color = \"#e74c3c\",\n    size = 1.5,\n    linetype = \"solid\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(40, 120, 10),\n    limits = c(40, 120)\n  ) +\n  scale_y_continuous(\n    labels = scales::label_number(accuracy = 0.001)\n  ) +\n  labs(\n    title = \"Employee Performance Ratings\",\n    subtitle = paste(\n      \"Empirical vs. Theoretical Distribution\\n\", \"Sample Mean =\", \n      round(sample_mean, 1), \", Sample SD =\", round(sample_sd, 1)),\n    x = \"Performance Rating (1-100 scale)\",\n    y = \"Density\",\n    caption = \"Blue bars: actual data\\nRed line: fitted normal distribution\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 13, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 11, color = \"gray60\", hjust = 0.5),\n    plot.caption = element_text(size = 9, color = \"gray60\"),\n    panel.grid.minor = element_blank()\n  )\n\n# Example 2: Manufacturing Defect Rates\n# Generate defect rate data (percentage) that approximates normal\nset.seed(456)\n\ndefect_data &lt;- tibble(\n  # Defect rates centered around 2.5% with some variation\n  # Using log-normal transformation to ensure positive values\n  defect_rate = exp(rnorm(n = 600, mean = log(2.5), sd = 0.3)) %&gt;%\n    # Cap at reasonable maximum (no batch has &gt;15% defects)\n    pmin(15) %&gt;%\n    # Round to realistic precision\n    round(2)\n)\n\n# Calculate sample statistics for theoretical fit\ndefect_mean &lt;- mean(defect_data$defect_rate)\ndefect_sd &lt;- sd(defect_data$defect_rate)\n\n# Create the second visualization\np2 &lt;- ggplot(defect_data, aes(x = defect_rate)) +\n  # Empirical distribution (histogram)\n  geom_histogram(\n    aes(y = after_stat(density)),\n    bins = 25,\n    fill = \"#27ae60\",\n    alpha = 0.7,\n    color = \"white\"\n  ) +\n  # Theoretical normal distribution overlay\n  stat_function(\n    fun = dnorm,\n    args = list(mean = defect_mean, sd = defect_sd),\n    color = \"#c0392b\",\n    size = 1.5,\n    linetype = \"solid\"\n  ) +\n  scale_x_continuous(\n    limits = c(-0.9, 6.5),\n    breaks = seq(0, 6, 2),\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  scale_y_continuous(\n    labels = scales::label_number(accuracy = 0.01)\n  ) +\n  labs(\n    title = \"Manufacturing Defect Rates\",\n    subtitle = paste0(\n      \"Empirical vs. Theoretical Distribution\\n\", \n      \"Sample Mean=\", round(defect_mean, 2), \n      \"%, Sample SD=\", round(defect_sd, 2), \"%\"),\n    x = \"Defect Rate per Batch\",\n    y = \"Density\",\n    caption = \"Green bars: actual data\\nRed line: fitted normal distribution\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 13, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 11, color = \"gray60\", hjust = 0.5),\n    plot.caption = element_text(size = 9, color = \"gray60\"),\n    panel.grid.minor = element_blank()\n  )\n\nggarrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#parameters-the-dials-that-control-distribution-shape",
    "href": "content/statrecap/ProbabilityTheory/index.html#parameters-the-dials-that-control-distribution-shape",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Parameters: The Dials That Control Distribution Shape",
    "text": "Parameters: The Dials That Control Distribution Shape\nNow that you understand what the normal distribution looks like, let’s explore how we can adjust its shape for different situations. Every probability distribution is governed by parameters - specific numbers that determine the distribution’s exact shape and characteristics. Parameters act like control dials on a stereo: change a parameter value, and you change the entire character of the distribution.\nUnderstanding parameters is crucial because they connect abstract mathematical distributions to concrete real-world phenomena. Different parameter values create different distributions that might describe the data from different situations.\nLet us stick to the example of the normal distribution for a bit longer. The normal distribution has two key parameters that completely determine its appearance:\nMean \\(\\mu\\): This parameter controls where the distribution is centered. The mean is the peak of the bell curve, the value around which all other values cluster. Change the mean, and you slide the entire distribution left or right without changing its shape.\nStandard deviation \\(\\sigma\\): This parameter controls how spread out the distribution is. A smaller standard deviation creates a narrow, tall bell curve where values cluster tightly around the mean. A larger standard deviation creates a wider, flatter bell curve where values are more dispersed.\n\n\nR code for the visualization\n# Create a range of x values for smooth curves\nx_values &lt;- seq(-10, 20, length.out = 1000)\n\n# Define four different normal distributions to showcase parameter effects\ndistributions &lt;- tibble(\n  # Create all combinations of x values with distribution parameters\n  x = rep(x_values),\n  \n  # Distribution 1: Small mean, small standard deviation (narrow, left-centered)\n  density_1 = dnorm(x_values, mean = 2, sd = 1),\n  \n  # Distribution 2: Small mean, large standard deviation (wide, left-centered)  \n  density_2 = dnorm(x_values, mean = 2, sd = 3),\n  \n  # Distribution 3: Large mean, small standard deviation (narrow, right-centered)\n  density_3 = dnorm(x_values, mean = 10, sd = 1),\n  \n  # Distribution 4: Large mean, large standard deviation (wide, right-centered)\n  density_4 = dnorm(x_values, mean = 10, sd = 3)\n) %&gt;%\n  # Reshape data for ggplot (convert from wide to long format)\n  pivot_longer(\n    cols = starts_with(\"density_\"),\n    names_to = \"distribution\",\n    values_to = \"density\",\n    names_prefix = \"density_\"\n  ) %&gt;%\n  # Add descriptive labels that explain each distribution's parameters\n  mutate(\n    distribution_label = case_when(\n      distribution == \"1\" ~ \"μ = 2, σ = 1\\n(Small mean, small SD)\",\n      distribution == \"2\" ~ \"μ = 2, σ = 3\\n(Small mean, large SD)\", \n      distribution == \"3\" ~ \"μ = 10, σ = 1\\n(Large mean, small SD)\",\n      distribution == \"4\" ~ \"μ = 10, σ = 3\\n(Large mean, large SD)\"\n    ),\n    # Create factor with logical ordering for facets\n    distribution_label = factor(distribution_label, levels = c(\n      \"μ = 2, σ = 1\\n(Small mean, small SD)\",\n      \"μ = 2, σ = 3\\n(Small mean, large SD)\",\n      \"μ = 10, σ = 1\\n(Large mean, small SD)\",\n      \"μ = 10, σ = 3\\n(Large mean, large SD)\"\n    ))\n  )\n\n# Create the four-panel visualization\nggplot(distributions, aes(x = x, y = density)) +\n  # Draw the normal distribution curves\n  geom_line(\n    aes(color = distribution_label),\n    size = 1.2,\n    alpha = 0.9\n  ) +\n  # Add area under curves for better visual impact\n  geom_area(\n    aes(fill = distribution_label),\n    alpha = 0.3\n  ) +\n  # Add vertical lines at the means to emphasize centering\n  geom_vline(\n    data = tibble(\n      distribution_label = factor(c(\n        \"μ = 2, σ = 1\\n(Small mean, small SD)\",\n        \"μ = 2, σ = 3\\n(Small mean, large SD)\",\n        \"μ = 10, σ = 1\\n(Large mean, small SD)\",\n        \"μ = 10, σ = 3\\n(Large mean, large SD)\"\n      ), levels = c(\n        \"μ = 2, σ = 1\\n(Small mean, small SD)\",\n        \"μ = 2, σ = 3\\n(Small mean, large SD)\",\n        \"μ = 10, σ = 1\\n(Large mean, small SD)\",\n        \"μ = 10, σ = 3\\n(Large mean, large SD)\"\n      )),\n      mean_value = c(2, 2, 10, 10)\n    ),\n    aes(xintercept = mean_value),\n    linetype = \"dashed\",\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  # Create separate panels for each distribution\n  facet_wrap(~ distribution_label, scales = \"free_y\", ncol = 2) +\n  # Define custom colors that are distinct but harmonious\n  scale_color_manual(values = c(\n    \"μ = 2, σ = 1\\n(Small mean, small SD)\" = \"#e74c3c\",\n    \"μ = 2, σ = 3\\n(Small mean, large SD)\" = \"#3498db\",\n    \"μ = 10, σ = 1\\n(Large mean, small SD)\" = \"#27ae60\",\n    \"μ = 10, σ = 3\\n(Large mean, large SD)\" = \"#9b59b6\"\n  )) +\n  scale_fill_manual(values = c(\n    \"μ = 2, σ = 1\\n(Small mean, small SD)\" = \"#e74c3c\",\n    \"μ = 2, σ = 3\\n(Small mean, large SD)\" = \"#3498db\", \n    \"μ = 10, σ = 1\\n(Large mean, small SD)\" = \"#27ae60\",\n    \"μ = 10, σ = 3\\n(Large mean, large SD)\" = \"#9b59b6\"\n  )) +\n  # Customize axis formatting\n  scale_x_continuous(\n    breaks = seq(-5, 15, 5),\n    limits = c(-8, 20)\n  ) +\n  scale_y_continuous(\n    labels = scales::label_number(accuracy = 0.01)\n  ) +\n  labs(\n    title = \"How Mean and Standard Deviation Shape Normal Distributions\",\n    x = \"Value\",\n    y = \"Probability Density\",\n    caption = \"Dashed lines show the mean (μ) of each distribution\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    strip.text = element_text(size = 11, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\", margin = margin(b = 5)),\n    plot.subtitle = element_text(size = 12, color = \"gray60\", margin = margin(b = 15)),\n    plot.caption = element_text(size = 10, color = \"gray60\", margin = margin(t = 10)),\n    axis.title = element_text(size = 11),\n    axis.text = element_text(size = 10),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color = \"gray90\", size = 0.3)\n  )\n\n\n\n\n\n\n\n\n\nTo write that we are talking about a random variable \\(X\\) that follows a normal distribution with particular values for \\(\\mu\\) and \\(\\sigma\\) we often write \\[X \\sim \\mathcal{N}\\left(\\mu,  \\sigma\\right) \\] for the general case or\n\\[X \\sim \\mathcal{N}\\left(2,1\\right) \\]\nfor the case with concrete values for \\(\\mu\\) and \\(\\sigma\\).\nLet us now look now at a real world example where we can use the normal distribution with two different parameter constellations to “fit” the data. Note that “fitting” here refers to the process of choosing those parameter values that maximize the similarity between the theoretical probability distribution and the empirical distribution of the data.\n\nExample: Consider two different business scenarios:\n\nCustomer satisfaction scores might be distributed such that the best fit of a normal distribution is achieved if we choose \\(\\mu=7.5\\) and \\(\\sigma=1.2\\). This means satisfaction centers around 7.5, with most scores falling between roughly 6 and 9.\nMonthly sales revenue might be roughly follow a normal distribution with \\(\\mu=50,000\\) and \\(\\sigma=8,000\\) (in EUR), such that we should choose these values for a theoretical distribution to get the best fit.\n\n\n\n\nR code for the visualization\n# Set seed for reproducible results\nset.seed(789)\n\n# Generate realistic customer satisfaction data\n# We'll create data that naturally centers around 7.5 with spread of 1.2\ncustomer_satisfaction &lt;- tibble(\n  # Generate satisfaction scores with slight boundary effects\n  # (scores can't go below 1 or above 10 on typical scales)\n  satisfaction_score = rnorm(n = 400, mean = 7.5, sd = 1.2) %&gt;%\n    # Apply realistic bounds for satisfaction surveys\n    pmax(1) %&gt;% pmin(10) %&gt;%\n    # Round to one decimal place (typical for survey scales)\n    round(1)\n)\n\n# Generate realistic monthly sales revenue data\n# Create data that centers around €50,000 with spread of €8,000\nmonthly_sales &lt;- tibble(\n  # Generate sales figures with business-realistic constraints\n  sales_revenue = rnorm(n = 350, mean = 50000, sd = 8000) %&gt;%\n    # Ensure no negative sales (impossible in practice)\n    pmax(10000) %&gt;%\n    # Round to nearest 100 (realistic for business reporting)\n    round(-2)  # -2 rounds to nearest hundred\n)\n\n# Calculate actual sample statistics to verify our fit\nsatisfaction_stats &lt;- customer_satisfaction %&gt;%\n  summarise(\n    sample_mean = mean(satisfaction_score),\n    sample_sd = sd(satisfaction_score)\n  )\n\nsales_stats &lt;- monthly_sales %&gt;%\n  summarise(\n    sample_mean = mean(sales_revenue),\n    sample_sd = sd(sales_revenue)\n  )\n\n# Create visualization for customer satisfaction\np1 &lt;- ggplot(customer_satisfaction, aes(x = satisfaction_score)) +\n  # Empirical distribution using histogram\n  geom_histogram(\n    aes(y = after_stat(density)),\n    bins = 18,  # Good resolution for satisfaction scale  \n    fill = \"#3498db\",\n    alpha = 0.7,\n    color = \"white\",\n    boundary = 1  # Align bins with whole numbers\n  ) +\n  # Overlay the fitted theoretical normal distribution\n  stat_function(\n    fun = dnorm,\n    args = list(mean = 7.5, sd = 1.2),\n    color = \"#e74c3c\",\n    size = 1.5,\n    linetype = \"solid\"\n  ) +\n  # Add vertical lines to mark mean and one standard deviation\n  geom_vline(\n    xintercept = 7.5,\n    color = \"#2c3e50\",\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  geom_vline(\n    xintercept = c(7.5 - 1.2, 7.5 + 1.2),\n    color = \"#95a5a6\",\n    linetype = \"dotted\",\n    alpha = 0.8\n  ) +\n  # Format x-axis for satisfaction scale\n  scale_x_continuous(\n    breaks = 1:10,\n    limits = c(1, 10)\n  ) +\n  scale_y_continuous(\n    labels = scales::label_number(accuracy = 0.01)\n  ) +\n  # Add comprehensive labels with statistical details\n  labs(\n    title = \"Customer Satisfaction Scores\",\n    subtitle = \"Empirical Data vs. Fitted Normal Distribution\",\n    x = \"Customer Satisfaction Score (1-10 scale)\",\n    y = \"Probability Density\",\n    caption = \"Blue bars: actual survey data \\n Red line: N(7.5, 1.2) | Dashed: mean | Dotted: ±1 SD\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 13, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 11, color = \"gray60\", hjust = 0.5),\n    plot.caption = element_text(size = 9, color = \"gray60\"),\n    axis.title = element_text(size = 11),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(color = \"gray90\", size = 0.3)\n  )\n\n# Create visualization for monthly sales revenue\np2 &lt;- ggplot(monthly_sales, aes(x = sales_revenue)) +\n  # Empirical distribution using histogram\n  geom_histogram(\n    aes(y = after_stat(density)),\n    bins = 20,\n    fill = \"#27ae60\",\n    alpha = 0.7,\n    color = \"white\"\n  ) +\n  # Overlay the fitted theoretical normal distribution\n  stat_function(\n    fun = dnorm,\n    args = list(mean = 50000, sd = 8000),\n    color = \"#c0392b\",\n    size = 1.5,\n    linetype = \"solid\"\n  ) +\n  # Add vertical lines to mark mean and one standard deviation\n  geom_vline(\n    xintercept = 50000,\n    color = \"#2c3e50\",\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  geom_vline(\n    xintercept = c(50000 - 8000, 50000 + 8000),\n    color = \"#95a5a6\",\n    linetype = \"dotted\",\n    alpha = 0.8\n  ) +\n  # Format x-axis for currency values\n  scale_x_continuous(\n    labels = scales::label_number(\n      scale = 1/1000,\n      suffix = \"k\",\n      accuracy = 1\n    ),\n    breaks = scales::pretty_breaks(n = 6)\n  ) +\n  scale_y_continuous(\n    labels = scales::label_scientific(digits = 2)\n  ) +\n  # Add comprehensive labels with statistical details\n  labs(\n    title = \"Monthly Sales Revenue: \",\n    subtitle = \"Empirical Data vs. Fitted Normal Distribution\",\n    x = \"Monthly Sales Revenue (EUR)\",\n    y = \"Probability Density\", \n    caption = \"Green bars: actual sales data \\n Red line: N(50000, 8000²) | Dashed: mean | Dotted: ±1 SD\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 13, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 11, color = \"gray60\", hjust = 0.5),\n    plot.caption = element_text(size = 9, color = \"gray60\"),\n    axis.title = element_text(size = 11),\n    panel.grid.minor = element_blank()\n  )\n\nggarrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\nThe ability to adjust parameters means you can fit the normal distribution to match the specific characteristics of your data. In practice, you’ll estimate these parameters from your sample data, then use the fitted distribution to make predictions about future observations or the broader population. But this is the topic of the next chapters."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#why-focus-on-the-normal-distribution",
    "href": "content/statrecap/ProbabilityTheory/index.html#why-focus-on-the-normal-distribution",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Why Focus on the Normal Distribution?",
    "text": "Why Focus on the Normal Distribution?\nYou might wonder why it is always the normal distribution that is used as the basic example for probability distributions almost everywhere? While it might true that the use of the normal distribution is sometimes excessive and even misleading, there are some good reasons for why it is often (but not always) a very good application case:\nUbiquity in business data: Many measurements in management research approximate normal distributions, especially when multiple factors influence outcomes. This makes it a practical starting point for many analyses.\nMathematical tractability: The normal distribution has elegant mathematical properties that make statistical calculations manageable and formulas interpretable. This is why it appears so frequently in statistical methods.\nFoundation for inference: The Central Limit Theorem (which we’ll discuss later) shows that sample means tend toward normal distributions regardless of the underlying population shape, making the normal distribution central to statistical inference.\nBenchmark for comparison: Understanding the normal distribution helps you recognize when your data deviates from this pattern, often revealing important insights about underlying business processes.\nStill, you should be aware of the fact that the normal distribution is often used also for situations in which it is not the best choice and might even be misleading. You can find information about other distributions below in Section 8."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#working-with-the-normal-distribution-in-r",
    "href": "content/statrecap/ProbabilityTheory/index.html#working-with-the-normal-distribution-in-r",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Working with the normal distribution in R",
    "text": "Working with the normal distribution in R\nThere are three important functions that you might use in R when working with the normal distribution: dnorm(), pnorm(), and rnorm()."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#working-with-the-normal-distribution-in-r-1",
    "href": "content/statrecap/ProbabilityTheory/index.html#working-with-the-normal-distribution-in-r-1",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Working with the normal distribution in R",
    "text": "Working with the normal distribution in R\nThere are three important functions that you might use in R when working with the normal distribution: dnorm(), pnorm(), and rnorm(). Each function takes the same basic arguments: the value(s) of interest, the mean (mean), and standard deviation (sd). By default, they assume the standard normal distribution (mean = 0, sd = 1), but you can specify any normal distribution by adjusting these parameters.\nThink of these functions as three different ways to interact with the normal distribution, each serving a distinct purpose in your analysis:\n\ndnorm() (for density of the normal) calculates the height of the normal curve at any given point. This function gives you the probability density, which tells you how likely values are in that region of the distribution. You use this when you want to know how “concentrated” the probability is at a specific value, or when you’re creating smooth curves for visualization.\n\n\n\nR example code\n# Basic usage with standard normal distribution (mean=0, sd=1)\ndnorm(0)    # Height at the peak (mean)\ndnorm(1)    # Height one unit to the right of mean\ndnorm(-1)   # Height one unit to the left of mean\n\n# Customer satisfaction example: Normal distribution with mean=7.5, sd=1.2\n# How dense is the probability around a score of 8?\ndnorm(8, mean = 7.5, sd = 1.2)\n\n# You can calculate densities for multiple values at once\nsatisfaction_scores &lt;- c(5, 6.5, 7.5, 8.5, 10)\ndnorm(satisfaction_scores, mean = 7.5, sd = 1.2)\n\n# This is particularly useful for creating smooth curves in plots\nggplot() +\n  stat_function(\n    fun = dnorm, args = list(mean = 7.5, sd = 1.2), xlim = c(4, 11)\n  ) +\n  labs(\n    title = \"Customer Satisfaction Distribution\",\n    x = \"Satisfaction Score\", y = \"Density\") + \n  theme_linedraw()\n\n\n\npnorm() (for probability of the normal) calculates cumulative probabilities, answering questions like “What’s the probability that a randomly selected value is less than or equal to X?”. This is equivalent to calculating the area under the normal curve up to that point. So this function is your go-to tool for computing areas under the normal curve, which correspond to actual probabilities of events occurring.\n\n\n\nR example code\n# Standard normal examples\npnorm(0)    # Probability of getting 0 or less (exactly 0.5)\npnorm(1)    # Probability of getting 1 or less (about 0.84)\npnorm(-1)   # Probability of getting -1 or less (about 0.16)\n\n# Business application: Monthly sales with mean=€50,000, sd=€8,000\n# What's the probability that monthly sales are €45,000 or less?\npnorm(45000, mean = 50000, sd = 8000)\n\n# What's the probability of sales exceeding €60,000?\n# Remember: P(X &gt; 60000) = 1 - P(X ≤ 60000)\n1 - pnorm(60000, mean = 50000, sd = 8000)\n\n# Probability of sales falling between €45,000 and €55,000\n# P(45000 &lt; X &lt; 55000) = P(X ≤ 55000) - P(X ≤ 45000)\npnorm(55000, mean = 50000, sd = 8000) - pnorm(45000, mean = 50000, sd = 8000)\n\n# Customer satisfaction: Probability of score above 8.5\n1 - pnorm(8.5, mean = 7.5, sd = 1.2)\n\n\n\nqnorm() (for quantiles of the normal) works as the inverse of pnorm(), answering the opposite question: “Given a probability (or percentile), what value corresponds to that point in the distribution?” Think of it as finding the boundary values that separate different portions of your data. For instance, while pnorm() tells you the probability of scoring below a certain value, qnorm() tells you what score you need to achieve to be in the top 10% of performers. This function is essential for setting thresholds, identifying outliers, and understanding percentile ranks in business contexts. When you want to know what sales figure represents the 90th percentile of performance, or what score puts an employee in the bottom 5% for improvement planning, qnorm() provides those critical boundary values.\n\n\n\nR example code\n# Basic usage: What value corresponds to the 90th percentile?\nqnorm(0.9, mean = 7.5, sd = 1.2)  # Customer satisfaction: top 10%\n\n# Finding cutoff scores for performance ratings\nqnorm(0.25, mean = 75, sd = 12)  # Bottom 25% (needs improvement)\nqnorm(0.75, mean = 75, sd = 12)  # Top 25% (high performers)\n\n# Sales thresholds: What revenue puts you in top 5%?\nqnorm(0.95, mean = 50000, sd = 8000)\n\n# Finding values for symmetric intervals\nqnorm(c(0.025, 0.975), mean = 100, sd = 15)  # Middle 95% boundaries\n\n# Setting quality control limits (3-sigma rule)\nqnorm(c(0.00135, 0.99865), mean = 2.5, sd = 0.3)  # Defect rate limits\n\n# Notice how qnorm() essentially reverses the logic of pnorm(). \n# While pnorm(8.5, mean = 7.5, sd = 1.2) tells you what percentage of customers \n# score 8.5 or below, qnorm(0.9, mean = 7.5, sd = 1.2) tells you what score \n# puts a customer in the 90th percentile. This makes qnorm() particularly \n# valuable for setting benchmarks, identifying outliers, and establishing \n# performance thresholds in business contexts.\n\n\n\nrnorm() (for random number from the normal) generates random samples from a normal distribution. This function is important for creating example data for teaching purposes, or testing statistical methods under known conditions.\n\n\n\nR example code\n# Generate 10 random values from standard normal distribution\nrnorm(10)\n\n# Generate 100 customer satisfaction scores\n# with realistic parameters (mean=7.5, sd=1.2)\nset.seed(123)  # For reproducible results\ncustomer_scores &lt;- rnorm(100, mean = 7.5, sd = 1.2)\nhead(customer_scores)  # Look at first few values\n\n# Generate monthly sales data for a year (12 months)\nmonthly_sales &lt;- rnorm(12, mean = 50000, sd = 8000)\nmonthly_sales\n\n# Create a larger sample for simulation study\nset.seed(456)\nlarge_sample &lt;- rnorm(1000, mean = 7.5, sd = 1.2)\n\n# Verify our simulation matches the theoretical parameters\nmean(large_sample)    # Should be close to 7.5\nsd(large_sample)      # Should be close to 1.2\n\n# Visualize the simulated data\nhist(large_sample, breaks = 30, \n     main = \"Simulated Customer Satisfaction Scores\",\n     xlab = \"Satisfaction Score\", \n     col = \"lightblue\", border = \"white\")\n\n\n\n\nExample for combined use of all three functions\n# Scenario: Analyzing employee productivity scores (scale 0-100)\n# Assume scores follow Normal(μ = 75, σ = 12)\n\n# 1. Use rnorm() to simulate realistic data\nset.seed(789)\nproductivity_scores &lt;- rnorm(250, mean = 75, sd = 12)\n\n# 2. Use dnorm() to create theoretical comparison\nscore_range &lt;- seq(40, 110, by = 1)\ntheoretical_density &lt;- dnorm(score_range, mean = 75, sd = 12)\n\n# 3. Use pnorm() and qnorm() to answer business questions\n# What percentage of employees score above 85?\nhigh_performers &lt;- 1 - pnorm(85, mean = 75, sd = 12)\n\n# What score represents the 90th percentile? \npercentile_90 &lt;- qnorm(0.9, mean = 75, sd = 12)"
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#expected-value-vs.-typical-value",
    "href": "content/statrecap/ProbabilityTheory/index.html#expected-value-vs.-typical-value",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Expected Value vs. Typical Value",
    "text": "Expected Value vs. Typical Value\nA common misconception is confusing expected value with the most likely outcome or a typical observation. These can be quite different:\n\nExample: When rolling a standard die, the expected value is 3.5 (calculated as 1/6 × 1 + 1/6 × 2 + … + 1/6 × 6 = 3.5). However, you cannot actually roll 3.5! The expected value represents the average across many rolls, not the outcome of any single roll.\n\nThis distinction is vital for business planning. Expected revenue might be 50,000 EUR, but individual months might typically range from 30,000 EUR to 70,000 eUR, with the average working out to 50,000 EUR over time."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#properties-of-expected-value",
    "href": "content/statrecap/ProbabilityTheory/index.html#properties-of-expected-value",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Properties of Expected Value",
    "text": "Properties of Expected Value\nExpected values have mathematical properties that make them particularly useful for business analysis:\nLinearity: If you multiply all outcomes by a constant or add a constant to all outcomes, the expected value changes in the same predictable way. This helps with currency conversions, scaling, and adjusting for inflation.\nAdditivity for independent variables: When random variables are independent, the expected value of their sum equals the sum of their expected values. This property is invaluable when combining multiple uncertain factors in financial projections.\nLinearity: If you multiply all outcomes by a constant or add a constant to all outcomes, the expected value changes in the same predictable way. This helps with currency conversions, scaling, and adjusting for inflation.\n\nExample: Your company operates in both Germany and the United States, and you need to convert expected quarterly profits from euros to dollars. If your expected quarterly profit in Germany is 150,000 EUR and the exchange rate is 1.10 USD per Euro, you can simply multiply: \\[\\mathbb{E}[Profit_{\\text{USD}}] = 1.10 × 150,000 EUR = 165,000 USD\\] Similarly, if you need to account for a fixed quarterly tax of 20,000 USD, you add it directly: \\[\\mathbb{E}[Profit_{\\text{AfterTax}}] = 165,000 USD + 20,000 USD = 185,000 USD\\] The linearity property ensures that these transformations preserve the expected value relationship, making currency conversions and cost adjustments straightforward in your financial planning.\n\nAdditivity for independent variables: When random variables are independent, the expected value of their sum equals the sum of their expected values. This property is invaluable when combining multiple uncertain factors in financial projections.\n\nExample: When planning next year’s budget, you’re combining revenues from three independent business units: online sales (expected 500,000 EUR), retail stores (expected 300,000 EUR), and consulting services (expected 150,000 EUR). Because these revenue streams are independent, you can calculate the total expected revenue by simply adding the individual expected values: \\[\\mathbb{E}[Total Revenue] = 500,000 EUR + 300,000 EUR + 150,000 EUR = 950,000 EUR\\] This additivity property allows you to build complex financial models by breaking them into independent components, calculating expectations for each piece separately, and then combining them to get the overall expected outcome for your business.\n\nThese properties allow managers to break complex uncertain situations into manageable components and combine them systematically."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#properties-of-expected-value-1",
    "href": "content/statrecap/ProbabilityTheory/index.html#properties-of-expected-value-1",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Properties of Expected Value",
    "text": "Properties of Expected Value\nExpected values have mathematical properties that make them particularly useful for business analysis:\nLinearity: If you multiply all outcomes by a constant or add a constant to all outcomes, the expected value changes in the same predictable way. This helps with currency conversions, scaling, and adjusting for inflation.\n\nExample: Your company operates in both Germany and the United States, and you need to convert expected quarterly profits from euros to dollars. If your expected quarterly profit in Germany is 150,000 EUR and the exchange rate is 1.10 USD per Euro, you can simply multiply: \\[\\mathbb{E}[Profit_{\\text{USD}}] = 1.10 × 150,000 EUR = 165,000 USD\\] Similarly, if you need to account for a fixed quarterly tax of 20,000 USD, you add it directly: \\[\\mathbb{E}[Profit_{\\text{AfterTax}}] = 165,000 USD + 20,000 USD = 185,000 USD\\] The linearity property ensures that these transformations preserve the expected value relationship, making currency conversions and cost adjustments straightforward in your financial planning.\n\nAdditivity for independent variables: When random variables are independent, the expected value of their sum equals the sum of their expected values. This property is invaluable when combining multiple uncertain factors in financial projections.\n\nExample: When planning next year’s budget, you’re combining revenues from three independent business units: online sales (expected 500,000 EUR), retail stores (expected 300,000 EUR), and consulting services (expected 150,000 EUR). Because these revenue streams are independent, you can calculate the total expected revenue by simply adding the individual expected values: \\[\\mathbb{E}[Total Revenue] = 500,000 EUR + 300,000 EUR + 150,000 EUR = 950,000 EUR\\] This additivity property allows you to build complex financial models by breaking them into independent components, calculating expectations for each piece separately, and then combining them to get the overall expected outcome for your business.\n\nThese properties allow managers to break complex uncertain situations into manageable components and combine them systematically."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#understanding-the-connection-between-sample-and-population",
    "href": "content/statrecap/ProbabilityTheory/index.html#understanding-the-connection-between-sample-and-population",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Understanding the Connection Between Sample and Population",
    "text": "Understanding the Connection Between Sample and Population\nThe relationship between descriptive statistics and probability becomes clear when we distinguish between what we observe in our sample data and what we want to understand about the broader population or underlying process.\nWhen you calculate the mean of customer satisfaction scores from 100 surveyed customers, you’re computing a sample mean using descriptive statistics. But the expected value of customer satisfaction represents the theoretical average you would get if you could survey all customers infinitely many times. These concepts are intimately related - your sample mean serves as an estimate of the expected value.\n\nBusiness Example: Imagine you’re analyzing employee productivity scores for 50 employees in your department. You calculate a sample mean of 85 points. This descriptive statistic summarizes your observed data. However, the expected value of productivity scores for all employees in similar departments represents the theoretical average you’re trying to estimate. Your sample mean of 85 points is your best estimate of this expected value, though you recognize it might differ somewhat due to sampling variation."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#the-estimation-connection",
    "href": "content/statrecap/ProbabilityTheory/index.html#the-estimation-connection",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "The Estimation Connection",
    "text": "The Estimation Connection\nDescriptive statistics serve as estimates of probability concepts. When we calculate sample statistics, we’re making educated guesses about the corresponding population parameters or probability characteristics:\nSample statistics estimate population parameters: Your sample mean can be used as an estimator for the population mean (which equals the expected value for the population). Your sample standard deviation can be used as an estimator for the population standard deviation (a parameter of the population’s probability distribution).\nSample distributions approximate theoretical distributions: When you create a histogram of your sample data, you’re approximating what the true probability distribution might look like. The larger your sample, the better this approximation typically becomes."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#two-perspectives-on-the-same-data",
    "href": "content/statrecap/ProbabilityTheory/index.html#two-perspectives-on-the-same-data",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Two Perspectives on the Same Data",
    "text": "Two Perspectives on the Same Data\nConsider the same dataset from two viewpoints. You survey 200 customers about their monthly spending and find the average is 347 EUR with a standard deviation of 89 EUR.\nFrom a descriptive statistics perspective, you’re summarizing what happened: “These 200 customers spent an average of 347 EUR, with spending typically varying by about 89 EUR from this average.”\nFrom a probability perspective, you’re making inferences: “Based on this sample, we estimate the expected monthly spending per customer is approximately 347 EUR, and the underlying spending distribution appears to have a standard deviation of about 89 EUR. This suggests future customers will likely spend around this amount, with similar variability.”"
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#a-comparison-table",
    "href": "content/statrecap/ProbabilityTheory/index.html#a-comparison-table",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "A Comparison Table",
    "text": "A Comparison Table\n\n\n\n\n\n\n\n\nProbability Concept\nDescriptive Statistic\nRelationship & Interpretation\n\n\n\n\nExpected Value (μ)\nSample Mean (x̄)\nThe sample mean serves as an estimator for the expected value. As sample size increases, this estimator converges to the true expected value by the Law of Large Numbers\n\n\nPopulation Variance (\\(var\\))\nSample Variance (\\(\\sigma^2\\))\nSample variance serves as an estimator for population variance. Both measure spread, but sample version adjusts for estimation uncertainty\n\n\nProbability Distribution\nSample Distribution (Histogram)\nSample histogram approximates the shape of the probability distribution. More data yields better approximation\n\n\nPopulation Median\nSample Median\nSample median serves as an estimator for the population median. Both represent “middle” values, but sample version depends on specific data points\n\n\nTheoretical Quartiles\nSample Quartiles (Q1, Q3)\nSample quartiles serve as estimators for theoretical quartiles. Both divide data into quarters for analysis\n\n\nProbability (P(X = x))\nRelative Frequency\nSample relative frequency serves as an estimator for probability. Proportion of sample with specific value estimates probability of that value occurring\n\n\n\nNote that the estimators mentioned above are not necessarily the best estimators you can use to estimate a population property of interest. How to come up with the best estimators is a question of inferential statistics."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#from-description-to-prediction",
    "href": "content/statrecap/ProbabilityTheory/index.html#from-description-to-prediction",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "From Description to Prediction",
    "text": "From Description to Prediction\nUnderstanding these connections transforms how you think about data analysis and prepares you for inferential statistics in the next chapters. Descriptive statistics tell you what happened in your sample, but probability concepts help you predict what might happen in the future or with different samples.\nWhen you calculate that the average customer spends 347 EUR, you’re not bound to just describing past behavior - you can estimate a parameter that helps you predict future customer behavior and forms the basis for confidence intervals about the true population mean. When you observe that spending appears normally distributed in your sample, you’re gathering evidence about the probability distribution that generates customer spending patterns, which will inform hypothesis tests about population characteristics."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#discrete-distributions",
    "href": "content/statrecap/ProbabilityTheory/index.html#discrete-distributions",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Discrete Distributions",
    "text": "Discrete Distributions\nBinomial Distribution The binomial distribution models the number of successes in a fixed number of independent trials, each with the same probability of success.\nParameters:\n\n\\(n\\) (number of trials)\n\\(p\\) (probability of success per trial)\n\nBusiness applications:\n\nQuality control (number of defective items in a batch)\nMarketing (number of customers who respond to an email campaign)\nEmployee behavior (number of employees who attend optional training)\nSurvey research (number of positive responses)\n\n\nExample: If 30% of customers typically purchase after viewing a product demo, and you show demos to 50 customers, the binomial distribution tells you the probability of getting exactly 12, 15, or 20 purchases.\n\nPoisson Distribution\nThe Poisson distribution models the number of events occurring in a fixed interval when events happen independently at a constant average rate.\nParameters:\n\n\\(\\lambda\\) (lambda, the average rate of occurrence)\n\nBusiness applications:\n\nCustomer arrivals (number of customers entering a store per hour)\nSystem failures (number of server crashes per month)\nCall center volume (number of support calls per day)\nDefect counting in manufacturing\n\n\nExample: If your website averages 3 crashes per month, the Poisson distribution helps you calculate the probability of experiencing 0, 1, 2, or 5 crashes in a given month."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#continuous-distributions",
    "href": "content/statrecap/ProbabilityTheory/index.html#continuous-distributions",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Continuous Distributions",
    "text": "Continuous Distributions\nUniform Distribution The uniform distribution assigns equal probability to all values within a specified range, representing complete uncertainty within known bounds.\nParameters:\n\n\\(a\\) (minimum value)\n\\(b\\) (maximum value)\n\nBusiness applications: Monte Carlo simulations, modeling worst-case scenarios where you know only the possible range, random sampling for A/B testing, representing complete uncertainty about timing within a known window\n\nExample: If project completion time could be anywhere between 30 and 50 days with no particular preference, the uniform distribution represents this complete uncertainty within the known bounds.\n\nExponential Distribution The exponential distribution models the time between events in a Poisson process, or the duration until something happens.\nParameters:\n\n\\(\\lambda\\) (rate parameter)\n\nBusiness applications:\n\nCustomer service (time until next service request)\nProduct reliability (time until failure)\nQueue management (waiting times)\nModeling time between purchases\n\n\nExample: If customers arrive at your service desk following a Poisson process, the exponential distribution models how long you’ll wait between consecutive arrivals.\n\nBeta Distribution The beta distribution is highly flexible and bounded between 0 and 1, making it ideal for modeling proportions and percentages.\nParameters:\n\n\\(\\alpha\\) (alpha)\n\\(\\beta\\) (beta)\n\nBusiness applications:\n\nMarket share analysis\nProject completion percentages\nConversion rates\nBudget allocation proportions\nMoeling success probabilities when you have prior information\n\n\nExample: When modeling the proportion of budget different departments might receive, the beta distribution can represent various scenarios from equal allocation to highly skewed distributions.\n\nGamma Distribution\nThe gamma distribution models positive continuous values and includes the exponential distribution as a special case. It’s particularly useful for modeling sums of exponential random variables.\nParameters:\n\n\\(\\alpha\\) (shape) and \\(\\beta\\) (rate) or\n\\(\\alpha\\) (shape) and \\(\\theta\\) (scale)\n\nBusiness applications:\n\nProject duration modeling when projects consist of multiple phases\nIncome distribution analysis\nInsurance claim amounts\nInventory management (modeling demand over lead time)\n\n\nExample: Total project time when the project consists of several independent phases, each following an exponential distribution, results in a gamma distribution."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#visual-illustration",
    "href": "content/statrecap/ProbabilityTheory/index.html#visual-illustration",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Visual illustration",
    "text": "Visual illustration\n\n\nR code for the visualization\n# Set up common theme for all plots\ncustom_theme &lt;- theme_minimal() +\n  theme(\n    plot.title = element_text(size = 12, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, color = \"gray60\"),\n    axis.title = element_text(size = 10),\n    axis.text = element_text(size = 8),\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 9),\n    legend.text = element_text(size = 8)\n  )\n\n# Common color palette for consistency\ncolors &lt;- c(\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#9b59b6\")\n\n# 1. BINOMIAL DISTRIBUTION\n# Create data for different parameter combinations\nbinomial_data &lt;- expand_grid(\n  # Different scenarios: small n with different p, large n with different p\n  scenario = c(\"n=10, p=0.3\", \"n=10, p=0.7\", \"n=50, p=0.3\", \"n=50, p=0.7\"),\n  x = 0:50\n) %&gt;%\n  mutate(\n    # Extract parameters for calculation\n    n = case_when(\n      str_detect(scenario, \"n=10\") ~ 10,\n      str_detect(scenario, \"n=50\") ~ 50\n    ),\n    p = case_when(\n      str_detect(scenario, \"p=0.3\") ~ 0.3,\n      str_detect(scenario, \"p=0.7\") ~ 0.7\n    ),\n    # Calculate probabilities for valid range only\n    probability = ifelse(x &lt;= n, dbinom(x, n, p), 0),\n    # Keep only non-zero probabilities for cleaner visualization\n    probability = ifelse(probability &gt; 0.001, probability, NA)\n  ) %&gt;%\n  filter(!is.na(probability))\n\np1 &lt;- ggplot(binomial_data, aes(x = x, y = probability, color = scenario)) +\n  geom_point(size = 1.5, alpha = 0.8) +\n  geom_line(alpha = 0.6) +\n  scale_color_manual(values = colors, ) +\n  labs(\n    title = \"Binomial Distribution\",\n    subtitle = \"Number of successes in fixed trials\",\n    x = \"Number of Successes\",\n    y = \"Probability\",\n    color = \"Parameters\"\n  ) +\n  custom_theme +\n  guides(color = guide_legend(nrow = 2, byrow = TRUE))\n\n# 2. POISSON DISTRIBUTION  \n# Model different rates of occurrence\npoisson_data &lt;- expand_grid(\n  lambda = c(0.5, 2, 5, 10),\n  x = 0:25\n) %&gt;%\n  mutate(\n    scenario = paste0(\"λ = \", lambda),\n    probability = dpois(x, lambda),\n    # Filter out very small probabilities for cleaner visualization\n    probability = ifelse(probability &gt; 0.001, probability, NA)\n  ) %&gt;%\n  filter(!is.na(probability))\n\np2 &lt;- ggplot(poisson_data, aes(x = x, y = probability, color = scenario)) +\n  geom_point(size = 1.5, alpha = 0.8) +\n  geom_line(alpha = 0.6) +\n  scale_color_manual(values = colors) +\n  labs(\n    title = \"Poisson Distribution\", \n    subtitle = \"Number of events in fixed interval\",\n    x = \"Number of Events\",\n    y = \"Probability\",\n    color = \"Parameters\"\n  ) +\n  custom_theme +\n  guides(color = guide_legend(nrow = 2, byrow = TRUE))\n\n# 3. UNIFORM DISTRIBUTION\n# Different ranges and intervals\nuniform_data &lt;- expand_grid(\n  scenario = c(\"U(0,1)\", \"U(0,10)\", \"U(5,15)\", \"U(-2,2)\"),\n  x = seq(-3, 16, 0.1)\n) %&gt;%\n  mutate(\n    # Extract bounds for each scenario\n    a = case_when(\n      scenario == \"U(0,1)\" ~ 0,\n      scenario == \"U(0,10)\" ~ 0, \n      scenario == \"U(5,15)\" ~ 5,\n      scenario == \"U(-2,2)\" ~ -2\n    ),\n    b = case_when(\n      scenario == \"U(0,1)\" ~ 1,\n      scenario == \"U(0,10)\" ~ 10,\n      scenario == \"U(5,15)\" ~ 15, \n      scenario == \"U(-2,2)\" ~ 2\n    ),\n    # Calculate uniform density\n    density = ifelse(x &gt;= a & x &lt;= b, 1/(b-a), 0)\n  )\n\np3 &lt;- ggplot(uniform_data, aes(x = x, y = density, color = scenario)) +\n  geom_line(linewidth = 1.2, alpha = 0.8) +\n  scale_color_manual(values = colors) +\n  labs(\n    title = \"Uniform Distribution\",\n    subtitle = \"Equal probability across specified range\", \n    x = \"Value\",\n    y = \"Density\",\n    color = \"Parameters\"\n  ) +\n  custom_theme +\n  guides(color = guide_legend(nrow = 2, byrow = TRUE))\n\n# 4. EXPONENTIAL DISTRIBUTION\n# Different rates showing varying \"wait times\"\nexponential_data &lt;- expand_grid(\n  lambda = c(0.5, 1, 2, 3),\n  x = seq(0, 8, 0.1)\n) %&gt;%\n  mutate(\n    scenario = paste0(\"λ = \", lambda),\n    density = dexp(x, lambda)\n  )\n\np4 &lt;- ggplot(exponential_data, aes(x = x, y = density, color = scenario)) +\n  geom_line(size = 1.2, alpha = 0.8) +\n  scale_color_manual(values = colors) +\n  labs(\n    title = \"Exponential Distribution\",\n    subtitle = \"Time between events in Poisson process\",\n    x = \"Time\",\n    y = \"Density\", \n    color = \"Parameters\"\n  ) +\n  custom_theme +\n  guides(color = guide_legend(nrow = 2, byrow = TRUE))\n\n# 5. BETA DISTRIBUTION\n# Different shapes representing various proportion scenarios\nbeta_data &lt;- expand_grid(\n  scenario = c(\"α=1, β=1\", \"α=2, β=5\", \"α=5, β=2\", \"α=3, β=3\"),\n  x = seq(0, 1, 0.01)\n) %&gt;%\n  mutate(\n    # Extract alpha and beta parameters\n    alpha = case_when(\n      scenario == \"α=1, β=1\" ~ 1,\n      scenario == \"α=2, β=5\" ~ 2,\n      scenario == \"α=5, β=2\" ~ 5,\n      scenario == \"α=3, β=3\" ~ 3\n    ),\n    beta = case_when(\n      scenario == \"α=1, β=1\" ~ 1,\n      scenario == \"α=2, β=5\" ~ 5,\n      scenario == \"α=5, β=2\" ~ 2,\n      scenario == \"α=3, β=3\" ~ 3\n    ),\n    density = dbeta(x, alpha, beta)\n  )\n\np5 &lt;- ggplot(beta_data, aes(x = x, y = density, color = scenario)) +\n  geom_line(size = 1.2, alpha = 0.8) +\n  scale_color_manual(values = colors) +\n  labs(\n    title = \"Beta Distribution\",\n    subtitle = \"Modeling proportions and percentages\",\n    x = \"Proportion\",\n    y = \"Density\",\n    color = \"Parameters\"\n  ) +\n  custom_theme +\n  guides(color = guide_legend(nrow = 2, byrow = TRUE))\n\n# 6. GAMMA DISTRIBUTION\n# Different shape and rate combinations\ngamma_data &lt;- expand_grid(\n  scenario = c(\"α=1, β=1\", \"α=2, β=1\", \"α=5, β=1\", \"α=2, β=2\"),\n  x = seq(0, 10, 0.1)\n) %&gt;%\n  mutate(\n    # Extract shape (alpha) and rate (beta) parameters\n    shape = case_when(\n      str_detect(scenario, \"α=1\") ~ 1,\n      str_detect(scenario, \"α=2\") ~ 2,\n      str_detect(scenario, \"α=5\") ~ 5\n    ),\n    rate = case_when(\n      str_detect(scenario, \"β=1\") ~ 1,\n      str_detect(scenario, \"β=2\") ~ 2\n    ),\n    density = dgamma(x, shape = shape, rate = rate)\n  )\n\np6 &lt;- ggplot(gamma_data, aes(x = x, y = density, color = scenario)) +\n  geom_line(size = 1.2, alpha = 0.8) +\n  scale_color_manual(values = colors) +\n  labs(\n    title = \"Gamma Distribution\",\n    subtitle = \"Sum of exponential random variables\",\n    x = \"Value\",\n    y = \"Density\",\n    color = \"Parameters\"\n  ) +\n  custom_theme +\n  guides(color = guide_legend(nrow = 2, byrow = TRUE))\n\n# Combine all plots using ggarrange\ncombined_plot &lt;- ggarrange(\n  p1, p2, p3, p4, p5, p6,\n  ncol = 2, nrow = 3,\n  common.legend = FALSE,  # Each plot keeps its own legend for clarity\n  align = \"hv\"  # Align both horizontally and vertically\n)\n\n# Add an overall title\nannotated_plot &lt;- annotate_figure(\n  combined_plot,\n  top = text_grob(\"Common Probability Distributions in Business Analytics\",\n                  color = \"black\", face = \"bold\", size = 16)\n)\n\n# For such larger plots I find it easier to save them and then include them\n#  as pdf:\nggexport(\n  annotated_plot,\n  filename = \"probability_distributions_overview.pdf\", \n  width = 8, height = 10, \n  dpi = 300)\n\n\n\n\nR example code\nknitr::include_graphics(\"probability_distributions_overview.pdf\")\n\n\n\n\n\n\n\n\nFigure 1: Overview of common probability distributions"
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#overview-table",
    "href": "content/statrecap/ProbabilityTheory/index.html#overview-table",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Overview table",
    "text": "Overview table\n\n\n\nTable 1: Overview of probability distributions and their R functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nParameters\nDensity Function\nProbability Function\nQuantile Function\nRandom Function\n\n\n\n\nNormal\nContinuous\nμ (mean), σ (sd)\ndnorm(x, mean, sd)\npnorm(q, mean, sd)\nqnorm(p, mean, sd)\nrnorm(n, mean, sd)\n\n\nBinomial\nDiscrete\nn (trials), p (success prob)\ndbinom(x, size, prob)\npbinom(q, size, prob)\nqbinom(p, size, prob)\nrbinom(n, size, prob)\n\n\nPoisson\nDiscrete\nλ (rate)\ndpois(x, lambda)\nppois(q, lambda)\nqpois(p, lambda)\nrpois(n, lambda)\n\n\nUniform\nContinuous\na (min), b (max)\ndunif(x, min, max)\npunif(q, min, max)\nqunif(p, min, max)\nrunif(n, min, max)\n\n\nExponential\nContinuous\nλ (rate)\ndexp(x, rate)\npexp(q, rate)\nqexp(p, rate)\nrexp(n, rate)\n\n\nBeta\nContinuous\nα (shape1), β (shape2)\ndbeta(x, shape1, shape2)\npbeta(q, shape1, shape2)\nqbeta(p, shape1, shape2)\nrbeta(n, shape1, shape2)\n\n\nGamma\nContinuous\nα (shape), β (rate)\ndgamma(x, shape, rate)\npgamma(q, shape, rate)\nqgamma(p, shape, rate)\nrgamma(n, shape, rate)\n\n\n\n\n\n\nThere are three observations that I would like to highlight:\n\nFirst, notice how every distribution, regardless of type, follows the same four-function pattern (d, p, q, r followed by the abbrevation for the distribution name).\nSecond, when you examine the Type column, you’ll notice an interesting pattern that connects to fundamental concepts in probability theory. The discrete distributions (binomial and Poisson) deal with counting scenarios, which naturally produce whole number outcomes. In business contexts, you use these when analyzing events like:\n\nNumber of successful sales calls (binomial)\nNumber of customer complaints per day (Poisson)\nNumber of defective products in a batch (binomial)\nNumber of website crashes per month (Poisson)\n\nThe continuous distributions handle measurement scenarios where the outcomes can take any value within a range. These prove essential for modeling:\n\nCustomer satisfaction scores (normal, beta)\nSales revenue amounts (normal, gamma)\nTime until next customer arrival (exponential)\nProject completion percentages (beta)\nManufacturing tolerances (normal, uniform)\n\nFinally, note that the parameter names in R functions sometimes differ slightly from the mathematical notation we typically use. For instance, where we write \\(\\mu\\) for the mean in mathematical contexts, R uses the more explicit mean parameter. Similarly, the binomial distribution uses size instead of \\(n\\) for the number of trials, which helps distinguish it from the \\(n\\) parameter used in random number generation."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#choosing-the-right-distribution",
    "href": "content/statrecap/ProbabilityTheory/index.html#choosing-the-right-distribution",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Choosing the Right Distribution",
    "text": "Choosing the Right Distribution\nSelecting appropriate distributions depends on several factors that connect to the research design principles you’ll encounter in later chapters:\nNature of your data: Discrete vs. continuous, bounded vs. unbounded, positive vs. can be negative\nUnderlying process: Are you counting events, measuring durations, looking at proportions, or modeling sums of other random variables?\nAvailable information: Do you know the range, the average rate, the shape characteristics? What does theory suggest about the process?\nPractical considerations: Can you estimate the distribution parameters from your data? Does the distribution have a reasonable interpretation in your business context?\nUnderstanding these distributions expands your analytical toolkit beyond the normal distribution. While the Central Limit Theorem, a concept that we will explore in later chapters, often makes normal distribution methods appropriate for sample means, recognizing situations where other distributions better model the underlying phenomena leads to more accurate analyses and better business insights.\nThe choice of distribution also connects to the concept of model assumptions that becomes important in regression analysis and other advanced techniques. Different distributions embody different assumptions about the data-generating process, and choosing appropriately helps ensure your statistical inferences are valid.\nNote for R implementation: Students could explore these distributions using R’s built-in functions like dbinom(), dpois(), dunif(), dexp(), dbeta(), and dgamma(), along with their corresponding random number generators (rbinom(), rpois(), etc.) to see how different parameter values affect distribution shapes and characteristics. This exploration reinforces the connection between theoretical distributions and practical data analysis."
  },
  {
    "objectID": "content/statrecap/ProbabilityTheory/index.html#footnotes",
    "href": "content/statrecap/ProbabilityTheory/index.html#footnotes",
    "title": "4: Essentials in Probability Theory for Statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n This is the so called “frequentist interpretation of probability”. The most influential alternative is the “Bayesian” interpretation, but we will not go into the details here.↩︎"
  },
  {
    "objectID": "content/statrecap/DataFundamentals/index.html",
    "href": "content/statrecap/DataFundamentals/index.html",
    "title": "2: Data Fundamentals",
    "section": "",
    "text": "Before diving into the details, it’s important to understand that data types and scales of measurement are two different but related ways of classifying variables. Think of them as two lenses through which we examine the same data.\nData types (categorical vs. numerical) tell us about the fundamental nature of our variables—whether we’re dealing with labels or numbers. Scales of measurement (nominal, ordinal, interval, ratio) tell us about the mathematical properties and operations we can perform on those variables.\nThe key insight is that categorical variables operate at nominal or ordinal scales, while numerical variables operate at interval or ratio scales. A categorical variable can never achieve interval or ratio properties, but the scale tells us how sophisticated our analysis can be within each data type."
  },
  {
    "objectID": "content/statrecap/DataFundamentals/index.html#categorical-variables-labels-and-categories",
    "href": "content/statrecap/DataFundamentals/index.html#categorical-variables-labels-and-categories",
    "title": "2: Data Fundamentals",
    "section": "Categorical Variables: Labels and Categories",
    "text": "Categorical Variables: Labels and Categories\nCategorical variables represent qualities or characteristics that can be divided into distinct groups. These variables answer questions like “what type?” or “which category?”\n\nConsider a survey of student transportation methods: “car,” “bicycle,” “public transport,” “walking.” Each student falls into exactly one category, and we cannot perform mathematical operations on these labels—it makes no sense to add “car” plus “bicycle.”\n\nCategorical variables come in two main flavors:\nNominal Variables have categories with no inherent order. The categories are simply different, not better or worse than each other.\n\nExamples include: gender (male, female, non-binary), academic majors (business, engineering, psychology), or country of origin (Germany, France, Italy, Spain).\n\nOrdinal Variables have categories that follow a meaningful order, though the distances between categories may not be equal.\n\nConsider satisfaction ratings: “very dissatisfied,” “dissatisfied,” “neutral,” “satisfied,” “very satisfied.” While we know “satisfied” is better than “dissatisfied,” we cannot assume the difference between “satisfied” and “very satisfied” is the same as between “neutral” and “satisfied.”"
  },
  {
    "objectID": "content/statrecap/DataFundamentals/index.html#numerical-variables-quantities-and-measurements",
    "href": "content/statrecap/DataFundamentals/index.html#numerical-variables-quantities-and-measurements",
    "title": "2: Data Fundamentals",
    "section": "Numerical Variables: Quantities and Measurements",
    "text": "Numerical Variables: Quantities and Measurements\nNumerical variables represent quantities that can be measured or counted. These variables answer questions like “how much?” or “how many?”\nDiscrete Variables can only take specific, distinct values—usually whole numbers you can count.\n\nNumber of employees in a company (5, 10, 15, but not 10.5), number of products sold (100, 101, 102), or number of customer complaints (0, 1, 2, 3…).\n\nContinuous Variables can take any value within a range, including decimal values.\n\nTemperature (23.7°C), height (175.3 cm), time spent on a website (2.45 minutes), or quarterly revenue (€2,847,293.67).\n\nUnderstanding whether your variable is discrete or continuous affects which statistical methods you can use. For instance, when we calculate averages for discrete variables like “number of children,” we might get 2.3 children per family—a meaningful statistic even though no family actually has 2.3 children."
  },
  {
    "objectID": "content/statrecap/DataFundamentals/index.html#nominal-scale-the-foundation",
    "href": "content/statrecap/DataFundamentals/index.html#nominal-scale-the-foundation",
    "title": "2: Data Fundamentals",
    "section": "Nominal Scale: The Foundation",
    "text": "Nominal Scale: The Foundation\nAt the nominal level, variables are merely labels or names. You can count how many observations fall into each category, but you cannot rank them or perform arithmetic operations.\n\nCompany departments: “Marketing,” “Finance,” “HR,” “Operations.” You can count how many employees work in each department, but it makes no sense to say Marketing &gt; Finance or to calculate an average department."
  },
  {
    "objectID": "content/statrecap/DataFundamentals/index.html#ordinal-scale-adding-order",
    "href": "content/statrecap/DataFundamentals/index.html#ordinal-scale-adding-order",
    "title": "2: Data Fundamentals",
    "section": "Ordinal Scale: Adding Order",
    "text": "Ordinal Scale: Adding Order\nOrdinal scales introduce ranking and order. You can determine which category is “higher” or “better,” but the intervals between ranks may not be equal.\n\nEducational levels: “High School,” “Bachelor’s,” “Master’s,” “PhD.” We know a Master’s degree is higher than a Bachelor’s, but the “distance” from Bachelor’s to Master’s might not equal the distance from Master’s to PhD in terms of time, effort, or knowledge gained."
  },
  {
    "objectID": "content/statrecap/DataFundamentals/index.html#interval-scale-equal-differences",
    "href": "content/statrecap/DataFundamentals/index.html#interval-scale-equal-differences",
    "title": "2: Data Fundamentals",
    "section": "Interval Scale: Equal Differences",
    "text": "Interval Scale: Equal Differences\nInterval scales have equal intervals between values, allowing us to compare differences meaningfully. However, they lack a true zero point.\n\nTemperature in Celsius: The difference between 20°C and 30°C is the same as between 30°C and 40°C (10 degrees). However, 40°C is not “twice as hot” as 20°C because 0°C doesn’t represent the absence of heat—it’s an arbitrary reference point."
  },
  {
    "objectID": "content/statrecap/DataFundamentals/index.html#ratio-scale-the-gold-standard",
    "href": "content/statrecap/DataFundamentals/index.html#ratio-scale-the-gold-standard",
    "title": "2: Data Fundamentals",
    "section": "Ratio Scale: The Gold Standard",
    "text": "Ratio Scale: The Gold Standard\nRatio scales have all the properties of interval scales plus a meaningful zero point. This allows for all mathematical operations, including ratios and percentages.\n\nIncome in euros: €0 means no income, €60,000 is twice as much as €30,000, and the difference between €30,000 and €40,000 is the same as between €50,000 and €60,000.\n\nUnderstanding these scales helps you choose appropriate statistical techniques. For example, you can calculate a meaningful average (mean) for ratio and interval data, but for ordinal data, the median is often more appropriate."
  },
  {
    "objectID": "content/statrecap/DataFundamentals/index.html#sample-vs.-population-a-quick-note",
    "href": "content/statrecap/DataFundamentals/index.html#sample-vs.-population-a-quick-note",
    "title": "2: Data Fundamentals",
    "section": "Sample vs. Population: A Quick Note",
    "text": "Sample vs. Population: A Quick Note\nBefore we dive into notation, it’s important to know that statistics distinguishes between: - Population: The entire group we’re interested in studying - Sample: A subset of the population that we actually observe and collect data from\nWe’ll explore this distinction in much more detail later, but for now, just remember that different symbols often indicate whether we’re talking about sample or population values.\n\nUnderstanding Mathematical Objects: Numbers, Vectors, and Matrices\nIn statistics, we work with different types of mathematical objects that help us organize and manipulate data. Think of these as different containers, each suited for different kinds of information.\nNumbers (Scalars) are single values—the simplest form of data.\n\nA student’s age (22), a company’s profit (€50,000), or a satisfaction score (7.5). Each represents one piece of information.\n\nVectors are ordered lists of numbers, like a column or row in a spreadsheet. We use vectors to store multiple observations of the same variable.\n\nA vector of test scores might look like: x = [85, 90, 78, 92, 88]. This represents five students’ scores on the same exam, organized in a single mathematical object.\n\nMatrices are rectangular arrangements of numbers in rows and columns, like a complete spreadsheet. Each row typically represents one observation (like one student), and each column represents one variable (like test scores, ages, etc.).\n\nA matrix might store data for 3 students across 2 variables:\n\\[\\mathbf{X} = \\begin{bmatrix}\n85 & 22 \\\\\n90 & 23 \\\\\n78 & 21\n\\end{bmatrix}\\]\nHere, the first column shows test scores, the second shows ages.\n\nUnderstanding these structures is crucial because different statistical operations work with different mathematical objects. When you calculate a mean, you’re working with a vector of numbers to produce a single number. When you analyze relationships between multiple variables, you’re often working with matrices."
  },
  {
    "objectID": "content/statrecap/DataFundamentals/index.html#variables-and-observations",
    "href": "content/statrecap/DataFundamentals/index.html#variables-and-observations",
    "title": "2: Data Fundamentals",
    "section": "Variables and Observations",
    "text": "Variables and Observations\nIn statistics, we typically use letters to represent variables. The choice of letter case and formatting follows specific conventions:\nUppercase Letters (X, Y) represent:\n\nVariables in general (before we collect specific data)\nRandom variables in probability contexts\n\nLowercase Letters (x, y) represent:\n\nSpecific observed values of a variable\nIndividual data points in our dataset\n\nBold Letters (x, X)** represent:\n\nVectors (lists of numbers)\nMatrices (rectangular arrays of numbers)\n\n\nIf we study student ages, X represents the age variable in general, while x₁, x₂, x₃… represent the actual ages we observe: 22, 25, 23…\n\n\n\\(n\\) represents the number of observations in a sample\n\\(N\\) represents the number of observations in a population\n\n\nIf we survey 150 students about their study hours per week, n = 150. Each student’s response is one observation of the variable “study hours.”"
  },
  {
    "objectID": "content/statrecap/DataFundamentals/index.html#subscripts-identifying-individual-observations",
    "href": "content/statrecap/DataFundamentals/index.html#subscripts-identifying-individual-observations",
    "title": "2: Data Fundamentals",
    "section": "Subscripts: Identifying Individual Observations",
    "text": "Subscripts: Identifying Individual Observations\nSubscripts help us refer to specific observations within a dataset: - \\(X_1\\) refers to the first observation of variable \\(X\\) - \\(X_2\\) refers to the second observation - \\(X_i\\) refers to the i-th observation (where i can be any number from 1 to n)\n\nIn a dataset of student ages: X₁ = 22, X₂ = 25, X₃ = 23… where X₁ represents the first student’s age, X₂ the second student’s age, and so on.\n\n\nSummation Notation: Adding It All Up\nThe Greek letter sigma \\(\\Sigma\\) represents summation:\n\n\\(\\sum_{i=1}^nX_i\\) means “sum all values of X from i = 1 to n”\n\n\nIf we have test scores: 85, 90, 78, 92, 88\nThen we can sum them like this:\n\\(\\sum_{i=1}^nX_i=85 + 90 + 78 + 92 + 88 = 433\\)\n\n\n\nCommon Statistical Symbols\nUnderstanding these symbols will help you read statistical formulas and research papers:\n\n\\(\\mu\\) (mu): Population mean\n\\(\\bar{x}\\) (x-bar): Sample mean\n\n\\(\\sigma\\) (sigma): Population standard deviation\n\\(sd\\): Sample standard deviation\n\\(p\\): Probability or proportion\n\nRemember, this notation exists to make communication clearer and more precise. When you see x̄ = 75, you immediately know this refers to a sample mean of 75, without needing a lengthy explanation."
  },
  {
    "objectID": "content/statrecap/DataFundamentals/index.html#putting-it-all-together-a-practical-perspective",
    "href": "content/statrecap/DataFundamentals/index.html#putting-it-all-together-a-practical-perspective",
    "title": "2: Data Fundamentals",
    "section": "Putting It All Together: A Practical Perspective",
    "text": "Putting It All Together: A Practical Perspective\nUnderstanding data fundamentals allows you to approach any dataset with confidence. Before diving into complex analyses, always ask yourself:\n\nWhat type of variables am I working with?\nWhat scale of measurement applies to each variable?\nWhat does this tell me about which analytical methods are appropriate?\n\n\nConsider a customer satisfaction survey with three questions: 1. “Which product did you purchase?” (Nominal categorical) 2. “How satisfied are you with your purchase?” (Ordinal categorical) 3. “How much did you spend?” (Ratio numerical)\nEach variable type suggests different analytical approaches and visualizations.\n\nThese fundamentals serve as the foundation for everything else in statistics. Just as a strong foundation supports a building, understanding these concepts will support your entire statistical learning journey. Take time to practice identifying variable types and scales in real datasets—this skill will serve you well throughout your research career."
  },
  {
    "objectID": "content/exercises/quarto-exercises/index.html",
    "href": "content/exercises/quarto-exercises/index.html",
    "title": "Quarto exercises",
    "section": "",
    "text": "Create a new Quarto document where you set the title, date, and the author explicitly. Write a sample text that comprises…\n\n…at least one level 1 heading\n…at least two level 2 headings\n…a YAML part that specifies that R code remains hidden by default\n…one R chunk where both the output and the code is printed in the final document\n…one R chunk that produces a simply ggplot object and where the code producing the plot is hidden\n\nThen do the following:\n\nKnit the document to html with a floating table of contents and a special theme.\nMake the document available via Netlify Drop and add the possibility to download the underlying Rmd file. Note: For Netlify Drop to work, the html file must be called index.html!\nKnit the document to PDF and make sure that it includes a table of contents.\n\nA sample solution for the Rmd file can be found here (the Netlify version is here)."
  },
  {
    "objectID": "content/exercises/multiple-regression/index.html",
    "href": "content/exercises/multiple-regression/index.html",
    "title": "Exercises on multiple linear regression",
    "section": "",
    "text": "Read in the data set coffee_data.csv.\nIt contains the following variables:\n\nCoffeePrice: The world market price for coffee from Brazil in EUR per kilo\nCoffeeDemand: The demand for coffee from Brazil in tons\nCoffeeSeller: The kind of coffee company: Standard or FairTrade\n\nIn the following, treat CoffeeDemand as the dependent variable, and CoffeePrice and CoffeeSeller as the independent variables.\nEstimate a parallel slopes model, and an interaction model. Which of the models would you prefer?\nYou should justify your choice using two main arguments.\n\n\n\nRead in the data set ice_data.csv. What kind of relationships could you reasonably study in a linear regression framework (without further data transformation).\n\n\n\nThe solutions to both exercises can be found here."
  },
  {
    "objectID": "content/exercises/multiple-regression/index.html#studying-coffee-data",
    "href": "content/exercises/multiple-regression/index.html#studying-coffee-data",
    "title": "Exercises on multiple linear regression",
    "section": "",
    "text": "Read in the data set coffee_data.csv.\nIt contains the following variables:\n\nCoffeePrice: The world market price for coffee from Brazil in EUR per kilo\nCoffeeDemand: The demand for coffee from Brazil in tons\nCoffeeSeller: The kind of coffee company: Standard or FairTrade\n\nIn the following, treat CoffeeDemand as the dependent variable, and CoffeePrice and CoffeeSeller as the independent variables.\nEstimate a parallel slopes model, and an interaction model. Which of the models would you prefer?\nYou should justify your choice using two main arguments."
  },
  {
    "objectID": "content/exercises/multiple-regression/index.html#exploring-feasible-model-specifications",
    "href": "content/exercises/multiple-regression/index.html#exploring-feasible-model-specifications",
    "title": "Exercises on multiple linear regression",
    "section": "",
    "text": "Read in the data set ice_data.csv. What kind of relationships could you reasonably study in a linear regression framework (without further data transformation)."
  },
  {
    "objectID": "content/exercises/multiple-regression/index.html#solutions",
    "href": "content/exercises/multiple-regression/index.html#solutions",
    "title": "Exercises on multiple linear regression",
    "section": "",
    "text": "The solutions to both exercises can be found here."
  },
  {
    "objectID": "content/tutorials/using-exercises/index.html",
    "href": "content/tutorials/using-exercises/index.html",
    "title": "Using the exercise package",
    "section": "",
    "text": "When learning a programming language, applying the new concepts regularly is absolutely essential. Without regular practice it is hard to impossible to remember everything you need to actually enjoy working with R.\nTherefore, I prepared a small set of exercises for each session that I recommend you to do after the session. This will help you to remember what you have learned and to find out what you did not understand well. I then urge you to post your problems on Moodle, to ask your colleagues and to help each other out. This will be a great boost to your learning progress: explaining something to others is not only great in a normative sense, it also helps you to get a deeper understanding of the concepts yourself.\nIn this short post I quickly explain how you can use the exercises that I have prepared for you. For those of you interested in the underlying mechanics: all exercises were prepared using the package learnr together with the gradethis package, and are distributed in the package DataScienceExercises.\nTo use the exercises you must have installed the packages learnr, gradethis and DataScienceExercises. If you followed the instructions in the tutorial on installing R packages this should be the case. If for some reasons you need to install them, you can do this via:\n\npack_names &lt;- c(\n  \"rstudio/learnr\",\n  \"rstudio/gradethis\",\n  \"graebnerc/DataScienceExercises\"\n)\nremotes::install_github(\n  repo = pack_names, upgrade = \"always\")\n\nNote that this requires a previous installation of the package remotes:\n\ninstall.packages(\"remotes\")\n\nSince I update the exercises according to your feedback, and add new exercises over the semester, I strongly recommend you to update the package DataScienceExercises each time before you start practicing. To do so you need an internet connection. If you just want to do the exercises without updating the package, an internet connection is not required: the exercises themselves also work offline.\nTo update the package, simply type the following into the console and press Enter:\n\nremotes::install_github(\n  repo = \"graebnerc/DataScienceExercises\", upgrade = \"always\")\n\nThis should update your package version to the most recent release.\nIf you want to start an exercise you first need to figure out the name of the exercise sheet. This is provided in the Material section of the course webpage. Then you call execute the following code via the console in R Studio, replacing ‘EX_NAME’ with the name of the exercise sheet:\n\nlearnr::run_tutorial(\n  name = \"EX_NAME\", \n  package = \"DataScienceExercises\", \n  shiny_args=list(\"launch.browser\"=TRUE))\n\nThe first exercise sheet, for instance, is called Basics. Thus, to call it execute the following:\n\nlearnr::run_tutorial(\n  name = \"Basics\", \n  package = \"DataScienceExercises\", \n  shiny_args=list(\"launch.browser\"=TRUE))\n\nLastly, if you encounter a bug or a mistake, or have an idea for a good exercise, please let me know via the issue tracker of the DataScienceExercises package or via Moodle. Thank you!"
  },
  {
    "objectID": "content/tutorials/obj-types-iii-adv/index.html",
    "href": "content/tutorials/obj-types-iii-adv/index.html",
    "title": "Fundamental object types in R III: Factors and data frames",
    "section": "",
    "text": "In the previous posts you learned about the most important fundamental data types in R. The types we will learn about below and not less important, but less fundamental. This means they are built by taking one of the base types we encountered before, and ‘adding some features’. These features change the behavior of the type, e.g.m how it is printed or how it is affected by certain function calls, but also what kind of operations it allows.1\nThis process of ‘adding features’ is usually done by adding ‘attributes’ to an object. In principle, you can add attributes to any objects without much effect by using the attr() function:\n\nx &lt;- 2.0\nattr(x, \"Mood\") &lt;- \"Haha!\"\n\nTo retrieve attributes use attributes():\n\nattributes(x)\n\n$Mood\n[1] \"Haha!\"\n\n\nSometimes, adding attributes of a particular name have more relevant implications. One useful way to use attributes, for instance, is to name the single elements of vectors, something that changes the way the objects are printed and something that we already discussed in the context of lists:\n\ny &lt;- c(1, 2, 3)\nattr(y, \"names\") &lt;- c(\"First\", \"Second\", \"Third\")\ny\n\n First Second  Third \n     1      2      3 \n\n\n\nattributes(y)\n\n$names\n[1] \"First\"  \"Second\" \"Third\" \n\n\nNow we can call elements by their name:\n\ny[\"Second\"]\n\nSecond \n     2 \n\n\nBut things become really interesting if you add an attribute called class: this really transforms the data types into a new, less fundamental type. In fact, this is how the types we discuss below, are created: smart people added, among other things, a class attribute to a more fundamental data type (integer in the case of factors and list in the case of data.frames). The art of writing new classes is part of object oriented programming, an advanced concept that we do not cover in this course (and, to be honest, one of the areas where R is not particularly well designed).\nOne implication of this ‘less fundamental’ nature of the objects we encounter below is that typeof() usually returns the base type. For instance, below we will learn about the factor, a type that is built upon integer. If we call typeof() on a factor, it will return the fundamental type, i.e. integer:\n\nxx &lt;- factor(c(1,2))\ntypeof(xx)\n\n[1] \"integer\"\n\n\nFortunately, the standard test functions (is.*()) usually work, so you can use is.factor():\n\nis.factor(xx)\n\n[1] TRUE\n\n\nAlternatively, you can always inspect that attributes of the object to find out about its class:\n\nattributes(xx)\n\n$levels\n[1] \"1\" \"2\"\n\n$class\n[1] \"factor\"\n\n\nAll this can be confusing at first, so it is important to keep it in mind. Once you wrapped your head upon this, many confusing behaviors suddenly start to make sense, e.g. that mutating factors within a data.frame can result in whole numbers, a phenomenon we will discuss in the context of data wrangling later."
  },
  {
    "objectID": "content/tutorials/obj-types-iii-adv/index.html#footnotes",
    "href": "content/tutorials/obj-types-iii-adv/index.html#footnotes",
    "title": "Fundamental object types in R III: Factors and data frames",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn fact, some new types allow you to do less than the original type, i.e. here the new features are restrictions. The tibble we will encounter below is such an example.↩︎\nIf you read the section on attributes above: matrices are objects that were given a new attribute dim.↩︎\nThis is another piece of evidence for the very confusing class concept and object-orientated style of R. See this chapter for more details, in case you are interested.↩︎"
  },
  {
    "objectID": "content/tutorials/obj-types-i-functions/index.html",
    "href": "content/tutorials/obj-types-i-functions/index.html",
    "title": "Fundamental object types in R I: Functions",
    "section": "",
    "text": "We already learned that everything in R that exists is an object. You most likely already noted that there are different types of objects: 2, for instance, was a number, but assign was a function.1 As you might have guessed, there are many more types of objects. To understand the fundamental object types in R is an essential prerequisite to master more complicated programming challenges than those we have encountered so far. Thus, this post is among those that will introduce you to the most important object types that you will encounter in R.\nThese data types are summarized in the following figure:\n\n\n\n\n\n\n\n\n\nThis post will be about functions. Different types of vectors are covered in the upcoming posts."
  },
  {
    "objectID": "content/tutorials/obj-types-i-functions/index.html#footnotes",
    "href": "content/tutorials/obj-types-i-functions/index.html#footnotes",
    "title": "Fundamental object types in R I: Functions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn fact, we will learn below that 2 is not really a number, but a vector or length 1. Only in a next step, 2 counts as a ‘number’, or, more precisely as a ‘double’.↩︎\nUsing return is, strictly speaking, not necessary, but I always use it for the sake of readability and transparency. An interesting debate about whether you should use return or not can be found here.↩︎\nOr, as the well-known R developer Hadley Wickham puts it: “You are always coorpering with at least one other person: future-you.”↩︎"
  },
  {
    "objectID": "content/tutorials/installing-packages/index.html",
    "href": "content/tutorials/installing-packages/index.html",
    "title": "Installing R packages",
    "section": "",
    "text": "After installing R and R-Studio, you still need to install a number of so called R packages. We will learn more about what packages are and how to use them later. Nevertheless, I strongly recommend you to install already all the packages you will need over the following semester already now. This way you make sure that everything is working now, and you save yourself from trouble during the semester. Moreover, installing these packages is necessary to do the exercises provided after each session.\nTo install packages, a stable internet connection is required. Then, proceed as follows:\n\nDownload the script install_packages_script.R and save it in a directory of your choice\nOpen the file install_packages_script.R in R-Studio. To this end, right-click on the file and select Open with, and then choose R-Studio.\nAdjust the first line of the script to the OS you are using. For instance, when you are using a Mac the file should look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelect lines 1-52 and click on the button Run (the screenshot only shows the lines 42-52, the previous lines are also selected):\n\n\n\n\n\n\n\n\n\n\n\nCheck if the package here was installed. You will get the respective message in the console:\n\n\n\n\n\n\n\n\n\n\nIf you encounter any problems, please make a screenshot and post it in the Moodle forum.\n\nSelect the rest of the scrip and run it as you did with the first lines. If you get the following message everything worked well and all packages were installed successfully:\n\n\n\n\n\n\n\n\n\n\nIf not, please post the file InstallationLog.txt and a screenshot with an error into the Moodle forum.\nNote: maybe you will see the following message during the installation process (possible multiple times):\n\n\n\n\n\n\n\n\n\nI recommend to type No and press enter. If, for any reason, the installation process is not successful you might run the installation commands again and try responding with Yes, this might sometimes fix the problem."
  },
  {
    "objectID": "content/tutorials/installing-packages/index.html#a-common-problem-when-installing-tinytex-on-a-mac",
    "href": "content/tutorials/installing-packages/index.html#a-common-problem-when-installing-tinytex-on-a-mac",
    "title": "Installing R packages",
    "section": "A common problem when installing tinytex on a Mac",
    "text": "A common problem when installing tinytex on a Mac\nThe following hints should be helpful if after the attempted installation of tinytex you see either one of these error messages:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this case, execute the following comment in your R console within R-Studio:\n\ntinytex::install_tinytex(force = TRUE)\n\nThen close R-Studio and restart your computer. If test_pdf.qmd still cannot be compiled after this, please open your Mac Terminal (via the app Terminal) and enter the following commands:\nsudo chown -R `whoami`:admin /usr/local/bin\n\n~/Library/TinyTeX/bin/x86_64-darwin/tlmgr path add\nThen install tinytex again as described above, restart your computer, and try to compile test_pdf.qmd again."
  },
  {
    "objectID": "content/tutorials/importing-exporting-data/index.html",
    "href": "content/tutorials/importing-exporting-data/index.html",
    "title": "Importing and exporting data",
    "section": "",
    "text": "Packages used in this tutorial:\nlibrary(here)\nlibrary(data.table)\nlibrary(dplyr)"
  },
  {
    "objectID": "content/tutorials/importing-exporting-data/index.html#specify-the-column-separator-using-sep-and-the-decimal-sign-using-dec",
    "href": "content/tutorials/importing-exporting-data/index.html#specify-the-column-separator-using-sep-and-the-decimal-sign-using-dec",
    "title": "Importing and exporting data",
    "section": "Specify the column separator using sep and the decimal sign using dec",
    "text": "Specify the column separator using sep and the decimal sign using dec\nWhile the example file above represents the widespread standard case in which columns are separated by a comma and the dot is used as the decimal sign, many files use other symbols. In Germany, for instance, it is very common to use ; as a separator for columns, and , as a decimal sign instead. Thus, the ‘German version’ of our example from above would look like this:\niso2c;year;Exporte\nAT;2012;53,97\nAT;2013;53,44\nAT;2014;53,38\nSometimes, data.table::fread() detects such cases automatically and adjusts the values for the optional arguments implicitly. But it is always better to explicit and to specify decimal signs and column separators explicitly! This also increases the reading speed of data.table::fread(). To set them explicitly, we use the arguments sep and dec as follows:\n\nexp_data &lt;- data.table::fread(\n  file = file_path,\n  sep = \";\", \n  dec = \",\"\n  )\n\nAfter completing the function call we should always inspect the imported object to make sure everything went well. We might have a look at the first lines:\n\nexp_data &lt;- tibble::as_tibble(exp_data)\nhead(exp_data, n = 2)\n\n# A tibble: 2 × 3\n  iso2c  year exports\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;\n1 AT     2012    54.0\n2 AT     2013    53.4\n\n\nOr use dplyr::glimpse() or str():\n\nstr(exp_data)\n\ntibble [3 × 3] (S3: tbl_df/tbl/data.frame)\n $ iso2c  : chr [1:3] \"AT\" \"AT\" \"AT\"\n $ year   : int [1:3] 2012 2013 2014\n $ exports: num [1:3] 54 53.4 53.4\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt;"
  },
  {
    "objectID": "content/tutorials/importing-exporting-data/index.html#set-the-object-type-of-the-columns-using-colclasses",
    "href": "content/tutorials/importing-exporting-data/index.html#set-the-object-type-of-the-columns-using-colclasses",
    "title": "Importing and exporting data",
    "section": "Set the object type of the columns using colClasses",
    "text": "Set the object type of the columns using colClasses\nUsually, the automatic type recognition of data.table::fread() works quite well. This means that R chooses the right data type for each column automatically. Sometimes, however, this detection fails and you need to specify the column types manually. But even if the automatic recognition works, there are some good reasons for playing save and specify the column types yourself:\n\nYou will notice more easily if there is a problem with a column, e.g. if a word occurs in a column that consists exclusively of numbers. occurs. If you did not specify this column manually as double, data.table::fread() would simply interpret it silently as a character and you would later wonder later why you cannot calculate an average for the column;\nYour code will be more transparent and easier to read if one immediately knows what kind of data you are importing\nThe import process will be much faster if you provide the column types yourself and the function does not need to guess the types itself.\n\nOne situation where specifying column types yourself is extremely important is when a column contains numerical codes that might contain a leading zero, e.g. when the data contain HS product codes, such as here:\ncommoditycode,complexity\n0101,0.06\n0102,-0.49\n0103,0.51\n0104,-1.12\n0105,-0.17\nAssuming the file is called exp_data_hs.csv and also is stored in data/tidy/, we might try to import it using the default argument values:\n\nfile_path &lt;- here::here(\"data/tidy/exp_data_hs.csv\")\nexp_prod_data &lt;- data.table::fread(file = file_path)\nexp_prod_data &lt;- tibble::as_tibble(exp_prod_data)\nexp_prod_data\n\n\n\n# A tibble: 5 × 2\n  commoditycode complexity\n          &lt;int&gt;      &lt;dbl&gt;\n1           101       0.06\n2           102      -0.49\n3           103       0.51\n4           104      -1.12\n5           105      -0.17\n\n\nAs you can see, data.table::fread() interpreted the column commoditycode as double. But since numbers do not have leading zeros, these are removed silently, meaning that R does not issue a warning message. This is dangerous and might come with serious misinterpretations later on. To avoid this, you must choose the column types yourself via the colClasses argument, by simply specifying a vector with the data types:\n\nfile_path &lt;- here::here(\"data/tidy/exp_data_hs.csv\")\nexp_prod_data &lt;- data.table::fread(\n  file = daten_pfad, colClasses = c(\"character\", \"double\")\n  )\ntibble::as_tibble(exp_prod_data)\n\n\n\n# A tibble: 5 × 2\n  commoditycode complexity\n  &lt;chr&gt;              &lt;dbl&gt;\n1 0101                0.06\n2 0102               -0.49\n3 0103                0.51\n4 0104               -1.12\n5 0105               -0.17\n\n\nAs you can see, encoding the column commoditycode as character preserves the leading zeros and the correct product codes.\nFor data sets with many columns it is often tedious to specify column types one by one. Here it might be useful to use the function rep(): it saves space if, for instance, 6 subsequent columns are all of type double. In this case you may just write rep(\"double\" , 6)."
  },
  {
    "objectID": "content/tutorials/importing-exporting-data/index.html#specify-how-many-rows-should-be-readskipped-using-nrows-and-skip",
    "href": "content/tutorials/importing-exporting-data/index.html#specify-how-many-rows-should-be-readskipped-using-nrows-and-skip",
    "title": "Importing and exporting data",
    "section": "Specify how many rows should be read/skipped using nrows and skip",
    "text": "Specify how many rows should be read/skipped using nrows and skip\nKeep in mind that you can increase the reading speed of data.table::fread() considerably by manually specifying the columns types. At the same time, opening very large data files in R Studio or even a text editor can slow down your computer considerably.\nThus, it is advisable to read in the first 3-5 rows, inspect them, and then read in the whole data set with the right specification for colClasses.\nYou can load only the first \\(n\\) rows by using the argument nrows:\n\nexp_data &lt;- tibble::as_tibble(data.table::fread(\n  file = here::here(\"data/tidy/exp_data.csv\"), \n  nrows = 1)\n  )\nexp_data\n\n\n\n# A tibble: 1 × 3\n  iso2c  year exports\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;\n1 AT     2012    54.0\n\n\nIn other instances, you might also want to skip the first \\(n\\) rows. This is often the case if your file contains some general introductory header, which is placed before the actual data set. Such data with a header might look like this:\nThis is awesome data from 2012-2014\nIt was compiled be Claudius\nHe also added this useless header\niso2c,year,Exporte\nAT,2012,53.97\nAT,2013,53.44\nAT,2014,53.38\n\nIn this case, you definitely want to ignore the first three rows when importing the data set. Otherwise you will get hodgepodge:\n\nexp_data &lt;- data.table::fread(\n  file = here::here(\"data/tidy/exp_data_header.csv\")\n  )\ntibble::as_tibble(exp_data)\n\n\n\n# A tibble: 1 × 6\n  V1    It    was   compiled be      Claudius\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   \n1 He    also  added this     useless header  \n\n\nTo ignore the first three rows just set skip to 3:\n\nexp_data &lt;- tibble::as_tibble(data.table::fread(\n  file = here::here(\"data/tidy/exp_data_header.csv\"), \n  skip = 3)\n  )\nexp_data\n\n\n\n# A tibble: 3 × 3\n  iso2c  year Exporte\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;\n1 AT     2012    54.0\n2 AT     2013    53.4\n3 AT     2014    53.4\n\n\nAgain, the automatic detection of fread() often works quite well when it comes to the identification of useless headers, but better be prepared to use skip whenever necessary."
  },
  {
    "objectID": "content/tutorials/importing-exporting-data/index.html#specify-columns-that-should-not-be-read-using-select-and-drop",
    "href": "content/tutorials/importing-exporting-data/index.html#specify-columns-that-should-not-be-read-using-select-and-drop",
    "title": "Importing and exporting data",
    "section": "Specify columns that should (not) be read using select and drop",
    "text": "Specify columns that should (not) be read using select and drop\nSometimes you only want to read in a certain selection of columns. This can also save a lot of time when working with large data sets. In the following example we only want to import the columns year and exports:\n\nexp_data &lt;- data.table::fread(\n  file = here::here(\"data/tidy/exp_data.csv\")\n  nrows = 1, \n  select = c(\"year\", \"Exporte\")\n  )\nexp_data &lt;- tibble::as_tibble(exp_data)\nexp_data\n\n\n\n# A tibble: 2 × 2\n   year exports\n  &lt;int&gt;   &lt;dbl&gt;\n1  2012    54.0\n2  2013    53.4\n\n\nIf you want to manually specify column types, you can do so without using colClasses by passing a named vector to select:\n\nexp_data &lt;- data.table::fread(\n  file = here::here(\"data/tidy/exp_data.csv\")\n  nrows = 1, \n  select = c(\"year\"=\"double\", \"exports\"=\"double\")\n  )\nexp_data &lt;- tibble::as_tibble(exp_data)\nexp_data\n\n\n\n# A tibble: 2 × 2\n   year exports\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  2012    54.0\n2  2013    53.4\n\n\nAlternatively, we can also specify columns to be ignored via drop:\n\nexp_data &lt;- data.table::fread(\n  file = here::here(\"data/tidy/exp_data.csv\")\n  nrows = 1, \n  drop = \"iso2c\"\n  )\nexp_data &lt;- tibble::as_tibble(exp_data)\nexp_data\n\n\n\n# A tibble: 2 × 2\n   year exports\n  &lt;int&gt;   &lt;dbl&gt;\n1  2012    54.0\n2  2013    53.4"
  },
  {
    "objectID": "content/tutorials/importing-exporting-data/index.html#footnotes",
    "href": "content/tutorials/importing-exporting-data/index.html#footnotes",
    "title": "Importing and exporting data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also specify alternative arguments, such as cmd when you want to parse the input file using a command line command. But we will not cover such more advanced cases here.↩︎"
  },
  {
    "objectID": "content/tutorials/data-wrangling/index.html",
    "href": "content/tutorials/data-wrangling/index.html",
    "title": "Data preparation",
    "section": "",
    "text": "library(dplyr)\nlibrary(tidyr)\nlibrary(data.table)\nlibrary(here)\n\nThe data sets used in these notes are available from the course homepage:\n\nwrangling_data_raw.csv (data_raw)\nwrangling_data_raw_long.csv (data_raw_long)\nwrangling_data_final_expl.csv (data_final_expl)\nwrangling_gini_join.csv (gini_join)\nwrangling_gdp_join.csv (gdp_join)\n\nThe brackets show the names of the data sets used below."
  },
  {
    "objectID": "content/tutorials/data-wrangling/index.html#wide-and-long-format-definition",
    "href": "content/tutorials/data-wrangling/index.html#wide-and-long-format-definition",
    "title": "Data preparation",
    "section": "Wide and long format: definition",
    "text": "Wide and long format: definition\nThere is no strict definition for wide and long data. Rather, the two should be understood as relative descriptions of data, meaning that it is more straightforward to speak of a data set that is longer relative to another one, rather than a long data set per se.\nHere is an example for a rather long data set:\n\n\n   country  year variable    value\n    &lt;char&gt; &lt;int&gt;   &lt;char&gt;    &lt;num&gt;\n1: Germany  2017    unemp     3.75\n2: Germany  2017      gdp 53071.46\n3: Germany  2018    unemp     3.38\n4: Germany  2018      gdp 53431.39\n5:  Greece  2017    unemp    21.49\n6:  Greece  2017      gdp 28604.86\n7:  Greece  2018    unemp    19.29\n8:  Greece  2018      gdp 29141.17\n\n\nHere, we have one column identifying the variable, the value of which is stored in a separate column. This means that the data is relatively ‘long’ in the sense of having many rows. At the same time, it is relatively ‘narrow’ in the sense of not having too many columns since the variable identifier is kept in a single column.\nContrast this with an example for a rather wide data set, where each variable has its own column:\n\n\n   country  year unemp      gdp\n    &lt;char&gt; &lt;int&gt; &lt;num&gt;    &lt;num&gt;\n1: Germany  2017  3.75 53071.46\n2: Germany  2018  3.38 53431.39\n3:  Greece  2017 21.49 28604.86\n4:  Greece  2018 19.29 29141.17\n\n\nWhile the number of columns remains the same, the data set has relatively more columns as compared to the rows. At the same time, it tends to be shorter in the sense of having fewer rows.1\nWhile the long format is often easier to read and preferable when communicating data to humans, making data tidy often involves the task of making data ‘longer’."
  },
  {
    "objectID": "content/tutorials/data-wrangling/index.html#transforming-long-data-into-wide-data",
    "href": "content/tutorials/data-wrangling/index.html#transforming-long-data-into-wide-data",
    "title": "Data preparation",
    "section": "Transforming long data into wide data",
    "text": "Transforming long data into wide data\nTo make data wider we use the function tidyr::pivor_wider().\nAssume that we start with our long data set introduced above and that this data set is bound to the name data_raw_long.\n\ndplyr::glimpse(data_raw_long)\n\nRows: 8\nColumns: 4\n$ country  &lt;chr&gt; \"Germany\", \"Germany\", \"Germany\", \"Germany\", \"Greece\", \"Greece…\n$ year     &lt;int&gt; 2017, 2017, 2018, 2018, 2017, 2017, 2018, 2018\n$ variable &lt;chr&gt; \"unemp\", \"gdp\", \"unemp\", \"gdp\", \"unemp\", \"gdp\", \"unemp\", \"gdp\"\n$ value    &lt;dbl&gt; 3.75, 53071.46, 3.38, 53431.39, 21.49, 28604.86, 19.29, 29141…\n\n\nWe will now use tidyr::pivor_wider() to make this data set wider. The most important arguments of this function are as follows:2\n\ndata is the first argument and refers to the name of the data set to be considered\nnames_from denotes the column that includes the names of the new columns\nvalues_from denotes the column that includes the values to be allocated in the newly created cells\n\nIn the present case, the call would look like the following:\n\ndata_raw_wide &lt;- tidyr::pivot_wider(\n  data = data_raw_long, \n  names_from = \"variable\", \n  values_from = \"value\")\ndata_raw_wide\n\n# A tibble: 4 × 4\n  country  year unemp    gdp\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Germany  2017  3.75 53071.\n2 Germany  2018  3.38 53431.\n3 Greece   2017 21.5  28605.\n4 Greece   2018 19.3  29141."
  },
  {
    "objectID": "content/tutorials/data-wrangling/index.html#transforming-wide-data-into-long-data",
    "href": "content/tutorials/data-wrangling/index.html#transforming-wide-data-into-long-data",
    "title": "Data preparation",
    "section": "Transforming wide data into long data",
    "text": "Transforming wide data into long data\nAssume we want to take the data set data_raw_wide and re-create the original long version. To achieve this we can use tidyr::pivot_longer(). Again, lets have a look at the most important arguments:3\n\ndata is the first argument and refers to the name of the data set to be considered\ncols denotes the columns that should be transformed into the longer format\nnames_to denotes the column that includes the names of the new columns\nvalues_to denotes the column that includes the values to be allocated in the newly created cells\n\nThe arguments names_to and values_to are not strictly necessary since they have useful default values, but its usually nicer to be explicit.\nWhen specifying the argument cols you have several possibilities. The simplest variant is to pass a character vector with the column names. But note that you can save a lot of writing by using so called selection helpers, a very useful tool we will learn about later.\nIn our case this amounts to:\n\ndata_raw_long &lt;- tidyr::pivot_longer(\n  data = data_raw_wide, \n  cols = c(\"unemp\", \"gdp\"), \n  names_to = \"indicator\", \n  values_to = \"values\")\ndata_raw_long\n\n# A tibble: 8 × 4\n  country  year indicator   values\n  &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n1 Germany  2017 unemp         3.75\n2 Germany  2017 gdp       53071.  \n3 Germany  2018 unemp         3.38\n4 Germany  2018 gdp       53431.  \n5 Greece   2017 unemp        21.5 \n6 Greece   2017 gdp       28605.  \n7 Greece   2018 unemp        19.3 \n8 Greece   2018 gdp       29141."
  },
  {
    "objectID": "content/tutorials/data-wrangling/index.html#creating-or-manipulating-variables",
    "href": "content/tutorials/data-wrangling/index.html#creating-or-manipulating-variables",
    "title": "Data preparation",
    "section": "Creating or manipulating variables",
    "text": "Creating or manipulating variables\nThe function dplyr::mutate() is used both for manipulating existing columns as well as creating new columns. In the first case the name of the column that the result of dplyr::mutate() is written into already exists, in the second case we just use a new name.\nConsider the following data set with the unemployment rate as an example:\n\ndata_unemp\n\n# A tibble: 2 × 3\n   year Germany Greece\n  &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1  2017    3.75   21.5\n2  2018    3.38   19.3\n\n\nAssume we want to express the percentage values via decimal numbers and, to this end, divide the values in the columns Germany and Greece by 100. We can use dplyr::mutate() to achieve this:\n\ndata_unemp %&gt;%\n  dplyr::mutate(\n    Germany = Germany/100,\n    Greece = Greece/100\n  )\n\n# A tibble: 2 × 3\n   year Germany Greece\n  &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1  2017  0.0375  0.215\n2  2018  0.0338  0.193\n\n\nBut we could use basically the same code to create a new column. Assume, for instance, we want a new column containing the difference between the unemployment rates:\n\ndata_unemp %&gt;%\n  dplyr::mutate(\n    Difference = Greece - Germany\n  )\n\n# A tibble: 2 × 4\n   year Germany Greece Difference\n  &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1  2017    3.75   21.5       17.7\n2  2018    3.38   19.3       15.9\n\n\nThe only difference here was that the left-hand-side name of the column to be manipulated did not exist before!"
  },
  {
    "objectID": "content/tutorials/data-wrangling/index.html#filtering-rows",
    "href": "content/tutorials/data-wrangling/index.html#filtering-rows",
    "title": "Data preparation",
    "section": "Filtering rows",
    "text": "Filtering rows\nThe function dplyr::filter() can be used to filter rows according to certain conditions. The conditions must evaluate for each cell entry to either TRUE or FALSE, and only those rows for which they evaluate to TRUE remain in the data set. Often, the conditions are specified via logical operators, which were already covered in the tutorial on vector types.\nAs always, the first argument to dplyr::filter() is data, i.e. the data set on which you want to operate. Then follow an arbitrary number of logical conditions on the different columns of the data set on question.\nAssume we want to take the previously defined data set data_raw_long\n\ndata_raw_long\n\n# A tibble: 8 × 4\n  country  year indicator   values\n  &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n1 Germany  2017 unemp         3.75\n2 Germany  2017 gdp       53071.  \n3 Germany  2018 unemp         3.38\n4 Germany  2018 gdp       53431.  \n5 Greece   2017 unemp        21.5 \n6 Greece   2017 gdp       28605.  \n7 Greece   2018 unemp        19.3 \n8 Greece   2018 gdp       29141.  \n\n\nand only want to keep data on GDP:\n\ndata_raw_long %&gt;%\n  dplyr::filter(indicator==\"gdp\")\n\n# A tibble: 4 × 4\n  country  year indicator values\n  &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 Germany  2017 gdp       53071.\n2 Germany  2018 gdp       53431.\n3 Greece   2017 gdp       28605.\n4 Greece   2018 gdp       29141.\n\n\nYou may also combine more than one condition in one call to dplyr::filter(). If you also want to filter by values and only keep those rows where the value is below 50.000:\n\ndata_raw_long %&gt;%\n  dplyr::filter(\n    indicator==\"gdp\",\n    values &lt; 50000)\n\n# A tibble: 2 × 4\n  country  year indicator values\n  &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 Greece   2017 gdp       28605.\n2 Greece   2018 gdp       29141."
  },
  {
    "objectID": "content/tutorials/data-wrangling/index.html#selecting-columns",
    "href": "content/tutorials/data-wrangling/index.html#selecting-columns",
    "title": "Data preparation",
    "section": "Selecting columns",
    "text": "Selecting columns\nWhen you only want to keep certain columns we speak of selecting (rather than filtering) columns. This is done - surprise - via the function ´dplyr::select()`.\nThere are different ways for selecting columns. In any case, the first argument is, again, data, i.e. the data set considered. In the present case, we will refer to data_raw:\n\ndata_raw\n\n   country  year unemp      gdp\n    &lt;char&gt; &lt;int&gt; &lt;num&gt;    &lt;num&gt;\n1: Germany  2017  3.75 53071.46\n2: Germany  2018  3.38 53431.39\n3:  Greece  2017 21.49 28604.86\n4:  Greece  2018 19.29 29141.17\n\n\nThen we can now select columns using one of the following two options. First, you may refer to columns via their name:\n\ndata_raw %&gt;%\n  dplyr::select(country, year, unemp)\n\n   country  year unemp\n    &lt;char&gt; &lt;int&gt; &lt;num&gt;\n1: Germany  2017  3.75\n2: Germany  2018  3.38\n3:  Greece  2017 21.49\n4:  Greece  2018 19.29\n\n\nBut this is often error-prone. Thus, it is usually better to refer to the columns via selection helpers, which is also the most flexible version. While we will learn about more selection helpers later, here we will mainly use dplyr::all_of(), which accepts a character vector of column names:\n\ndata_raw %&gt;%\n  dplyr::select(dplyr::all_of(c(\"country\", \"year\", \"gdp\")))\n\n   country  year      gdp\n    &lt;char&gt; &lt;int&gt;    &lt;num&gt;\n1: Germany  2017 53071.46\n2: Germany  2018 53431.39\n3:  Greece  2017 28604.86\n4:  Greece  2018 29141.17\n\n\n\nCaution: Do not forget the c()! Otherwise:\n\n\ndata_raw %&gt;%\n  dplyr::select(dplyr::all_of(\"country\", \"year\", \"gdp\"))\n\nError in `dplyr::select()`:\nℹ In argument: `dplyr::all_of(\"country\", \"year\", \"gdp\")`.\nCaused by error in `dplyr::all_of()`:\n! unused arguments (\"year\", \"gdp\")\n\n\n\nIt is also possible to define the column vector first:\n\n\ncols2keep &lt;- c(\"country\", \"year\", \"gdp\")\ndata_raw %&gt;%\n  dplyr::select(dplyr::all_of(cols2keep))\n\n   country  year      gdp\n    &lt;char&gt; &lt;int&gt;    &lt;num&gt;\n1: Germany  2017 53071.46\n2: Germany  2018 53431.39\n3:  Greece  2017 28604.86\n4:  Greece  2018 29141.17\n\n\n\nSelection helpers allow you to specify the columns to be selected more generally. For instance, dplyr::ends_with() allows you to select all colums that end with a certain pattern:\n\n\ndata_raw %&gt;%\n  dplyr::select(dplyr::ends_with(\"p\"))\n\n   unemp      gdp\n   &lt;num&gt;    &lt;num&gt;\n1:  3.75 53071.46\n2:  3.38 53431.39\n3: 21.49 28604.86\n4: 19.29 29141.17\n\n\nIn any case, you can also specify the columns you want to drop. To this end, just add a - in front of the selection command:\n\ndata_raw %&gt;%\n  dplyr::select(-unemp, -gdp)\n\n   country  year\n    &lt;char&gt; &lt;int&gt;\n1: Germany  2017\n2: Germany  2018\n3:  Greece  2017\n4:  Greece  2018"
  },
  {
    "objectID": "content/tutorials/data-wrangling/index.html#merging-data-sets",
    "href": "content/tutorials/data-wrangling/index.html#merging-data-sets",
    "title": "Data preparation",
    "section": "Merging data sets",
    "text": "Merging data sets\nOften you need to obtain data from different sources. To merge all your data in one single data set, you need to use one of the *_join() functions of the dplyr-package. These functions all merge two data sets, but the way they do it is different. Below we illustrate the most common joins (so called mutating joins).4\nAs a guiding example we use the following two data sets:\nFirst, data on income inequality from the SWIID data base:\n\ngini_join\n\n   country  year  gini\n    &lt;char&gt; &lt;int&gt; &lt;num&gt;\n1:  Greece  2015  33.1\n2:  Greece  2017  32.2\n\n\nSecond, data on GDP per capita from the World Bank:\n\ngdp_join\n\n   country  year      gdp\n    &lt;char&gt; &lt;int&gt;    &lt;num&gt;\n1: Germany  2017 53071.46\n2: Germany  2018 53431.39\n3:  Greece  2017 28604.86\n4:  Greece  2018 29141.17\n\n\nWe will consider the behavior of the following four functions:\n\ndplyr::left_join()\ndplyr::right_join()\ndplyr::full_join()\ndplyr::inner_join()\n\nAll of them accept the following arguments:\n\nx and y: the two data sets to be merged\nby: a vector or a named vector indicating on which columns the data sets should be merged\n\nIts easier to understand their behavior if you contrast them directly with each other. First, dplyr::left_join() joins the data sets on those columns mentioned in by, but only keeps those rows for which x contains an observation:\n\ndplyr::left_join(x = gdp_join, y = gini_join, by = c(\"country\", \"year\"))\n\n   country  year      gdp  gini\n    &lt;char&gt; &lt;int&gt;    &lt;num&gt; &lt;num&gt;\n1: Germany  2017 53071.46    NA\n2: Germany  2018 53431.39    NA\n3:  Greece  2017 28604.86  32.2\n4:  Greece  2018 29141.17    NA\n\n\nThis might introduce NAs into the columns of y, but not of x. It is the other way around for dplyr::right_join(): it only keeps those rows for which y contains an observation:\n\ndplyr::right_join(x = gdp_join, y = gini_join, by = c(\"country\", \"year\"))\n\n   country  year      gdp  gini\n    &lt;char&gt; &lt;int&gt;    &lt;num&gt; &lt;num&gt;\n1:  Greece  2017 28604.86  32.2\n2:  Greece  2015       NA  33.1\n\n\ndplyr::inner_join() is the most restrictive option, keeping only those rows for which both x and y contain an observation (i.e. it never introduces NAs):\n\ndplyr::inner_join(x = gdp_join, y = gini_join, by = c(\"country\", \"year\"))\n\n   country  year      gdp  gini\n    &lt;char&gt; &lt;int&gt;    &lt;num&gt; &lt;num&gt;\n1:  Greece  2017 28604.86  32.2\n\n\nFinally, dplyr::full_join() contains all rows that occur at least in x or y, i.e. it might introduce NAs in both the columns of x and y:\n\ndplyr::full_join(x = gdp_join, y = gini_join, by = c(\"country\", \"year\"))\n\n   country  year      gdp  gini\n    &lt;char&gt; &lt;int&gt;    &lt;num&gt; &lt;num&gt;\n1: Germany  2017 53071.46    NA\n2: Germany  2018 53431.39    NA\n3:  Greece  2017 28604.86  32.2\n4:  Greece  2018 29141.17    NA\n5:  Greece  2015       NA  33.1\n\n\nTwo final remarks: first, the types of the columns on which you merge the data sets must be equal, otherwise R throws an error:\n\ngini_join &lt;- dplyr::mutate(gini_join, year=as.character(year))\ndplyr::left_join(x = gdp_join, y = gini_join, by = c(\"country\", \"year\"))\n\nError in `dplyr::left_join()`:\n! Can't join `x$year` with `y$year` due to incompatible types.\nℹ `x$year` is a &lt;integer&gt;.\nℹ `y$year` is a &lt;character&gt;.\n\n\nJust enforce the correct data type before merging:\n\ngini_join %&gt;% \n  dplyr::mutate(year=as.integer(year)) %&gt;%\n  dplyr::left_join(x = gdp_join, y = ., by = c(\"country\", \"year\"))\n\n   country  year      gdp  gini\n    &lt;char&gt; &lt;int&gt;    &lt;num&gt; &lt;num&gt;\n1: Germany  2017 53071.46    NA\n2: Germany  2018 53431.39    NA\n3:  Greece  2017 28604.86  32.2\n4:  Greece  2018 29141.17    NA\n\n\nSecond, you can also merge on columns with different names by passing named vectors to by:\n\ngini_join &lt;- gini_join %&gt;%\n  mutate(Year=as.double(year)) %&gt;%\n  select(-year)\ngini_join\n\n   country  gini  Year\n    &lt;char&gt; &lt;num&gt; &lt;num&gt;\n1:  Greece  33.1  2015\n2:  Greece  32.2  2017\n\n\nThen this does not work any more:\n\ndplyr::left_join(\n  x = gdp_join, y = gini_join, \n  by = c(\"country\", \"year\"))\n\nError in `dplyr::left_join()`:\n! Join columns in `y` must be present in the data.\n✖ Problem with `year`.\n\n\nBut the named vector fixes it:\n\ndplyr::left_join(\n  x = gdp_join, y = gini_join, \n  by = c(\"country\", \"year\"=\"Year\"))\n\n   country  year      gdp  gini\n    &lt;char&gt; &lt;num&gt;    &lt;num&gt; &lt;num&gt;\n1: Germany  2017 53071.46    NA\n2: Germany  2018 53431.39    NA\n3:  Greece  2017 28604.86  32.2\n4:  Greece  2018 29141.17    NA"
  },
  {
    "objectID": "content/tutorials/data-wrangling/index.html#grouping-and-summarising-data",
    "href": "content/tutorials/data-wrangling/index.html#grouping-and-summarising-data",
    "title": "Data preparation",
    "section": "Grouping and summarising data",
    "text": "Grouping and summarising data\nThe final challenge we consider involves the application of two functions (at least in most cases): dplyr::group_by() and dplyr::summarize().\ndplyr::group_by() is usually used within pipes and groups a data set according to an arbitrary number of variables, each of which must refer to one (and only one) column. It produces a grouped data set:\n\ndata_raw_grouped &lt;- data_raw %&gt;%\n  dplyr::group_by(country)\ndata_raw_grouped\n\n# A tibble: 4 × 4\n# Groups:   country [2]\n  country  year unemp    gdp\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Germany  2017  3.75 53071.\n2 Germany  2018  3.38 53431.\n3 Greece   2017 21.5  28605.\n4 Greece   2018 19.3  29141.\n\n\nAs you can see, the data set is now grouped by the variable country. We can specify the grouping variables the same way we selected columns in the context of dplyr::select() (see above).\nGrouped data sets are usually not interesting in itself. You can ungroup them via dplyr::ungroup():\n\ndata_raw_grouped %&gt;%\n  dplyr::ungroup()\n\n# A tibble: 4 × 4\n  country  year unemp    gdp\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Germany  2017  3.75 53071.\n2 Germany  2018  3.38 53431.\n3 Greece   2017 21.5  28605.\n4 Greece   2018 19.3  29141.\n\n\nThey are most useful if used in conjunction with dplyr::summarise(), which summarizes variables. While it can be used without dplyr::group_by(), it is most useful if it is applied to grouped data sets: then it computes summary statistics for each group.\n\ndata_raw %&gt;%\n  summarise(\n    avg_gdp=mean(gdp)\n  )\n\n   avg_gdp\n1 41062.22\n\n\n\ndata_raw_grouped %&gt;%\n  summarise(\n    avg_gdp=mean(gdp)\n  )\n\n# A tibble: 2 × 2\n  country avg_gdp\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Germany  53251.\n2 Greece   28873.\n\n\nYou can also summarized more than one column:\n\ndata_raw_grouped %&gt;%\n  summarise(\n    avg_gdp=mean(gdp),\n    median_unemp=median(unemp)\n  )\n\n# A tibble: 2 × 3\n  country avg_gdp median_unemp\n  &lt;chr&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 Germany  53251.         3.57\n2 Greece   28873.        20.4 \n\n\nNote that dplyr::summarise() drops all columns that it is not asked to compute summary statistics for, except potential grouping variables. There are also some advanced features of the functions, which are explained in the official documentation."
  },
  {
    "objectID": "content/tutorials/data-wrangling/index.html#footnotes",
    "href": "content/tutorials/data-wrangling/index.html#footnotes",
    "title": "Data preparation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf we had a data set with three instead of two variables, the wide data set would have the same number of rows, but more columns, i.e. it would be wider in an absolute sense as well.↩︎\n The function allows for much more finetuning. You might read more about its argument in the help page of the function or the online documentation.↩︎\n See the online documentation for a more complete description.↩︎\n The other join types are filtering joins and nest joins. You find more information in the web, and more details on the underlying theory in chapter 13 of R4DS.↩︎\nWe have not yet covered the function ifelse(). It contains a logical test as a first argument, and then two further arguments: one return value for the case in which the test returns TRUE, and one for which the test returns FALSE.↩︎"
  },
  {
    "objectID": "content/material/session09.html",
    "href": "content/material/session09.html",
    "title": "🗓️ Session 9: A very short introduction to Quarto",
    "section": "",
    "text": "Quarto is a modern multi-language version of R Markdown. As with its predecessor, the idea is to provide people with the opportunity to write text and code into the very same document. This makes the creation of nice looking and reproducible reports or paper very easy. Moreoever, with Quarto it is very easy to create very nice papers, reports, websites or interactive apps. This website, for example, is fully written in Quarto. In this lecture, you learn everything you need to get started with writing your first Quarto documents. In fact, its really straightforward once you get the basic idea.",
    "crumbs": [
      "🗓️ Material",
      "Session 09"
    ]
  },
  {
    "objectID": "content/material/session09.html#lecture-slides",
    "href": "content/material/session09.html#lecture-slides",
    "title": "🗓️ Session 9: A very short introduction to Quarto",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to download the slides.\n\n  \n\n\nDesasterMarkdown.pdf\nNicerMarkdown.pdf\n\n\n\n\n\n\n\nCode for the markdown desaster and a possible solution",
    "crumbs": [
      "🗓️ Material",
      "Session 09"
    ]
  },
  {
    "objectID": "content/material/session09.html#lecture-videos",
    "href": "content/material/session09.html#lecture-videos",
    "title": "🗓️ Session 9: A very short introduction to Quarto",
    "section": "🎥 Lecture videos",
    "text": "🎥 Lecture videos\nAll the videos are available via this playlist.",
    "crumbs": [
      "🗓️ Material",
      "Session 09"
    ]
  },
  {
    "objectID": "content/material/session09.html#mandatory-reading",
    "href": "content/material/session09.html#mandatory-reading",
    "title": "🗓️ Session 9: A very short introduction to Quarto",
    "section": "📚 Mandatory Reading",
    "text": "📚 Mandatory Reading\n\nThe CommonMark markdown tutorial\nQuarto tutorial I: the basics\nQuarto tutorial II: computations\nQuarto tutorial III: authoring quarto documents",
    "crumbs": [
      "🗓️ Material",
      "Session 09"
    ]
  },
  {
    "objectID": "content/material/session09.html#further-reading",
    "href": "content/material/session09.html#further-reading",
    "title": "🗓️ Session 9: A very short introduction to Quarto",
    "section": "Further Reading",
    "text": "Further Reading\n\nBlog introducing Quarto\nQuarto and R Markdown\nThe comprehensive Quarto documentation\nMarkdown basics\nThe R Markdown Cookbook",
    "crumbs": [
      "🗓️ Material",
      "Session 09"
    ]
  },
  {
    "objectID": "content/material/session09.html#coursework",
    "href": "content/material/session09.html#coursework",
    "title": "🗓️ Session 9: A very short introduction to Quarto",
    "section": "✍️ Coursework",
    "text": "✍️ Coursework\n\nDo the exercises Quarto from the DataScienceExercises package\n\n\n\n\n\n\n\nQuick code for starting the exercises\n\n\n\n\n\n\nlearnr::run_tutorial(\n  name = \"Quarto\", \n  package = \"DataScienceExercises\", \n  shiny_args=list(\"launch.browser\"=TRUE))\n\n\n\n\n\nDo the following practical exercise:\n\n\n\n\n\n\n\nExercise description\n\n\n\n\n\nCreate a new Quarto document where you set the title, date, and the author explicitly. Write a sample text that comprises…\n\n…at least one level 1 heading\n…at least two level 2 headings\n…a YAML part that specifies that R code remains hidden by default\n…one R chunk where both the output and the code is printed in the final document\n…one R chunk that produces a simply ggplot object and where the code producing the plot is hidden\n\nThen do the following:\n\nKnit the document to html with a floating table of contents and a special theme.\nMake the document available via Netlify Drop and add the possibility to download the underlying Rmd file. &gt; Note: For Netlify Drop to work, the html file must be called `index.html```!\nKnit the document to PDF and make sure that it includes a table of contents.\n\n\n\n\n\n\n\n\n\n\nPossible solution",
    "crumbs": [
      "🗓️ Material",
      "Session 09"
    ]
  },
  {
    "objectID": "content/material/session08-ex-solution.html",
    "href": "content/material/session08-ex-solution.html",
    "title": "Sample solution for analyzing sales data",
    "section": "",
    "text": "This is an example solution revealing one of the hidden patterns in the data.\nWe use the following packages:\n\nlibrary(readr)\nlibrary(ggplot2)\n\nLoading the data:\n\ncustomer_data &lt;- read_csv(\"customer_sample.csv\")\n\nRows: 200 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): customer_type, preferred_category\ndbl (4): customer_id, age, spending, visit_frequency\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCreate a figure on the relationship between age, spending, and product category:\n\nggplot(customer_data, aes(x = age, y = spending, color = preferred_category, size = visit_frequency)) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(aes(group = preferred_category), method = \"loess\", se = FALSE, linewidth = 1) +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_size_continuous(range = c(1, 8)) +\n  labs(\n    title = \"Customer Spending Patterns by Age and Product Category\",\n    subtitle = \"Bubble size represents visit frequency\",\n    x = \"Customer Age\",\n    y = \"Monthly Spending ($)\",\n    color = \"Preferred Category\",\n    size = \"Visits per Month\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: size.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n\nFrom his visualization we can infer the following insights: 1. The non-linear relationship between age and spending (peaks at middle age) 2. Electronics customers spend more across all age groups 3. Visit frequency (bubble size) correlates with spending 4. Different product categories have different age-spending curves 5. Some categories are more popular with specific age groups\nOne conclusion for a marketing strategy could be:\n\nMiddle-aged customers (35-55) have the highest spending across all categories, with Electronics shoppers spending significantly more. A targeted campaign for Electronics products to middle-aged customers combined with cross-selling opportunities to younger and older demographics could maximize revenue."
  },
  {
    "objectID": "content/material/session07.html",
    "href": "content/material/session07.html",
    "title": "🗓️ Session 7: Importing data",
    "section": "",
    "text": "This is also one of the most underestimated topics. This session builds upon the previous session on setting up a project environment. Now you will learn how to import data obtained elsewhere into R. We focus on importing .csv files, but with the skills acquired in this context you will face no difficulties in importing other data types as well.",
    "crumbs": [
      "🗓️ Material",
      "Session 07"
    ]
  },
  {
    "objectID": "content/material/session07.html#lecture-slides",
    "href": "content/material/session07.html#lecture-slides",
    "title": "🗓️ Session 7: Importing data",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to download the slides.\n\n  \n\n\nData for the exercises",
    "crumbs": [
      "🗓️ Material",
      "Session 07"
    ]
  },
  {
    "objectID": "content/material/session07.html#lecture-videos",
    "href": "content/material/session07.html#lecture-videos",
    "title": "🗓️ Session 7: Importing data",
    "section": "🎥 Lecture videos",
    "text": "🎥 Lecture videos\nAll the videos are available via this playlist.",
    "crumbs": [
      "🗓️ Material",
      "Session 07"
    ]
  },
  {
    "objectID": "content/material/session07.html#mandatory-reading",
    "href": "content/material/session07.html#mandatory-reading",
    "title": "🗓️ Session 7: Importing data",
    "section": "📚 Mandatory Reading",
    "text": "📚 Mandatory Reading\n\nThe tutorial Importing and exporting data using data.table",
    "crumbs": [
      "🗓️ Material",
      "Session 07"
    ]
  },
  {
    "objectID": "content/material/session07.html#further-reading",
    "href": "content/material/session07.html#further-reading",
    "title": "🗓️ Session 7: Importing data",
    "section": "Further Reading",
    "text": "Further Reading\nTBA",
    "crumbs": [
      "🗓️ Material",
      "Session 07"
    ]
  },
  {
    "objectID": "content/material/session07.html#coursework",
    "href": "content/material/session07.html#coursework",
    "title": "🗓️ Session 7: Importing data",
    "section": "✍️ Coursework",
    "text": "✍️ Coursework\n\nDo the exercises ProjectOrga from the DataScienceExercises package\n\n\n\n\n\n\n\nQuick code for starting the exercises\n\n\n\n\n\n\nlearnr::run_tutorial(\n  name = \"ProjectOrga\", \n  package = \"DataScienceExercises\", \n  shiny_args=list(\"launch.browser\"=TRUE))",
    "crumbs": [
      "🗓️ Material",
      "Session 07"
    ]
  },
  {
    "objectID": "content/material/session05.html",
    "href": "content/material/session05.html",
    "title": "🗓️ Session 5: Recap and practice",
    "section": "",
    "text": "This session is about recap and practice. We will do exercises on topics that you suggest, and recap concepts you found particularly hard to grasp. To this end, make sure you communicate your preferences on topics via Moodle until one week before this session.",
    "crumbs": [
      "🗓️ Material",
      "Session 05"
    ]
  },
  {
    "objectID": "content/material/session05.html#lecture-script",
    "href": "content/material/session05.html#lecture-script",
    "title": "🗓️ Session 5: Recap and practice",
    "section": "👨‍🏫 Lecture script",
    "text": "👨‍🏫 Lecture script\n\nThe script summarizing the key parts of the session",
    "crumbs": [
      "🗓️ Material",
      "Session 05"
    ]
  },
  {
    "objectID": "content/material/session05.html#lecture-videos",
    "href": "content/material/session05.html#lecture-videos",
    "title": "🗓️ Session 5: Recap and practice",
    "section": "🎥 Lecture videos",
    "text": "🎥 Lecture videos\nThere will be no videos for recap sessions.",
    "crumbs": [
      "🗓️ Material",
      "Session 05"
    ]
  },
  {
    "objectID": "content/material/session05.html#suggested-reading",
    "href": "content/material/session05.html#suggested-reading",
    "title": "🗓️ Session 5: Recap and practice",
    "section": "📚 Suggested Reading",
    "text": "📚 Suggested Reading\n\nThe tutorial Setting up an R project\nFundamental object types in R I: Functions\nThe section on lists in Fundamental object types in R II: Vectors\nThe section on factors in Fundamental object types in R III: Factors and data frames",
    "crumbs": [
      "🗓️ Material",
      "Session 05"
    ]
  },
  {
    "objectID": "content/material/session05.html#coursework",
    "href": "content/material/session05.html#coursework",
    "title": "🗓️ Session 5: Recap and practice",
    "section": "✍️ Coursework",
    "text": "✍️ Coursework\nDuring the fthe session you will work on this sheet in groups of three people. Please use only one computer but develop the solutions together. LAter we will go through your solutions and discuss problems. I will post my own solutions online after the session, but urge to first try to come up with solutions on your own.\n\nExercise sheet for this session\nR file template for your solutions\nPossible solutions to the exercises",
    "crumbs": [
      "🗓️ Material",
      "Session 05"
    ]
  },
  {
    "objectID": "content/material/session05-notes.html",
    "href": "content/material/session05-notes.html",
    "title": "First recap session",
    "section": "",
    "text": "library(DataScienceExercises)"
  },
  {
    "objectID": "content/material/session05-notes.html#using-functions",
    "href": "content/material/session05-notes.html#using-functions",
    "title": "First recap session",
    "section": "Using functions",
    "text": "Using functions\nFunctions are algorithms that apply a certain routine on an input, thereby producing (almost always) an output. The function sqrt(), for instance, takes as input a number and returns as output another number, namely the square root of the input:\n\nsqrt(2.5)\n\n[1] 1.581139\n\n\nTo call a function we first write its name, followed by parentheses. Inside the parentheses we can specify the arguments of the function. Most of the time, the first argument(s) are the input(s).\nOften, you can give more arguments to the function. These specify how the algorithm of the function works. If you are not sure which arguments a function accepts, you can always use help().\nLet us consider the example of mean(). This function takes a vector of numbers as input and returns the mean of these numbers:\n\ntest_1 &lt;- c(1, 4, 2, 4, 9, NA, 44)\nmean(test_1)\n\n[1] NA\n\n\nThis result might be surprising. Lets inspect mean() further:\n\nhelp(mean)\n\nWe see that the first argument of mean() is called x. We can, but do not have to specify this name for the first argument. Neither we must for the further argumens, but this is highly recommended.\nThere are two more arguments for mean(): trim and na.rm. In our case, the second one is relevant: if na.rm is set to TRUE, the function removes all NA values from the input before computing the mean:\n\nmean(test_1, na.rm = TRUE)\n\n[1] 10.66667"
  },
  {
    "objectID": "content/material/session05-notes.html#defining-functions",
    "href": "content/material/session05-notes.html#defining-functions",
    "title": "First recap session",
    "section": "Defining functions",
    "text": "Defining functions\nWhenever you perform certain actions several times, it is often a good idea to define a function. This way, you can call the function instead of writing the code for the action every time anew.\nAssume we want to define a function that takes as input a vector of numbers and returns the sum of the natural logarithms of these numbers.\nWe define a new function by using the function function(). - We start our definition by associating the new function with a name (here: log_sum) so that we can use it later. - The arguments to function() are then arguments that our new function should accept. In our case, we only have one argument. - After that comes the function body. It contains all the routines that the function should execute when called. The function body is always enclosed by curly brackets. - Finally, we use the return() function to specify what the function should return. This is not strictly necessary, but it is a good practice to do so.\n\nlog_sum &lt;- function(input_vector){\n  logs &lt;- log(input_vector)\n  sum_logs &lt;- sum(logs)\n  return(sum_logs)\n}\n\nWe can then call the function by name:\n\nlog_sum(c(1, 2, 3, 4, 5))\n\n[1] 4.787492"
  },
  {
    "objectID": "content/material/session04.html",
    "href": "content/material/session04.html",
    "title": "🗓️ Session 4: Object types in R",
    "section": "",
    "text": "In the first part of this session you learn some additional details about R packages and about the most important and most fundamental object types in R, such as decimal numbers or words. While this might look a bit boring at first, understanding these basic types is fundamental for all the more advanced (and exiting) stuff in the future!\nIn the second part of this session, you are then introduced to two more advanced object types in R. The two object types covered, factors and data frames, are advanced in the sense that they can be thought of extensions of some of the basic object types you encountered before: factors and special kinds of integers, and data frames are special kinds of lists.",
    "crumbs": [
      "🗓️ Material",
      "Session 04"
    ]
  },
  {
    "objectID": "content/material/session04.html#lecture-slides",
    "href": "content/material/session04.html#lecture-slides",
    "title": "🗓️ Session 4: Object types in R",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nFor the slides of the first part of this session (Packages and basic object types) on click on the slide area below or click here to download the slides.\n\n  \n\n\n\n\n\n\n\nSolutions to the intermediate exercises of part 1\n\n\n\n\n\n\n\n\n\nFor the slides of the second part of this session (Advanced object types) see the slide area below or click here to download the slides.\n\n  \n\n\n\n\n\n\n\nNotes and solutions for the second part of this session",
    "crumbs": [
      "🗓️ Material",
      "Session 04"
    ]
  },
  {
    "objectID": "content/material/session04.html#lecture-videos",
    "href": "content/material/session04.html#lecture-videos",
    "title": "🗓️ Session 4: Object types in R",
    "section": "🎥 Lecture videos",
    "text": "🎥 Lecture videos\nAll the videos for the first part on basic object types are available via this playlist.\n\n\n\nAll the videos for the second part on advanced object types are available via this playlist.",
    "crumbs": [
      "🗓️ Material",
      "Session 04"
    ]
  },
  {
    "objectID": "content/material/session04.html#mandatory-reading",
    "href": "content/material/session04.html#mandatory-reading",
    "title": "🗓️ Session 4: Object types in R",
    "section": "📚 Mandatory Reading",
    "text": "📚 Mandatory Reading\nRead the following tutorials for the first part on basic object types:\n\nFundamental object types in R I: Functions\nFundamental object types in R II: Vectors\n\nThe topics of the second part on advanced object types are covered here:\n\nFundamental object types in R III: Factors and data frames\n\n\n🏆 Further readings\nI suggest you read these references after you learned about data wrangling techniques in session 10.\n\nSections 1-3 in Chapter 12 of Wickham et al. (2023).\nChapter 13 in Wickham et al. (2023).\nChapter 14 in Wickham et al. (2023).\nChapter 16 in Wickham et al. (2023).",
    "crumbs": [
      "🗓️ Material",
      "Session 04"
    ]
  },
  {
    "objectID": "content/material/session04.html#coursework",
    "href": "content/material/session04.html#coursework",
    "title": "🗓️ Session 4: Object types in R",
    "section": "✍️ Coursework",
    "text": "✍️ Coursework\n\nDo the ObjectTypes1 exercises of the package DataScienceExercises for the first part on basic object types:\n\n\n\n\n\n\n\nQuick code for starting the exercises\n\n\n\n\n\n\nlearnr::run_tutorial(\n  name = \"ObjectTypes1\", \n  package = \"DataScienceExercises\", \n  shiny_args=list(\"launch.browser\"=TRUE))\n\n\n\n\n\nFor the second part do the ObjectTypes2 exercises of the package DataScienceExercises\n\n\n\n\n\n\n\nQuick code for starting the exercises\n\n\n\n\n\n\nlearnr::run_tutorial(\n  name = \"ObjectTypes2\", \n  package = \"DataScienceExercises\", \n  shiny_args=list(\"launch.browser\"=TRUE))\n\n\n\n\n\nIf you have questions or problems, please post them in the Moodle forum",
    "crumbs": [
      "🗓️ Material",
      "Session 04"
    ]
  },
  {
    "objectID": "content/material/session02.html",
    "href": "content/material/session02.html",
    "title": "🗓️ Session 2: First steps in R",
    "section": "",
    "text": "In this session, you will learn about how to use the integrated development environment R-Studio to edit and execute R script and about the basic commands in R. Moreover, you will be introduced to two fundamental concepts: assignments and functions.",
    "crumbs": [
      "🗓️ Material",
      "Session 02"
    ]
  },
  {
    "objectID": "content/material/session02.html#lecture-slides",
    "href": "content/material/session02.html#lecture-slides",
    "title": "🗓️ Session 2: First steps in R",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to download the slides.\n\n  \n\n\n\n\n\n\n\nThe R script of this session",
    "crumbs": [
      "🗓️ Material",
      "Session 02"
    ]
  },
  {
    "objectID": "content/material/session02.html#accompanying-lecture-videos",
    "href": "content/material/session02.html#accompanying-lecture-videos",
    "title": "🗓️ Session 2: First steps in R",
    "section": "🎥 Accompanying lecture videos",
    "text": "🎥 Accompanying lecture videos\nAll the videos are available via this playlist.",
    "crumbs": [
      "🗓️ Material",
      "Session 02"
    ]
  },
  {
    "objectID": "content/material/session02.html#mandatory-reading",
    "href": "content/material/session02.html#mandatory-reading",
    "title": "🗓️ Session 2: First steps in R",
    "section": "📚 Mandatory Reading",
    "text": "📚 Mandatory Reading\n\nTutorial First steps in R\nChapter 2 in Wickham et al. (2023).\n\n\n🏆 Further readings\n\nR Studio Cheat Sheet\nChapter 4 in Wickham et al. (2023).\nChapter 8 in Wickham et al. (2023).",
    "crumbs": [
      "🗓️ Material",
      "Session 02"
    ]
  },
  {
    "objectID": "content/material/session02.html#coursework",
    "href": "content/material/session02.html#coursework",
    "title": "🗓️ Session 2: First steps in R",
    "section": "✍️ Coursework",
    "text": "✍️ Coursework\n\nDo the Basics exercises of the package DataScienceExercises\n\n\n\n\n\n\n\nQuick code for starting the exercises\n\n\n\n\n\n\nlearnr::run_tutorial(\n  name = \"Basics\", \n  package = \"DataScienceExercises\", \n  shiny_args=list(\"launch.browser\"=TRUE))\n\n\n\n\n\nDo the Functions exercises of the package DataScienceExercises\n\n\n\n\n\n\n\nQuick code for starting the exercises\n\n\n\n\n\n\nlearnr::run_tutorial(\n  name = \"Functions\", \n  package = \"DataScienceExercises\", \n  shiny_args=list(\"launch.browser\"=TRUE))\n\n\n\n\n\nIf you have questions or problems, please post them in the Moodle forum",
    "crumbs": [
      "🗓️ Material",
      "Session 02"
    ]
  },
  {
    "objectID": "content/material/index.html",
    "href": "content/material/index.html",
    "title": "Material overview",
    "section": "",
    "text": "The following textbooks offer a good general reference to the course content, and I think its a good idea to read into these books in a general way. Moreover, I often point to chapters in the respective session pages.\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data (2nd edition). O’Reilly. https://r4ds.hadley.nz/\nIsmay, C., & Kim, A. Y.-S. (2020). Statistical inference via data science: A ModernDive, into R and the tidyverse. CRC Press, Taylor and Francis Group. https://moderndive.com/index.html\n\nFor more advanced details on the fundamentals of programming in R, I recommend the following:\n\nWickham, H. (2019). Advanced R (Second edition). CRC Press/Taylor & Francis Group. https://adv-r.hadley.nz/\n\nFor the model-related parts of the lecture I recommend the following book as a further reading reference:\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An introduction to statistical learning: With applications in R (Second edition). Springer. https://www.statlearning.com/",
    "crumbs": [
      "🗓️ Material",
      "Overview"
    ]
  },
  {
    "objectID": "content/material/index.html#general-references",
    "href": "content/material/index.html#general-references",
    "title": "Material overview",
    "section": "",
    "text": "The following textbooks offer a good general reference to the course content, and I think its a good idea to read into these books in a general way. Moreover, I often point to chapters in the respective session pages.\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data (2nd edition). O’Reilly. https://r4ds.hadley.nz/\nIsmay, C., & Kim, A. Y.-S. (2020). Statistical inference via data science: A ModernDive, into R and the tidyverse. CRC Press, Taylor and Francis Group. https://moderndive.com/index.html\n\nFor more advanced details on the fundamentals of programming in R, I recommend the following:\n\nWickham, H. (2019). Advanced R (Second edition). CRC Press/Taylor & Francis Group. https://adv-r.hadley.nz/\n\nFor the model-related parts of the lecture I recommend the following book as a further reading reference:\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An introduction to statistical learning: With applications in R (Second edition). Springer. https://www.statlearning.com/",
    "crumbs": [
      "🗓️ Material",
      "Overview"
    ]
  },
  {
    "objectID": "content/material/index.html#session-specific-material",
    "href": "content/material/index.html#session-specific-material",
    "title": "Material overview",
    "section": "🔖 Session-specific material",
    "text": "🔖 Session-specific material\n\n\n\n\n\n\nHow to use the exercise codes?\n\n\n\n\n\nFor information on how to use the exercise code, read this tutorial.\n\n\n\n\n\n\nSession\nTopic\nExercise code\n\n\n\n\n1\nGeneral introduction\n\n\n\n2\nBasics of R and R-Studio\nBasics, Functions\n\n\n3\nProject management\nProjectOrga\n\n\n4\nObject types\nObjectTypes1, ObjectTypes2\n\n\n5\nRecap & practice\n\n\n\n6\nVisualization\nVisualization1\n\n\n7\nQuarto\nQuarto\n\n\n8\nAI for coding\n\n\n\n9\nImporting data\nWrangling1, Wrangling2\n\n\n10\nPreparing data\nWrangling1, Wrangling2\n\n\n11\nTBD\n\n\n\n12\nLinear models\nLinearRegression1, LinearRegression2\n\n\n13\nAnalyzing experimental data\n\n\n\n14\nRecap & practice",
    "crumbs": [
      "🗓️ Material",
      "Overview"
    ]
  },
  {
    "objectID": "content/statrecap.html",
    "href": "content/statrecap.html",
    "title": "A short and friendly recap of mathematical and statistical fundamentals",
    "section": "",
    "text": "Here I provide you with explanations of the basics in statistics and mathematics that are required for this course, as explicated in the seminar description.\nThis is ongoing work, so I greatly value your feedback and your suggestions on how to make this material more comprehensive and accessible.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\n1: What is Statistics all about? Conceptual Foundations\n\n\n\n\n\n\n2: Data Fundamentals\n\n\n\n\n\n\n3: Descriptive Statistics\n\n\n\n\n\n\n4: Essentials in Probability Theory for Statistics\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "📈 Statistics recap"
    ]
  },
  {
    "objectID": "content/exercises.html",
    "href": "content/exercises.html",
    "title": "Additional exercises",
    "section": "",
    "text": "Here are some additional exercises, on top of those provided by the exercise package.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Session date - Oldest\n      \n      \n        Session date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nSession date\n\n\n\nTitle\n\n\n\n\n\n\n\n\nMarch 28, 2024\n\n\nBasic object types: exercises\n\n\n\n\n\n\nMay 16, 2024\n\n\nQuarto exercises\n\n\n\n\n\n\nJune 7, 2024\n\n\nExercises on multiple linear regression\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Theoretical and Empirical Research Methodology",
    "section": "",
    "text": "📑 The course in brief\nFocus: This course is part of the module Theoretical and Empirical Research Methodology. It complements the theoretical lectures by introducing you to the statistical programming language R and acquire practical knowledge about how to implement essential research tools and all theoretical concepts discussed during the main lecture in R.\nHow: The implementation lab comprises a mixture of (i) lectures, in which I introduce concepts in the classroom, (ii) automated hands-on exercises for you to do at home on your own and (iii) instructional videos for topics that are best learned and practiced by yourself. Of course, you are always invited to post your questions to the course forum on Moodle and more intricate problems can be solved in class.\nPrerequisites: The lab does not require you to have any prior knowledge in R or any other programming language. Depending on your prior knowledge or affinity to programming, the course will be quite demanding, but equip you with computational skills that are most valuable both within academia and the business world.\n\n\n🎯 Learning Objectives\n\nUse R together with the integrated development environment R Studio\nUnderstand the use of R packages to perform specific data analytic tasks\nWrite reproducible data analysis reports using Quarto\nTransform raw data into tidy data, which is suitable for further analysis\nChoose and justify the correct visualization approach, and create appealing visualizations using the R package ggplot2\nImplement and interpret linear regression models with numerical and categorial variables\nAnalyze experimental data in R\n\n\n The main course on moodle",
    "crumbs": [
      "🏠 Home"
    ]
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Theoretical and Empirical Research Methodology",
    "section": "",
    "text": "📍 Lecture\nWednesday 14:15-15:45 in MAD 131 & Fridays 14:15-15:45 in MAD 131 (biweekly, see seminar timetable)\nPlease make sure to check out the most recent version of the seminar timetable since lecture dates might differ from one week to another. The most recent version can be found in Moodle.\n\n\n💻 Exercises\nThere is an R package with interactive exercises for this course. I strongly recommend to do the exercises for each session since regular practice is the most important determinant in your success in learning R. You find more information on how to use the exercises in this tutorial. On top of that, there are some additional exercises on selected topics here.\n\n\n📂 Material\nYou find an overview about all lecture materials, such as slides and reading lists here. Complete material lists for the single sessions are distributed via the respective session pages. Lecture videos can, so far, only be accessed via Moodle. EUF students can register for the Moodle course 15556 using the password ResearchMethods25.\n\n\n📖 Tutorials\nComplementary to the lectures there are also short tutorials, which explain certain concepts in a more detailed and applied manner. You can find an overview over all tutorials here.\n\n\n💌 Contact and discussion\nFor asking questions and starting discussions, please use the forum in Moodle.",
    "crumbs": [
      "🏢 Getting Started"
    ]
  },
  {
    "objectID": "content/tutorials.html",
    "href": "content/tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Here are tutorials, meant to complement the course.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Session date - Oldest\n      \n      \n        Session date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nSession date\n\n\n\nTitle\n\n\n\n\n\n\n\n\nMarch 14, 2025\n\n\nInstallation of the necessary software\n\n\n\n\n\n\nMarch 14, 2025\n\n\nInstalling R packages\n\n\n\n\n\n\nMarch 14, 2025\n\n\nUsing the exercise package\n\n\n\n\n\n\nMarch 21, 2025\n\n\nFirst steps in R\n\n\n\n\n\n\nMarch 28, 2025\n\n\nSetting up an R project\n\n\n\n\n\n\nApril 4, 2025\n\n\nFundamental object types in R I: Functions\n\n\n\n\n\n\nApril 4, 2025\n\n\nFundamental object types in R II: Vectors\n\n\n\n\n\n\nApril 4, 2025\n\n\nFundamental object types in R III: Factors and data frames\n\n\n\n\n\n\nApril 25, 2025\n\n\nVisualization\n\n\n\n\n\n\nMay 16, 2025\n\n\nImporting and exporting data\n\n\n\n\n\n\nMay 23, 2025\n\n\nData preparation\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "💻 Tutorials"
    ]
  },
  {
    "objectID": "content/material/session01.html",
    "href": "content/material/session01.html",
    "title": "🗓️ Session 01 - Introduction, overview and installation",
    "section": "",
    "text": "In this first week, we will cover what you can expect to learn from these exercises and how they differ from the main lecture. I will elaborate a bit on why I chose R for this course, what you can do with it, and what has helped previous participants helped excelling in this course. Finally, I explain a bit what R is, how it relates to R Studio and LaTeX.",
    "crumbs": [
      "🗓️ Material",
      "Session 01"
    ]
  },
  {
    "objectID": "content/material/session01.html#lecture-slides",
    "href": "content/material/session01.html#lecture-slides",
    "title": "🗓️ Session 01 - Introduction, overview and installation",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to download the slides.",
    "crumbs": [
      "🗓️ Material",
      "Session 01"
    ]
  },
  {
    "objectID": "content/material/session01.html#accompanying-lecture-videos",
    "href": "content/material/session01.html#accompanying-lecture-videos",
    "title": "🗓️ Session 01 - Introduction, overview and installation",
    "section": "🎥 Accompanying lecture videos",
    "text": "🎥 Accompanying lecture videos\nAll the videos are available via this playlist.",
    "crumbs": [
      "🗓️ Material",
      "Session 01"
    ]
  },
  {
    "objectID": "content/material/session01.html#mandatory-reading",
    "href": "content/material/session01.html#mandatory-reading",
    "title": "🗓️ Session 01 - Introduction, overview and installation",
    "section": "📚 Mandatory Reading",
    "text": "📚 Mandatory Reading\nRead the following tutorials:\n\nInstallation of the necessary software\nInstalling R packages\nUsing the exercise package",
    "crumbs": [
      "🗓️ Material",
      "Session 01"
    ]
  },
  {
    "objectID": "content/material/session01.html#coursework",
    "href": "content/material/session01.html#coursework",
    "title": "🗓️ Session 01 - Introduction, overview and installation",
    "section": "✍️ Coursework",
    "text": "✍️ Coursework\n\nMake sure you installed R, R-Studio, Git and all the required R packages\nIf you have questions or problems, please post them in the Moodle forum",
    "crumbs": [
      "🗓️ Material",
      "Session 01"
    ]
  },
  {
    "objectID": "content/material/session03.html",
    "href": "content/material/session03.html",
    "title": "🗓️ Session 3: Project management",
    "section": "",
    "text": "This is one of the most underestimated topics. In this session you learn how to adequately set up your working environment on your computer. This means where you should save which files, which directory structure you should use, and how to point the computer to other files on your computer. While this sounds boring at first, taking seriously the insights from this session will save you tons of hours of frustration in the future!",
    "crumbs": [
      "🗓️ Material",
      "Session 03"
    ]
  },
  {
    "objectID": "content/material/session03.html#lecture-slides",
    "href": "content/material/session03.html#lecture-slides",
    "title": "🗓️ Session 3: Project management",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to download the slides.\n\n  \n\n\n\n\n\n\n\nLecture script",
    "crumbs": [
      "🗓️ Material",
      "Session 03"
    ]
  },
  {
    "objectID": "content/material/session03.html#lecture-videos",
    "href": "content/material/session03.html#lecture-videos",
    "title": "🗓️ Session 3: Project management",
    "section": "🎥 Lecture videos",
    "text": "🎥 Lecture videos\nCurrently, no videos are available for this session. Check out the tutorial on project setup instead.",
    "crumbs": [
      "🗓️ Material",
      "Session 03"
    ]
  },
  {
    "objectID": "content/material/session03.html#mandatory-reading",
    "href": "content/material/session03.html#mandatory-reading",
    "title": "🗓️ Session 3: Project management",
    "section": "📚 Mandatory Reading",
    "text": "📚 Mandatory Reading\n\nThe tutorial Setting up an R project",
    "crumbs": [
      "🗓️ Material",
      "Session 03"
    ]
  },
  {
    "objectID": "content/material/session03.html#further-reading",
    "href": "content/material/session03.html#further-reading",
    "title": "🗓️ Session 3: Project management",
    "section": "Further Reading",
    "text": "Further Reading\nWhile we do not cover this in this course, I highly recommend learning the version control system Git. Personally, I can recommend the following:\n\nVery concise introduction to Git\nIntroduction to GitHub: GitHub skills and the official docs",
    "crumbs": [
      "🗓️ Material",
      "Session 03"
    ]
  },
  {
    "objectID": "content/material/session03.html#coursework",
    "href": "content/material/session03.html#coursework",
    "title": "🗓️ Session 3: Project management",
    "section": "✍️ Coursework",
    "text": "✍️ Coursework\n\nDo the exercises ProjectOrga from the DataScienceExercises package that refer to setting up an R project\nIf you still have problems with R Packages, re-read the respective section of the tutorial from session 2\n\n\n\n\n\n\n\nQuick code for starting the exercises\n\n\n\n\n\n\nlearnr::run_tutorial(\n  name = \"ProjectOrga\", \n  package = \"DataScienceExercises\", \n  shiny_args=list(\"launch.browser\"=TRUE))",
    "crumbs": [
      "🗓️ Material",
      "Session 03"
    ]
  },
  {
    "objectID": "content/material/session05-exercises.html",
    "href": "content/material/session05-exercises.html",
    "title": "Exercises for Recap Session 1",
    "section": "",
    "text": "Exercise 1: Basic object types I\n\nCreate a vector containing the numbers 2, 5, 2.4 and 11.\nReplace the second element with 5.9.\nAdd the elements 3 and 1 to the beginning, and the elements \"8.0\" and \"9.2\" to the end of the vector.\nCreate a vector with the numbers from -8 to 9 (step size: 0.5)\nCompute the square root of each element of the first vector using vectorisation.\nCreate a character vector containing then strings \"Number_1\" to \"Number_5\". Use suitable helper functions to create this vector quickly.\n\n\n\nExercise 2: Basic object types II\nConsider the following vector:\n\nex_2_vec &lt;- c(1, \"2\", FALSE)\n\n\nWhat is the type of this vector? Why?\nWhat happens if you coerce this vector into type integer? Why?\nWhat does sum(is.na(x)) tell you about a vector x? What is happening here?\nIs it a good idea to use as.integer() on double characters to round them to the next integer? Why (not)? What other ways are there to do the rounding?\n\n\n\nExercise 3: Define a function\nCreate functions that take a vector as input and returns:\n\nThe last value.\nEvery element except the last value and any missing values.\nOnly even numbers.\n\n\nHint: Use the operation x %% y to get the remainder from diving x by y, the so called ‘modulo y’. For even numbers, the modulo 2 is zero.\n\nApply your function to the following example vector:\n\nex_3_vec &lt;- c(1, -8, 99, 3, NA, 3, -0.5)\n\n\n\nExercise 4: Lists\n\nCreate a list that contains three elements called 'a', 'b' and 'c'. The first element should correspond to a double vector with the elements 1.5, -2.9 and 99. The second element should correspond to a character vector with the elments 'Hello', '3', and 'EUF'. The third element should contain three times the entry FALSE.\nTransform this list into a data.frame and a tibble. Then apply str() to get information about the respective structure. How do the results differ?\n\n\n\nExercise 5: Data frames and the study semester distribution at EUF\nThe package DataScienceExercises contains a data set called EUFstudentsemesters, which contains information about the distribution of study semesters of enrolled students at the EUF in 2021. You can shortcut the data set as follows:\n\neuf_semesters &lt;- DataScienceExercises::EUFstudentsemesters\n\n\nWhat happens if you extract the column with study semesters as a vector and transform it into a double?\nWhat is the average study semester of those students being in their 8th or earlier semester?\nHow many students are in their 9th or higher study semester?\nWhat does typeof(euf_semesters) return and why?"
  },
  {
    "objectID": "content/material/session05-solutions.html",
    "href": "content/material/session05-solutions.html",
    "title": "Exercises for Recap Session 1",
    "section": "",
    "text": "Exercise 1: Basic object types I\n\nCreate a vector containing the numbers 2, 5, 2.4 and 11.\n\n\nex1_vec &lt;- c(2, 5, 2.4, 11)\n\n\nReplace the second element with 5.9.\n\n\nex1_vec[2] &lt;- 5.9\nex1_vec\n\n[1]  2.0  5.9  2.4 11.0\n\n\n\nAdd the elements 3 and 1 to the beginning, and the elements \"8.0\" and \"9.2\" to the end of the vector.\n\n\nva_1 &lt;- c(3, 1)\nva_2 &lt;- c(\"8.0\", \"9.2\")\nex1_vec_extended &lt;- c(va_1, ex1_vec, va_2)\nex1_vec_extended\n\n[1] \"3\"   \"1\"   \"2\"   \"5.9\" \"2.4\" \"11\"  \"8.0\" \"9.2\"\n\n\n\nCreate a vector with the numbers from -8 to 9 (step size: 0.5)\n\n\nex1_vec_4 &lt;- seq(-8, 9, by = 0.5)\nex1_vec_4\n\n [1] -8.0 -7.5 -7.0 -6.5 -6.0 -5.5 -5.0 -4.5 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0\n[16] -0.5  0.0  0.5  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5\n[31]  7.0  7.5  8.0  8.5  9.0\n\n\n\nCompute the square root of each element of the first vector using vectorisation.\n\n\nsqrt(ex1_vec_4)\n\nWarning in sqrt(ex1_vec_4): NaNs produced\n\n\n [1]       NaN       NaN       NaN       NaN       NaN       NaN       NaN\n [8]       NaN       NaN       NaN       NaN       NaN       NaN       NaN\n[15]       NaN       NaN 0.0000000 0.7071068 1.0000000 1.2247449 1.4142136\n[22] 1.5811388 1.7320508 1.8708287 2.0000000 2.1213203 2.2360680 2.3452079\n[29] 2.4494897 2.5495098 2.6457513 2.7386128 2.8284271 2.9154759 3.0000000\n\n\n\nCreate a character vector containing then strings \"Number_1\" to \"Number_5\". Use suitable helper functions to create this vector quickly.\n\n\nex1_char_vec &lt;- paste0(\"Number_\", seq(1, 5))\nex1_char_vec\n\n[1] \"Number_1\" \"Number_2\" \"Number_3\" \"Number_4\" \"Number_5\"\n\n\n\n\nExercise 2: Basic object types II\nConsider the following vector:\n\nex_2_vec &lt;- c(1.9, \"2\", FALSE)\n\n\nWhat is the type of this vector? Why?\n\n\ntypeof(ex_2_vec)\n\n[1] \"character\"\n\n\nAtomic vectors only contain objects of the same type, and there is a hierarchy. Elements that themselves are of a type lower in the hierarchy are coerced to the same type as the object highest in the hierarchy. The hierarchy is as as follows:\n\ncharacter\ndouble\ninteger\nlogical\n\nTherefore, the type of ex_2_vec is character. The underlying reason is that you can, for instance, always transform a double value into a character but not vice versa.\n\nWhat happens if you coerce this vector into type integer? Why?\n\n\nas.integer(ex_2_vec)\n\nWarning: NAs introduced by coercion\n\n\n[1]  1  2 NA\n\n\nBecause integer is lower in the hierarchy than character, the transformation is not straightforward. By coincidence, the first two elements can actually be coerced into integers (albeit maybe not with the expected result), but there is no way you can transform the logical value FALSE into an integer, which is why a missing value is produced.\n\nWhat does sum(is.na(x)) tell you about a vector x? What is happening here?\n\n\nx &lt;- c(1,2,3,NA,NA,8)\n\nFirst, is.na(x) creates a vector with logical values indicating whether a value of the original vector is missing (i.e. NA):\n\nis.na(x)\n\n[1] FALSE FALSE FALSE  TRUE  TRUE FALSE\n\n\nThen, sum() computes the sum over this vecor of boolean values:\n\nsum(is.na(x))\n\n[1] 2\n\n\nHere, TRUE counts as one and FALSE as zero, so sum() gives the number of cases in which is.na(x) has evaluated to TRUE:\n\nIs it a good idea to use as.integer() on double characters to round them to the next integer? Why (not)? What other ways are there to do the rounding?\n\nNo, because as.integer() is not acutally rounding numbers (as, for example, as.integer(2.1) would make you think), but only removing the decimal part of the number:\n\nas.integer(2.9) # you might expect 2...\n\n[1] 2\n\n\nBetter use round():\n\nround(2.9)\n\n[1] 3\n\n\n\n\nExercise 3: Define a function\nCreate functions that take a vector as input and returns:\n\nThe last value.\n\n\nget_last_val &lt;- function(x){\n  last_val &lt;- x[length(x)]\n  return(last_val)\n}\n\n\nEvery element except the last value and any missing values.\n\n\nget_beginning &lt;- function(x){\n  beginning &lt;- x[-length(x)] # Removes last value\n  na_positions &lt;- which(is.na(beginning)) # Get positions of NA values\n  beginning_nonas &lt;- beginning[-na_positions] # Removes these values\n  return(beginning_nonas)\n} \n\n\nOnly even numbers.\n\n\nHint: Use the operation x %% y to get the remainder from diving x by y, the so called ‘modulo y’. For even numbers, the modulo 2 is zero.\n\n\nget_even &lt;- function(x){\n  modulo_2s &lt;- x%%2 # Module 2 is zero for even numbers only\n  even_nbs &lt;- x[modulo_2s==0] # Keep only those for which modulo 2 is zero\n  na_positions &lt;- which(is.na(even_nbs)) # Get positions of NA values\n  even_nbs_nonas &lt;- even_nbs[-na_positions] # Removes these values\n  return(even_nbs_nonas)\n}\n\nApply your function to the following example vector:\n\nex_3_vec &lt;- c(1, -8, 99, 3, NA, 4, -0.5, 50)\n\n\nget_last_val(ex_3_vec)\n\n[1] 50\n\nget_beginning(ex_3_vec)\n\n[1]  1.0 -8.0 99.0  3.0  4.0 -0.5\n\nget_even(ex_3_vec)\n\n[1] -8  4 50\n\n\n\n\nExercise 4: Lists\n\nCreate a list that contains three elements called 'a', 'b' and 'c'. The first element should correspond to a double vector with the elements 1.5, -2.9 and 99. The second element should correspond to a character vector with the elments 'Hello', '3', and 'EUF'. The third element should contain three times the entry FALSE.\n\n\nex_4_list &lt;- list(\n  'a' = c(1.5, -2.9, 99),\n  'b' = c('Hello', \"'3'\", 'EUF'),\n  'c' = rep(FALSE, 3)\n)\n\n\nTransform this list into a data.frame and a tibble. Then apply str() to get information about the respective structure. How do the results differ?\n\n\nex_4_df &lt;- as.data.frame(ex_4_list)\nex_4_tb &lt;- tibble::as_tibble(ex_4_list)\nstr(ex_4_list)\n\nList of 3\n $ a: num [1:3] 1.5 -2.9 99\n $ b: chr [1:3] \"Hello\" \"'3'\" \"EUF\"\n $ c: logi [1:3] FALSE FALSE FALSE\n\nstr(ex_4_df)\n\n'data.frame':   3 obs. of  3 variables:\n $ a: num  1.5 -2.9 99\n $ b: chr  \"Hello\" \"'3'\" \"EUF\"\n $ c: logi  FALSE FALSE FALSE\n\nstr(ex_4_tb)\n\ntibble [3 × 3] (S3: tbl_df/tbl/data.frame)\n $ a: num [1:3] 1.5 -2.9 99\n $ b: chr [1:3] \"Hello\" \"'3'\" \"EUF\"\n $ c: logi [1:3] FALSE FALSE FALSE\n\n\nstr() only differs with regard to the first line describing the type.\n\n\nExercise 5: Data frames and the study semester distribution at EUF\nThe package DataScienceExercises contains a data set called EUFstudentsemesters, which contains information about the distribution of study semesters of enrolled students at the EUF in 2021. You can shortcut the data set as follows:\n\neuf_semesters &lt;- DataScienceExercises::EUFstudentsemesters\n\n\nWhat happens if you extract the column with study semesters as a vector and transform it into a double?\n\n\nunique(euf_semesters[[\"Semester\"]])\n\n[1] \"6\"           \"4\"           \"2\"           \"8\"           \"9 or higher\"\n[6] \"7\"           \"5\"           \"3\"           \"1\"          \n\nsemesters &lt;- as.double(euf_semesters[[\"Semester\"]])\n\nWarning: NAs introduced by coercion\n\nunique(semesters)\n\n[1]  6  4  2  8 NA  7  5  3  1\n\n\nWe see that the previous entry \"9 or higher\" has been transformed into NA.\n\nWhat is the average study semester of those students being in their 8th or earlier semester?\n\n\nmean(semesters, na.rm = TRUE)\n\n[1] 4.177026\n\n\n\nHow many students are in their 9th or higher study semester?\n\n\nsum(euf_semesters$Semester==\"9 or higher\")\n\n[1] 469\n\n\n\nWhat does typeof(euf_semesters) return and why?\n\n\ntypeof(euf_semesters)\n\n[1] \"list\"\n\n\nIt returns list, because while euf_semesters is a tibble, typeof() always gives the underlying basic object type. For tibbles, this is list."
  },
  {
    "objectID": "content/material/session06.html",
    "href": "content/material/session06.html",
    "title": "🗓️ Session 6: Visualization",
    "section": "",
    "text": "One area for which R is particulary well-known for is the area of visualization. This is particularly because of the package ggplot2. This session introduces ggplot2 and the general approach to generate visualization in R. The good thing is that if you follow the approach described here, you can basically create every visualization type you can think of.",
    "crumbs": [
      "🗓️ Material",
      "Session 06"
    ]
  },
  {
    "objectID": "content/material/session06.html#lecture-slides",
    "href": "content/material/session06.html#lecture-slides",
    "title": "🗓️ Session 6: Visualization",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to download the slides.\n\n  \n\n\n\n\n\n\n\nThe R script of this session",
    "crumbs": [
      "🗓️ Material",
      "Session 06"
    ]
  },
  {
    "objectID": "content/material/session06.html#lecture-videos",
    "href": "content/material/session06.html#lecture-videos",
    "title": "🗓️ Session 6: Visualization",
    "section": "🎥 Lecture videos",
    "text": "🎥 Lecture videos\nCurrently, no videos are available for this session. Check out the tutorial instead.",
    "crumbs": [
      "🗓️ Material",
      "Session 06"
    ]
  },
  {
    "objectID": "content/material/session06.html#mandatory-reading",
    "href": "content/material/session06.html#mandatory-reading",
    "title": "🗓️ Session 6: Visualization",
    "section": "📚 Mandatory Reading",
    "text": "📚 Mandatory Reading\n\nThe tutorial visualization",
    "crumbs": [
      "🗓️ Material",
      "Session 06"
    ]
  },
  {
    "objectID": "content/material/session06.html#further-reading",
    "href": "content/material/session06.html#further-reading",
    "title": "🗓️ Session 6: Visualization",
    "section": "Further Reading",
    "text": "Further Reading\n\nWickham (2010), who introduces the theory underlying ggplot2\nggplot2 cheat sheet\nBrowse the website from Data to Viz and try to re-create some of the figures yourself",
    "crumbs": [
      "🗓️ Material",
      "Session 06"
    ]
  },
  {
    "objectID": "content/material/session06.html#coursework",
    "href": "content/material/session06.html#coursework",
    "title": "🗓️ Session 6: Visualization",
    "section": "✍️ Coursework",
    "text": "✍️ Coursework\n\nDo the exercises Visualization1 from the DataScienceExercises package\n\n\n\n\n\n\n\nQuick code for starting the exercises\n\n\n\n\n\n\nlearnr::run_tutorial(\n  name = \"Visualization1\", \n  package = \"DataScienceExercises\", \n  shiny_args=list(\"launch.browser\"=TRUE))",
    "crumbs": [
      "🗓️ Material",
      "Session 06"
    ]
  },
  {
    "objectID": "content/material/session08-Exercise.html",
    "href": "content/material/session08-Exercise.html",
    "title": "Identify Patterns in E-Commerce Data",
    "section": "",
    "text": "This dataset contains information about 200 customers of an e-commerce platform. It includes demographic information, spending habits, and behavioral metrics that the marketing team wants to analyze to develop more targeted campaigns.\n\n\n\ncustomer_sample.csv\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nType\n\n\n\n\ncustomer_id\nUnique identifier for each customer\nInteger\n\n\nage\nCustomer’s age in years\nInteger\n\n\nspending\nMonthly spending on the platform (in dollars)\nNumeric\n\n\nvisit_frequency\nAverage number of website visits per month\nNumeric\n\n\ncustomer_type\nCustomer classification based on purchasing history\nCategorical\n\n\npreferred_category\nPrimary product category the customer purchases\nCategorical\n\n\n\n\n\ncustomer_type:\n\nNew: Recently joined customers (first purchases within last 3 months)\nOccasional: Make purchases sporadically (no consistent pattern)\nRegular: Consistent purchases but not at high frequency\nLoyal: Frequent, consistent purchasers over extended period\n\npreferred_category:\n\nElectronics: Technology products, gadgets, computers, etc.\nClothing: Apparel items, accessories, shoes\nHome: Furniture, decor, kitchen items, garden supplies\nBeauty: Cosmetics, personal care, grooming products\nFood: Groceries, specialty foods, meal kits\n\n\n\n\n\nYour marketing team needs insights to develop targeted campaigns that will increase customer engagement and spending. The team believes there may be hidden patterns in how different customer segments interact with their platform.\nYour task: Create a visualization that reveals an interesting pattern in this data that could help inform marketing strategy.\n\nExample solution"
  },
  {
    "objectID": "content/material/session08-Exercise.html#overview",
    "href": "content/material/session08-Exercise.html#overview",
    "title": "Identify Patterns in E-Commerce Data",
    "section": "",
    "text": "This dataset contains information about 200 customers of an e-commerce platform. It includes demographic information, spending habits, and behavioral metrics that the marketing team wants to analyze to develop more targeted campaigns."
  },
  {
    "objectID": "content/material/session08-Exercise.html#file",
    "href": "content/material/session08-Exercise.html#file",
    "title": "Identify Patterns in E-Commerce Data",
    "section": "",
    "text": "customer_sample.csv"
  },
  {
    "objectID": "content/material/session08-Exercise.html#variables",
    "href": "content/material/session08-Exercise.html#variables",
    "title": "Identify Patterns in E-Commerce Data",
    "section": "",
    "text": "Variable\nDescription\nType\n\n\n\n\ncustomer_id\nUnique identifier for each customer\nInteger\n\n\nage\nCustomer’s age in years\nInteger\n\n\nspending\nMonthly spending on the platform (in dollars)\nNumeric\n\n\nvisit_frequency\nAverage number of website visits per month\nNumeric\n\n\ncustomer_type\nCustomer classification based on purchasing history\nCategorical\n\n\npreferred_category\nPrimary product category the customer purchases\nCategorical\n\n\n\n\n\ncustomer_type:\n\nNew: Recently joined customers (first purchases within last 3 months)\nOccasional: Make purchases sporadically (no consistent pattern)\nRegular: Consistent purchases but not at high frequency\nLoyal: Frequent, consistent purchasers over extended period\n\npreferred_category:\n\nElectronics: Technology products, gadgets, computers, etc.\nClothing: Apparel items, accessories, shoes\nHome: Furniture, decor, kitchen items, garden supplies\nBeauty: Cosmetics, personal care, grooming products\nFood: Groceries, specialty foods, meal kits"
  },
  {
    "objectID": "content/material/session08-Exercise.html#your-task",
    "href": "content/material/session08-Exercise.html#your-task",
    "title": "Identify Patterns in E-Commerce Data",
    "section": "",
    "text": "Your marketing team needs insights to develop targeted campaigns that will increase customer engagement and spending. The team believes there may be hidden patterns in how different customer segments interact with their platform.\nYour task: Create a visualization that reveals an interesting pattern in this data that could help inform marketing strategy.\n\nExample solution"
  },
  {
    "objectID": "content/material/session08.html",
    "href": "content/material/session08.html",
    "title": "🗓️ Session 8: Using AI for coding",
    "section": "",
    "text": "In this lecture you learn the basics of using AI tools for coding tasks. While no AI tool will help you to write code in a language you do not understand, modern AI tools are well suited to assist you in writing code in a language you are familiar with. Here we discuss some of the most common use cases for AI when it comes to the development of R code. We focus on, first, getting a first sketch of an R script and, second, on using AI to help you find mistakes in your current code.\nYou also learn how important it is to provide the AI with context - something you can only do if you already know the basics of the programming language you and the AI are working with. Finally, we also touch upon some AI-like tools and advice on how to avoid typos in code.",
    "crumbs": [
      "🗓️ Material",
      "Session 08"
    ]
  },
  {
    "objectID": "content/material/session08.html#lecture-slides",
    "href": "content/material/session08.html#lecture-slides",
    "title": "🗓️ Session 8: Using AI for coding",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to download the slides.\n\n  \n\n\n\n\n\n\n\nPrompts and code examples from the lecture\n\n\n\n\n\n\n\n\n\n\nDetails for the exercise on e-commerce data (“generate R code”)\n\n\n\n\n\n\n\nBase code for exercises on debugging, documenting and optimizing R code",
    "crumbs": [
      "🗓️ Material",
      "Session 08"
    ]
  },
  {
    "objectID": "content/material/session08.html#lecture-videos",
    "href": "content/material/session08.html#lecture-videos",
    "title": "🗓️ Session 8: Using AI for coding",
    "section": "🎥 Lecture videos",
    "text": "🎥 Lecture videos\nSo far, there are no learning videos available for this lecture.",
    "crumbs": [
      "🗓️ Material",
      "Session 08"
    ]
  },
  {
    "objectID": "content/material/session08.html#mandatory-reading",
    "href": "content/material/session08.html#mandatory-reading",
    "title": "🗓️ Session 8: Using AI for coding",
    "section": "📚 Mandatory Reading",
    "text": "📚 Mandatory Reading\nNA",
    "crumbs": [
      "🗓️ Material",
      "Session 08"
    ]
  },
  {
    "objectID": "content/material/session08.html#further-reading",
    "href": "content/material/session08.html#further-reading",
    "title": "🗓️ Session 8: Using AI for coding",
    "section": "Further Reading",
    "text": "Further Reading\n\nHow to use Github Copilot in R Studio and a guideline on how to get free student accessthrough Github Education\nResources for Gemini Code Assist, which is free but cannot be integrated directly into R Studio\nMore information on Claude Code, which is most interesting for doing advanced programming work\nMore information on Windsurf, which has a good free tier but no plugin for R Studio",
    "crumbs": [
      "🗓️ Material",
      "Session 08"
    ]
  },
  {
    "objectID": "content/material/session08.html#coursework",
    "href": "content/material/session08.html#coursework",
    "title": "🗓️ Session 8: Using AI for coding",
    "section": "✍️ Coursework",
    "text": "✍️ Coursework\n\nDo the in-class exercises you did not complete during the session",
    "crumbs": [
      "🗓️ Material",
      "Session 08"
    ]
  },
  {
    "objectID": "content/material/session10.html",
    "href": "content/material/session10.html",
    "title": "🗓️ Session 10: Data preparation",
    "section": "",
    "text": "In this session you learn how to turn your raw data into a state such that you can work with it. Luckily, there is one particular form for our data that represents the common starting point for all further operations, such as visualization or modelling. This form is called tidy data. And the goal of this session is to equip you with the tools that you need to turn the often messy raw data into tidy data. These skills are important because they make you independent: you will be able to prepare any data you find or create yourself such that you can further process it, and you will not rely on others to provide you data in a particular form.",
    "crumbs": [
      "🗓️ Material",
      "Session 10"
    ]
  },
  {
    "objectID": "content/material/session10.html#lecture-slides",
    "href": "content/material/session10.html#lecture-slides",
    "title": "🗓️ Session 10: Data preparation",
    "section": "👨‍🏫 Lecture Slides",
    "text": "👨‍🏫 Lecture Slides\nEither click on the slide area below or click here to download the slides.\n\n  \n\n\n\n\n\n\n\nData and solutions to the intermediate exercises\n\n\n\n\n\n\nData used in the video\nData for exercises",
    "crumbs": [
      "🗓️ Material",
      "Session 10"
    ]
  },
  {
    "objectID": "content/material/session10.html#lecture-videos",
    "href": "content/material/session10.html#lecture-videos",
    "title": "🗓️ Session 10: Data preparation",
    "section": "🎥 Lecture videos",
    "text": "🎥 Lecture videos\nAll the videos are available via this playlist.",
    "crumbs": [
      "🗓️ Material",
      "Session 10"
    ]
  },
  {
    "objectID": "content/material/session10.html#mandatory-reading",
    "href": "content/material/session10.html#mandatory-reading",
    "title": "🗓️ Session 10: Data preparation",
    "section": "📚 Mandatory Reading",
    "text": "📚 Mandatory Reading\n\nThe tutorial Data preparation\nChapter 5 in Wickham et al. (2023).",
    "crumbs": [
      "🗓️ Material",
      "Session 10"
    ]
  },
  {
    "objectID": "content/material/session10.html#further-reading",
    "href": "content/material/session10.html#further-reading",
    "title": "🗓️ Session 10: Data preparation",
    "section": "Further Reading",
    "text": "Further Reading\n\nThe help page for the selection helpers, which facilitate the selection of particular columns.\nChapter 13 in Wickham et al. (2023).\nChapter 14 in Wickham et al. (2023).\nWickham (2014) on the concept of ‘tidy data’ (see the Github repo for reproduction of the paper)",
    "crumbs": [
      "🗓️ Material",
      "Session 10"
    ]
  },
  {
    "objectID": "content/material/session10.html#coursework",
    "href": "content/material/session10.html#coursework",
    "title": "🗓️ Session 10: Data preparation",
    "section": "✍️ Coursework",
    "text": "✍️ Coursework\n\nDo the exercises Wrangling1 from the DataScienceExercises package\n\n\n\n\n\n\n\nQuick code for starting the exercises\n\n\n\n\n\n\nlearnr::run_tutorial(\n  name = \"Wrangling1\", \n  package = \"DataScienceExercises\", \n  shiny_args=list(\"launch.browser\"=TRUE))\n\n\n\n\n\nDownload data about the CO2 emissions for some countries of your choice from the World Bank website for the years 2000 to 2020. Set up an R project, save the data, import it, and make a line graph.\n\nZIP file with a possible solution1\n\nIf you want more exercises on the challenge of making data longer/wider, you can do the exercises Wrangling2 from the DataScienceExercises package\n\n\n\n\n\n\n\nQuick code for starting the exercises\n\n\n\n\n\n\nlearnr::run_tutorial(\n  name = \"Wrangling2\", \n  package = \"DataScienceExercises\", \n  shiny_args=list(\"launch.browser\"=TRUE))",
    "crumbs": [
      "🗓️ Material",
      "Session 10"
    ]
  },
  {
    "objectID": "content/material/session10.html#footnotes",
    "href": "content/material/session10.html#footnotes",
    "title": "🗓️ Session 10: Data preparation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can ignore the make_co2_data.R for now and only look at make_co2_plot.R.↩︎",
    "crumbs": [
      "🗓️ Material",
      "Session 10"
    ]
  },
  {
    "objectID": "content/tutorials/first-steps/index.html",
    "href": "content/tutorials/first-steps/index.html",
    "title": "First steps in R",
    "section": "",
    "text": "In this post we will learn about the basic syntax of R. The syntax basically refers to the grammatical rules you must adhere to when communicating with your computer in the language R: if you do not follow the right syntax, i.e. you ‘speak’ grammatically incorrect, your computer will not understand you and communicate this to you by throwing up an error message.\nTo learn about these important basics, the post follows the following structure:"
  },
  {
    "objectID": "content/tutorials/first-steps/index.html#footnotes",
    "href": "content/tutorials/first-steps/index.html#footnotes",
    "title": "First steps in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you do not know what the console is, you should have a look at the lecture slides from the Material section again.↩︎\nYou may try this out by typing 1:100 into your console and see what happens: this returns a vector of length 100, which certainly will contain some line breaks.↩︎\nAgain, the use of scripts has been explained in the lecture, so have a look at the slides (or the R-Studio cheat sheet) in the Material section.↩︎\nIn theory we can use &lt;- also the other way around: 2 + 3 -&gt; intermediate_result. At first sight this is more intuitive and respects the sequence of events: first, the result of 2 + 3 gets created, i.e. a new object gets defined. Then, this object gets the name intermediate_result. However, the code that results from such practice is usually much more difficult to read, so it is common practice to use &lt;- rather than -&gt;.↩︎\nWickham and Bryan (2022) provide an excellent introduction to the development of R packages↩︎\nPackages not released on this platform can also be installed directly from the repository they were published, e.g. Github. To this end, the package remotes must be installed first, then you can use functions such as install_github(). A short manual is provided here.↩︎"
  },
  {
    "objectID": "content/tutorials/installation/index.html",
    "href": "content/tutorials/installation/index.html",
    "title": "Installation of the necessary software",
    "section": "",
    "text": "During this course we will use the following software and services:\n\nR\nR-Studio\nGit\nGithub\nNetlify\n\nYou will need to install the software and register for these services on your own. While I offer an optional session for joint troubleshooting, it is absolutely necessary that you do your best to install the software on your own before that date. To this end, this document is meant to provide you with all the information needed. If you have questions, please use the Moodle forum. It is very unlikely that you are the only person having a particular problem. Maybe others can already help you out, and if not, all should benefit from the solution we find for your problem together.\nPlease also note that there is a separate tutorial on how to install the packages (a.k.a. R extensions) that we are going to use over the semester. Its best to continue with this tutorial on installing the required R packages directly after you have complete this one."
  },
  {
    "objectID": "content/tutorials/installation/index.html#install-r",
    "href": "content/tutorials/installation/index.html#install-r",
    "title": "Installation of the necessary software",
    "section": "Install R",
    "text": "Install R\nThe installation of R is very similar across operating systems (OS). The easiest way is to visit the R Homepage and to download the most recent version for your OS. In case you are using Mac OS and want to use Homebrew, its best to use this formula.\nImportant for Mac user: There are different versions of R for Intel chips, and Apple chips (M1, M2, etc.). It is very important that you install the correct version. If you are not sure whether your Mac contains a chip from Intel or Apple, click on the Apple symbol in the upper left of the screen, then click on About this Mac and you can see which processor your Mac is using in the new window. If you have an Apple chip, always install R for the so called arm64 architecture. Intel chip users must use the x86_64 architecture instead.\n\nOnly Windows: Install RTools\nIf you are using Windows, it is necessary to install RTools, which is required if you want to use packages written by others that are not officially released. To do so, simply visit the following website, download the installer, and install the software:\nWhen asked during the installation process, do not select the box for Add rtools to system PATH, but do select the box for Save version information to registry.\n\n\nOnly Mac: Command Line Developer Tools\nThe Command Line Developer Tools could be thought of as the Mac pendant to RTools. These allow you to build R packages from source (meaning, basically, you can use packages that are in early stages of distribution, or packages that are not released on the official R servers).\nThe easiest way to install them is to open the App Terminal, and then to type\nxcode-select --install\nand press Enter. Then a pop up window will open and allow you to install the software."
  },
  {
    "objectID": "content/tutorials/installation/index.html#update-r",
    "href": "content/tutorials/installation/index.html#update-r",
    "title": "Installation of the necessary software",
    "section": "Update R",
    "text": "Update R\nIn case R is already installed on your computer you should make sure that your version is more of less up to date. For our seminar you should use at least R version R 4.4.2. The version you are currently using is shown as soon as you start R.\nPlease note: if you installed R anew in the previous step, you do not need to update it. The information on updating R is mainly relevant for people who have installed R already some time ago.\n\nMacOS users\nFor MacOS users, the easiest route to update R is to just re-install the most current version from the R Homepage. Keep in mind that in this case you might need to re-install all previously installed packages. If you have a lot of packages installed that you want to keep, the following steps facilitates the re-installation process. First, save a list with all the packages you installed yourself. To this end type the following into the R console:1\npackage_overview &lt;- installed.packages()\npackage_names &lt;- as.vector(\n  package_overview[is.na(package_overview[,\"Priority\"]), 1])\nsave(package_names, file=\"r_packages.rda\")\nAfter re-installing R, you then need to load the file you previously saved and identify the missing packages. You can use the following code to do so if you are in the working directory in which you saved the file \"r_packages.rda\":\nload(\"r_packages.rda\")\npackages_new &lt;- installed.packages()\npackages_new_ &lt;- as.vector(packages_new[is.na(packages_new[,\"Priority\"]), 1])\nmissing_packages &lt;- setdiff(package_names, packages_new_)\ninstall.packages(missing_packages)\nupdate.packages()\n\n\nWindows users\nWindows users have a slightly more convenient route available to them: the installr package. It does not require you to re-install your packages. Just type the following code into your R console:2\ninstall.packages(\"installr\")\nlibrary(installr)\nupdateR(TRUE)\nFor more information see the package website.\n\n\nLinux users\nLinux users simply install R via their package manager. A quick search on Google should provide you with the information that are relevant for your particular Linux distribution. Updating is usually straightforward as well: just run the respective command from your package manager."
  },
  {
    "objectID": "content/tutorials/installation/index.html#r-studio",
    "href": "content/tutorials/installation/index.html#r-studio",
    "title": "Installation of the necessary software",
    "section": "R-Studio",
    "text": "R-Studio\nInstalling R-Studio is easy. The only thing you should keep in mind that you should install R first, and R-Studio second. So, after installing R got to the R-Studio download page and download the RStudio Desktop version for your OS according to the installation instructions provided.\nIf you are on Mac and you are using Homebrew you may use this formula.\nIf you want to update R-Studio, you just install it again. Please note that the minimal version for this seminar should be RStudio 2023.12.1+402, which is from late January 2024. You can check your version by clicking on RStudio in the upper left part of your screen when R-Studio is open. Then click on About RStudio."
  },
  {
    "objectID": "content/tutorials/installation/index.html#git",
    "href": "content/tutorials/installation/index.html#git",
    "title": "Installation of the necessary software",
    "section": "Git",
    "text": "Git\nInstalling Git is straightforward, but the right approach depends on your OS.\n\nMacOS\nOn MacOS you should install Git as part of the Command Line Developer Tools, which themselves are part of XCode (see above). Its easiest to run the following command from your Terminal:3\ngit --version\nIf you get an output such as git version 2.34.1 you already installed you need. If not, you will be asked to install the respective software packages (see above).\n\n\nWindows\nOn Windows you download Git for Windows from the official Webpage, which also provides you with all the relevant instructions.\n\n\nLinux\nOn Linux use you package manager. In most cases the name of the relevant package is git-all, so on Ubuntu, for instance, you would install Git via sudo apt install git-all."
  },
  {
    "objectID": "content/tutorials/installation/index.html#install-quarto",
    "href": "content/tutorials/installation/index.html#install-quarto",
    "title": "Installation of the necessary software",
    "section": "Install Quarto",
    "text": "Install Quarto\nQuarto allows you to write text and R code within one document. This is very useful in many instances, and allows you to create a wide variety of nicely looking and practically appealing outputs, including apps, websites, statistical reports, and much more. To install Quarto just follow the instructions from this webpage."
  },
  {
    "objectID": "content/tutorials/installation/index.html#github",
    "href": "content/tutorials/installation/index.html#github",
    "title": "Installation of the necessary software",
    "section": "Github",
    "text": "Github\nThis is easy. Just visit https://github.com/ and sign up using your email account."
  },
  {
    "objectID": "content/tutorials/installation/index.html#netlify",
    "href": "content/tutorials/installation/index.html#netlify",
    "title": "Installation of the necessary software",
    "section": "Netlify",
    "text": "Netlify\nThis is easy as well. Visit https://www.netlify.com/ and click on Sign up in the upper right of the webpage. You can now either create a Netlify account by clicking on Email and register a new email address, or you can link Netlify to one of the other accounts you might already have. I personally, for instance, linked Netlify to my Github account."
  },
  {
    "objectID": "content/tutorials/installation/index.html#footnotes",
    "href": "content/tutorials/installation/index.html#footnotes",
    "title": "Installation of the necessary software",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you do not yet know what the R console is don’t worry. You will learn this during the course. But for now it would then be better to update R by just re-installing it.↩︎\nIf you do not yet know what the R console is don’t worry. You will learn this during the course. But for now it would then be better to update R by just re-installing it.↩︎\nBy this I mean that you first open the app Terminal and then enter the commend into the window that has opened, and then press Enter.↩︎"
  },
  {
    "objectID": "content/tutorials/obj-types-ii-vectors/index.html",
    "href": "content/tutorials/obj-types-ii-vectors/index.html",
    "title": "Fundamental object types in R II: Vectors",
    "section": "",
    "text": "We already learned that everything in R that exists is an object. You most likely noted that there are different types of objects: 2, for instance, was a number, but assign was a function.1 As you might have guessed, there are many more types of objects. To understand the fundamental object types in R is an essential prerequisite to master more complicated programming challenges than those we have encountered so far. Thus, this post is among those that will introduce you to the most important object types that you will encounter in R.\nThese data types are summarized in the following figure:\n\n\n\n\n\n\n\n\n\nThis post will be about the most common types of vectors. See the previous post for a treatment of functions, and the upcoming one for more advanced types of vectors, such as factor, matrix, and data.frame."
  },
  {
    "objectID": "content/tutorials/obj-types-ii-vectors/index.html#atomic-vectors",
    "href": "content/tutorials/obj-types-ii-vectors/index.html#atomic-vectors",
    "title": "Fundamental object types in R II: Vectors",
    "section": "Atomic vectors",
    "text": "Atomic vectors\nThis makes it easy to classify atomic vectors in more detail: we usually say that the type of atomic vector is the type of the object it encompasses. Four major types of atomic vectors in this sense exist:\n\nlogical (logical values): there are only two relevant logical values: TRUE und FALSE3\ninteger (whole numbers): this type should be self-explanatory. Less intuitive is the rule that in order to define an integer in R you need to type the number followed by the letter L such that R interprets the number as an integer.4 Examples are 1L, -400L or 10L.\n\ndouble (decimal numbers): these should be self-explanatory as well. Examples are 1.5, 0.0, or -500.32.\nWhole and decimal numbers are often summarized in the category numeric. However, the use of numeric is almost always confusing, and many functions show counter-intuitive behavior when this category is used. I recommend you to never use it.\ncharacter (words): these can contain all kinds of tokens and are characterized by the fact that they always start and end with \" (or '). Examples would be \"Hello\", \"500\" or \"1_2_Three\".\n\nAs indicated above, an atomic vector only comprises elements of the same type. In this context, we should mention, however, the at first sight ‘strange’ data type NA, which denotes a missing value:5 whenever an element of a vector is missing, e.g. when the vector is used to store observations of subjects that have participated in an experiment, and for some subjects the observation is missing, we will use NA.6\n\nTesting and coercing types\nIn the following we will study the different types of atomic vectors and their typical behavior in more detail. But before doing so we should introduce the function typeof(): it helps us to identify the type of an object in the first place. To see how, lets call the function with the object (or the name of the object) we are interested about:\n\ntypeof(2L)\n\n[1] \"integer\"\n\n\n\nx &lt;- 22.0\ntypeof(x)\n\n[1] \"double\"\n\n\nThere is also a family of functions that allows us to test whether an object is actual of a certain type or not. The general syntax here is is.*(). For instance:\n\nx &lt;- 1.0\nis.integer(x)\n\n[1] FALSE\n\n\n\nis.double(x)\n\n[1] TRUE\n\n\nThis function always returns an object of type logical:\n\ny &lt;- is.double(x)\ntypeof(y)\n\n[1] \"logical\"\n\n\nWe can also try to transform objects from one type into another. We call this process ‘coercion’ an the general syntax is as.*()*. For instance:\n\nx &lt;- \"2\"\nprint(\n  typeof(x)\n)\n\n[1] \"character\"\n\nx &lt;- as.double(x)\nprint(\n  typeof(x)\n)\n\n[1] \"double\"\n\n\nSuch a transformation is, however, not always possible:\n\nas.double(\"Hello\")\n\nWarning: NAs introduced by coercion\n\n\n[1] NA\n\n\nSince R does not know how to turn the word ‘Hello’ into a decimal number, it transforms it into a ‘missing value’ - NA.\nFor the basic types discussed above there is a logical hierarchy of feasible transformations: logical → integer → double → character, meaning that you can always transform a decimal number into a word, but not vice versa.\n\nTransgression: Why change the types of objects anyway? Data types are extremely important for a programming language because otherwise it would remain unclear how mathematical operations could be applied to different objects such as numbers or words. You will transform objects yourself especially when you want to use a certain operation that is only defined for a certain type of object, and the object you are dealing with has been stored as a different type. This can happen, for example, when you read in data or translate words into numerical values yourself. If unexpected errors occur in your code with cryptic error messages, it is always a good idea to check the types of the objects used and transform them if necessary.\n\n\nx &lt;- 2\ny &lt;- as.character(x)\nprint(y)\n\n[1] \"2\"\n\nz &lt;- as.double(y) # This works\nprint(z)\n\n[1] 2\n\nk &lt;- as.double(\"Hallo\") # This does not work\n\nWarning: NAs introduced by coercion\n\nprint(k)\n\n[1] NA\n\n\nWhen transforming logical values, TRUE counts as 1 and FALSE as 0, a fact that will come in handy later on:\n\nx &lt;- TRUE\nas.integer(x)\n\n[1] 1\n\n\n\ny &lt;- FALSE\nas.integer(y)\n\n[1] 0\n\n\nSince it is not always clear when R issues a warning for transformations that are incompatible with the hierarchy just introduced and when it does not, you should always be cautious!\nMoreover, transformations might change the properties of the transformed objects implicitly in unexpected ways. For instance, a transformation from a decimal number to a whole number can lead to unexpected rounding behavior:\n\nx &lt;- 1.99\nas.integer(x)\n\n[1] 1\n\n\nAnother example is the following:\n\nz &lt;- as.logical(99)\nprint(z)\n\n[1] TRUE\n\n\nSuch implicit changes of the object properties do not necessary come with a warning message, so one should always be careful when transforming objects!\nIn many cases, functions do the necessary transformations of their arguments automatically. In most cases this is very practical:\n\nx &lt;- 1L # Integer\ny &lt;- 2.0 # Double\nz &lt;- x + y\ntypeof(z)\n\n[1] \"double\"\n\n\nBut it can be dangerous in some cases as well.\nWhen adding up logical values they are transformed to numbers:\n\nx &lt;- TRUE\ny &lt;- FALSE\nz &lt;- x + y # TRUE counts as 1, FALSE as 0\nprint(z) \n\n[1] 1\n\n\nThis is useful if you want to know, for instance, how many elements of a vector meet a certain logical criterion:\n\nx &lt;- c(1,2,3,4,5)\nsum(x &gt; 3)\n\n[1] 2\n\n\nIn all these cases it is very important to stay informed about the types of objects you are dealing with. To help you out, the following table contains an overview over the most important transformation and test functions:\n\n\n\nType\nTest\nTransformation\n\n\n\n\nlogical\nis.logical\nas.logical\n\n\ndouble\nis.double\nas.double\n\n\ninteger\nis.integer\nas.integer\n\n\ncharacter\nis.character\nas.character\n\n\nfunction\nis.function\nas.function\n\n\nNA\nis.na\nNA\n\n\nNULL\nis.null\nas.null\n\n\n\nA final remark on scalars: with scalar we usually refer to ‘single numbers’, such as 2. There is no such concept in R: 2 is a vector with one element (or: of length 1). Thus, we do not distinguish the type of a vector with or more than one elements.\nNote: As you might have guessed already, we use the function c() to create longer vectors:\n\nx &lt;- c(1, 2, 3)\nx\n\n[1] 1 2 3\n\n\nWe can also use this function to concatenate vectors:\n\nx &lt;- 1:3 # Shortcut for: x &lt;- c(1, 2, 3)\ny &lt;- 4:6\nz &lt;- c(x, y)\nz\n\n[1] 1 2 3 4 5 6\n\n\nSince atomic vectors can only contain objects of the same type, one might expect the following code, which tries to concatenate objects of different types, to produce an error:\n\nx &lt;- c(1, \"Hallo\")\n\nBut this is not what happens! R transforms the objects according to the hierarchy discussed above:\nlogical → integer → double → character. Due to the absence of errors or warning messages, such operations are a regular source for mistakes.\nNote: The length of a vector corresponds to its numbers of elements. We can ‘measure’ its length using the function length():\n\nx =  c(1, 2, 3)\nlen_x &lt;- length(x)\nlen_x\n\n[1] 3\n\n\n\nTransgression: How large can an integer become? In R, objects of type integer are stored as 32-bit files. This means that for each single integer, 32 bits of storage are available on your computer. This implies that really large numbers cannot be stored as integers, simply because the 32 bits are not sufficient:\n\n\nx &lt;- 2147483647L\ntypeof(x)\n\n[1] \"integer\"\n\n\n\ny &lt;- 2147483648L\ntypeof(y)\n\n[1] \"double\"\n\n\n\nAs you can see, the largest number that we can store as 32-bit integer is 2147483647. Larger numbers must be stored as double. The drawback of saving numbers in this type is, however, the risk of a loss of precision. If you want to avoid this you could try to save an integer as a 64 bit integer. This possibility has been added to R later to save large numbers as integers (something that happens faster than you think). To do so we must use the package7 bit64:\n\n\nz &lt;- bit64::as.integer64(2147483648)\nbit64::is.integer64(z)\n\n[1] TRUE\n\n\n\nBecause this is a data type that has been added to R later, several functions do not work with 64-bit integers if the package bit64 is not installed. Moreover, several standard functions return very irritating outputs:\n\n\ntypeof(z)\n\n[1] \"double\"\n\n\n\nFor this reason, and because bit64 is not part of the standard installation of R, you should avoid storing large numbers as integer64 whenever possible. Very large numbers should be stored as double or, when precision is a serious issue, you should scale them down and then save them as integer.\n\n\n\nLogical operations\nThe logical values TRUE and FALSE are often the result of logical operations, such as ‘Is 2 larger than 1?’. Such logical operations occur very frequently and its a good idea to familiarize yourself with the logical operators. You can find an overview in the following table:\n\n\n\nOperator\nFunction in R\nExample\n\n\n\n\nlarger\n&gt;\n2&gt;1\n\n\nsmaller\n&lt;\n2&lt;4\n\n\nequal\n==\n4==3\n\n\nlarger or equal\n&gt;=\n8&gt;=8\n\n\nsmaller or equal\n&lt;=\n5&lt;=9\n\n\nnot equal\n!=\n4!=5\n\n\nand\n&\nx&lt;90 & x&gt;55\n\n\nor\n|\nx&lt;90 | x&gt;55\n\n\neither or\nxor()\nxor(2&lt;1, 2&gt;1)\n\n\nnot\n!\n!(x==2)\n\n\nis true\nisTRUE()\nisTRUE(1&gt;2)\n\n\n\nThe result of such logical operations is always a logical value:\n\nx &lt;- 4\ny &lt;- x == 8\ntypeof(y)\n\n[1] \"logical\"\n\n\nYou may also test longer vectors:\n\nx &lt;- 1:3\nx&lt;2\n\n[1]  TRUE FALSE FALSE\n\n\nTests can also be chained:\n\nx &lt;- 1L\nx&gt;2 | x&lt;2 & (is.double(x) & x!=0)\n\n[1] FALSE\n\n\nSince many mathematical operations interpret TRUE as 1, it is easy to check how often a certain condition is met:\n\nx &lt;- 1:50 \nsmaller_20 &lt;- x&lt;20 \nprint(\n  sum(smaller_20) # How many elements are smaller then 20?\n  )\n\n[1] 19\n\nprint(\n  sum(smaller_20/length(x)) # Whats the share of these elements?\n)\n\n[1] 0.38\n\n\n\n\nVectorization\nThe chained operation we just saw is an example for vectorizing an operation. This means that the same operation is applied to many elements, all of which are concatenated as a vector. For instance, if you want to compute the square root of the numbers 5, 6 and 7 you could do:\n\nsqrt(5)\n\n[1] 2.236068\n\nsqrt(6)\n\n[1] 2.44949\n\nsqrt(7)\n\n[1] 2.645751\n\n\nOr you vectorize the operation:\n\nsqrt(c(5,6,7))\n\n[1] 2.236068 2.449490 2.645751\n\n\nVectorizing operations is very useful since it speeds up the computations considerably. Vectorized operations are far more efficient and faster than applying the operation to each element of the vector separately. Thus, whenever you need to apply a certain operation more than once you should always think about using vectorization.8\n\n\nMore on words\nWords are distinguished by the fact that their beginning and their end gets indicated by the symbol ' or \":\n\nx &lt;- \"Hello\"\ntypeof(x)\n\n[1] \"character\"\n\n\n\ny &lt;- 'Bye!'\ntypeof(y)\n\n[1] \"character\"\n\n\nJust as other kinds of atomic vectors, they can by concatenated using c():\n\nz &lt;- c(x, \"und\", y)\nz\n\n[1] \"Hello\" \"und\"   \"Bye!\" \n\n\nA useful function in this context is paste(), which transforms and combines elements of several vectors:\n\nx &lt;- 1:10\ny &lt;- paste(\"Try nb.\", x)\ny\n\n [1] \"Try nb. 1\"  \"Try nb. 2\"  \"Try nb. 3\"  \"Try nb. 4\"  \"Try nb. 5\" \n [6] \"Try nb. 6\"  \"Try nb. 7\"  \"Try nb. 8\"  \"Try nb. 9\"  \"Try nb. 10\"\n\n\nThe function paste() also accepts an optional argument sep, which allows us to specify a token that should be placed between the elements to be combined (the default is sep=\" \"):\n\nday_nr &lt;- 1:10\nx_axis &lt;- paste(\"Day\", day_nr, sep = \": \")\nx_axis\n\n [1] \"Day: 1\"  \"Day: 2\"  \"Day: 3\"  \"Day: 4\"  \"Day: 5\"  \"Day: 6\"  \"Day: 7\" \n [8] \"Day: 8\"  \"Day: 9\"  \"Day: 10\"\n\n\n\n*Note**: Here we have an example of what is called ‘recycling’. since the vector c(\"Day\") was shorter than the vector day_nr, c(\"Day\") is simply copied so that the operation with paste() makes sense. Recycling is useful, but sometimes it can be harmful, namely when you think that you are using two vectors of the same length, but this is actually not the case. In such a case recycling leads to the fact that no error message is printed and the fact that the two vectors are not of the same length remains unnoticed An example of this is the following code, in which the intention is clearly to connect all weekdays to numbers and one weekday was simply forgotten:\n\n\ndays &lt;- paste(\"Tag \", 1:7, \":\", sep=\"\")\nday_names &lt;- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\")\npaste(days, day_names)\n\n[1] \"Tag 1: Monday\"    \"Tag 2: Tuesday\"   \"Tag 3: Wednesday\" \"Tag 4: Thursday\" \n[5] \"Tag 5: Friday\"    \"Tag 6: Saturday\"  \"Tag 7: Monday\"   \n\n\n\n\nMissing values and NULL\nAs indicated above, missing values are encoded as NA. This is particularly useful in statistical contexts, where are particular element of a vector cannot simply be removed if it is unavailable.\n\nExample: The vector x contains a logical value that indicates whether a person has correctly answered the question on a questionnaire. If the person did not answer the third question on the questionnaire, this should be indicated by NA. Simply omitting the value makes it impossible to determine afterwards which question the person did not answer.\n\nMost operations that get NA as an input will also give NA as an output, because it is unclear what the result of the operation would be for different values for for the missing value:\n\n5 + NA\n\n[1] NA\n\n\nThe only exception is an operation that yields a certain value completely independent from what you would substitute for NA:\n\nNA | TRUE # Always TRUE, no matter what you substitute for NA\n\n[1] TRUE\n\n\nTo test whether a vector x contains missing values you should always use the function is.na, never x==NA:\n\nx &lt;- c(NA, 5, NA, 10)\nprint(x == NA) # Unclear since not clear whether all NA must stand for the same value\n\n[1] NA NA NA NA\n\nprint(\n  is.na(x)\n)\n\n[1]  TRUE FALSE  TRUE FALSE\n\n\nWhenever an operation yields a value that cannot be defined, the result is not NA but NaN (not a number):\n\n0 / 0\n\n[1] NaN\n\n\nAnother special element is NULL. NULL is in fact a data type in itself (i.e. it is not a vector), but in practice its best thought of as a vector of length zero:\n\nx &lt;- NULL\nlength(x)\n\n[1] 0\n\n\nNULL is frequently used to indicate that something does not exist. An empty vector, for instance, is NULL:\n\nx &lt;- c()\nx\n\nNULL\n\nlength(x)\n\n[1] 0\n\n\nThis is different to a vector with one (or more) missing elements:\n\ny &lt;- NA\nlength(y)\n\n[1] 1\n\n\nWhen you define your own functions, you might use NULL as the default value for optional arguments. We will learn about such more advanced strategies later in this course. For now, its best to think of NULL as an vector of length zero.\n\n\nIndexing and replacement\nWe can extract single elements of a vector using squared brackets:\n\nx &lt;- c(2,4,6)\nx[1]\n\n[1] 2\n\n\nThis also allows us to modify specific elements:\n\nx &lt;- c(2,4,6)\nx[2] &lt;- 99\nx\n\n[1]  2 99  6\n\n\nBut we can also extract more than one element:\n\nx[1:2]\n\n[1]  2 99\n\n\nNegative indices eliminate the respective elements:\n\nx[-1]\n\n[1] 99  6\n\n\nTo get the last element of a vector you might combine this idea with the function length():\n\nx[length(x)]\n\n[1] 6\n\n\n\n\nUseful functions when working with atomic vectors\nHere we shall mention a few functions that are particularly useful in the context of atomic vectors,9 especially when it comes to producing such vectors or to perform arithmetic operations with them.\nCreating atomic vectors:\nA sequence of whole numbers is something that we use very frequently. To create such sequences, the shortcut : comes in handy:\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\ny &lt;- 10:1\ny\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nTo build more complex sequences we can use seq(), which in its simplest case is equivalent to ::\n\nx &lt;- seq(1, 10)\nprint(x)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nThe function seq(), however, allows for a number of useful optional arguments. For instance, by allows us to control the space between the numbers:\n\ny &lt;- seq(1, 10, by = 0.5)\nprint(y)\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\n\nIf we want to specify the desired length of the resulting vector and let R choose the necessary space between the elements, we may use length.out:\n\nz &lt;- seq(2, 8, length.out = 4)\nprint(z)\n\n[1] 2 4 6 8\n\n\nAnd if we want to create a vector with the length as another vector, the argument along.with comes in handy. This is often used for creating index vectors.10 In such a case we do not have to specify the index numbers directly:\n\nz_index &lt;- seq(along.with = z)\nprint(z_index)\n\n[1] 1 2 3 4\n\n\nAnother common task is to repeat a certain vector. This can be done with rep():\n\nx &lt;- rep(NA, 5)\nprint(x)\n\n[1] NA NA NA NA NA\n\n\nOperations\nThere are a number of operations that we use very frequently together with vectors. Often we are interested in the length of a vector. For this we can use the function length():\n\nx &lt;- c(1,2,3,4)\nlength(x)\n\n[1] 4\n\n\nIf we are looking for the largest and smallest value of a vector we can use min() and max():\n\nmin(x)\n\n[1] 1\n\n\n\nmax(x)\n\n[1] 4\n\n\nBoth functions (and many more similar functions) have the optional argument na.rm, which can be either TRUE or FALSE. In the case of TRUE, all NA values are removed before the operation gets applied:\n\ny &lt;- c(1,2,3,4,NA)\nmin(y)\n\n[1] NA\n\n\n\nmin(y, na.rm = TRUE)\n\n[1] 1\n\n\nThe mean or the variance/standard deviation of the elements can be computed with mean(), var(), and sd(), all of which have also the optional argumentna.rm:\n\nmean(x)\n\n[1] 2.5\n\n\n\nvar(y)\n\n[1] NA\n\n\n\nvar(y, na.rm = T)\n\n[1] 1.666667\n\n\nFinally, we often want to compute the sum or the product of all the elements of the vector. Here the functions sum() and prod() are useful:\n\nsum(x)\n\n[1] 10\n\n\n\nprod(y, na.rm = T)\n\n[1] 24"
  },
  {
    "objectID": "content/tutorials/obj-types-ii-vectors/index.html#lists",
    "href": "content/tutorials/obj-types-ii-vectors/index.html#lists",
    "title": "Fundamental object types in R II: Vectors",
    "section": "Lists",
    "text": "Lists\nIn contrast to atomic vectors, lists can contain objects of different types. We create lists via the function list():\n\nl_1 &lt;- list(\n  \"a\",\n  c(1,2,3),\n  FALSE\n)\ntypeof(l_1)\n\n[1] \"list\"\n\n\n\nl_1\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 1 2 3\n\n[[3]]\n[1] FALSE\n\n\nLists can become very complex. The function str() (short for “structure”) helps us to get a quick overview over a list and its elements:\n\nstr(l_1)\n\nList of 3\n $ : chr \"a\"\n $ : num [1:3] 1 2 3\n $ : logi FALSE\n\n\nWe can name the elements of lists:11\n\nl_2 &lt;- list(\n  \"first_element\" = \"a\",\n  \"second_element\" = c(1,2,3),\n  \"third_element\" = FALSE\n)\n\nWe can retrieve the names of all elements of the list with names():\n\nnames(l_2)\n\n[1] \"first_element\"  \"second_element\" \"third_element\" \n\n\nThere are two very important differences in the handling of vectors and lists:\n\nVectorization does not work for lists\nIndexing works differently\n\nThe first issue can be illustrated easily:\n\nvec_expl &lt;- c(1,2,3)\nlist_expl &lt;- list(1,2,3)\nsqrt(vec_expl)\n\n[1] 1.000000 1.414214 1.732051\n\n\nBut:\n\nsqrt(list_expl)\n\nError in sqrt(list_expl): non-numeric argument to mathematical function\n\n\nThe second issue is due to the more complex structure of lists. For vectors we extracted single elements via [. For lists, there is a difference between [ and [[. The former always returns a list:\n\nl_1[2]\n\n[[1]]\n[1] 1 2 3\n\n\nThe second then returns a vector and is more similar to the behavior of [ in the context of atomic vectors:\n\nl_1[[2]]\n\n[1] 1 2 3\n\n\nTo extract an element of this vector we can chain the brackets:\n\nl_1[[2]][3]\n\n[1] 3\n\n\nWe can also extract elements by their name:\n\nl_2[[1]]\n\n[1] \"a\"\n\n\n\nl_2[[\"first_element\"]]\n\n[1] \"a\"\n\n\nLists are fundamental to many more complex structures that we will encounter later. They are more flexible than atomic vectors, but this flexibility also makes them more difficult to use and less efficient for tasks where this flexibility is not needed. As a rule of thumb, whenever you can represent something as an atomic vector, you should do so. You should always have a good reason for using lists!"
  },
  {
    "objectID": "content/tutorials/obj-types-ii-vectors/index.html#footnotes",
    "href": "content/tutorials/obj-types-ii-vectors/index.html#footnotes",
    "title": "Fundamental object types in R II: Vectors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn fact, we will learn below that 2 is not really a number, but a vector or length 1. Only in a next step, 2 counts as a ‘number’, or, more precisely as a ‘double’.↩︎\nThe only object type that is of relevance to us aside these two is NULL. We will learn about it during the end of this post.↩︎\nWhile you can abbreviate the two with T and F, respectively, I recommend against using these sometimes ambiguous abbreviations.↩︎\nThis syntax has historical reasons: when the type integer was introduced in R, the developers were guided by the type long integer of the programming language C. In C the suffix for such an integer was ‘l’ or ‘L’. The R developers just transferred this practice into R, only they did not use ‘i’ to avoid a possible confusion between ‘l’ and ‘i’, which look very similar in many fonts (the suffix ‘i’ in R is used for the imaginary component of complex numbers).↩︎\nIn principle there are different kinds of missing values, such as NA_integer_ or NA_character_, but they are irrelevant in practice: any NA value in an atomic vector automatically ‘mimics’ the type of the atomic vector.↩︎\nNULL on the other hand, is used to represent an absent vector, not an absent element of a vector. We will come back to NULL during the end of the post.↩︎\nIf you are not sure what a package is, recap the last chapter of the post on first steps in R.↩︎\nWe learn more about this later in the course when delving into the topic of iteration.↩︎\nFor many common tasks there is already a predefined function in R. The easiest way to find them is by googling↩︎\nAn index vector x to any vector y with N elements contains the integers from 1 to N. The nth value of x thus corresponds to the index of the nth value of y.↩︎\nWe can actually also do this with vectors, but it is more common in the context of lists.↩︎"
  },
  {
    "objectID": "content/tutorials/setting-up-an-r-project/index.html",
    "href": "content/tutorials/setting-up-an-r-project/index.html",
    "title": "Setting up an R project",
    "section": "",
    "text": "This post is about how you set up an adequate project environment. By this I mean the folders you should create, and how you should save your files. The structure introduced here will help you to keep your project structured and to keep an overview about your work, but also to make it easier to share your project with others.\nIn all, whenever you start a new programming project you should set up the infrastructure described below. Such project could be a term paper, a research endeavor, or just the code to create some visualizations. Later you might find that some aspects of the infrastructure below feel like a bit of an overkill, especially for very small undertakings. But especially in the beginning its better to be save than sorry and to set up the whole project as described below.\nIn all, setting up a good working environment includes the following steps:\n\nFind a good place for the project on your computer.\nCreate a directory with an R project\nCreate the relevant sub-directories\n\nThen you should always familiarize yourself with how to use the here-package with your project.\nThere are some additional steps one might to take, such as initiating a Git repository or setting up a renv environment . Moreover, for larger projects you might also want to add a README.md. But for now the steps mentioned above are sufficient. But before going through them one by one, we need to clarify two important technical concepts:\n\nthe concept of a working directory and\nthe distinction between absolute and relative paths"
  },
  {
    "objectID": "content/tutorials/setting-up-an-r-project/index.html#footnotes",
    "href": "content/tutorials/setting-up-an-r-project/index.html#footnotes",
    "title": "Setting up an R project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe could also have created a folder in the previous step and then chosen this folder here via Existing directory. This is useful if you want to add an R project file to an already existing project, but the approach of creating a new directory is more general and should be your default approach.↩︎\nWith R Markdown you can write texts directly in R. This allows you to keep statistical analysis and the description of the results in one document. This homepage, for example, was also written entirely in R Markdown. You will learn how to use R-Markdown soon.↩︎"
  },
  {
    "objectID": "content/tutorials/visualization/index.html",
    "href": "content/tutorials/visualization/index.html",
    "title": "Visualization",
    "section": "",
    "text": "Packages used\n\nlibrary(DataScienceExercises)\nlibrary(ggplot2)\n\n\n\nDeveloping a ggplot - the general workflow\nMake a shortcut to the data and inspect it:\n\ngdp_data &lt;- DataScienceExercises::gdplifexp2007\nhead(gdp_data, 3)\n\n        country continent lifeExp        pop gdpPercap\n1         China      Asia  72.961 1318683096  4959.115\n2         India      Asia  64.698 1110396331  2452.210\n3 United States  Americas  78.242  301139947 42951.653\n\n\nPlots in ggplot2 are created layer by layer. We now go through each step that, in the end, will produce the following plot:\n\n\n\n\n\n\n\n\n\nWe start by creating the basic ggplot2 object, which is best thought of as a fancy list. To this end we use the function ggplot2::ggplot()\n\ngdp_plot &lt;- ggplot2::ggplot()\ntypeof(gdp_plot)\n\n[1] \"list\"\n\n\nWhen we call this list, the plot described by it gets rendered:\n\ngdp_plot\n\n\n\n\n\n\n\n\nOf, course, there is no plot since the list is basically empty. All the specifications in the ggplot2::ggplot() function are best thought of as default values. In our case we fist specify the data set we use for our plot:\n\ngdp_plot &lt;- ggplot2::ggplot(\n  data = gdp_data\n)\n\nBut this alone does not do anything good. We also need to inform ggplot2 on how it should map the variables from the data set onto the plot. In a first step, lets clarify that the variable gdpPercap should be mapped on the x-axis and the variable lifeExp on the y-axis.\nThis is done via the argument mapping and the function ggplot2::aes(), which takes as arguments the aesthetics of the plot and the variable names that should be plotted on them:\n\ngdp_plot &lt;- ggplot2::ggplot(\n  data = gdp_data, \n  mapping = ggplot2::aes(\n    x = gdpPercap,\n    y = lifeExp\n  )\n)\ngdp_plot\n\n\n\n\n\n\n\n\nThis looks better. Note that ggplot2 chooses a default range for the axes based on the range of the variables in the underlying data set:\n\nmin(gdp_data$lifeExp); max(gdp_data$lifeExp)\n\n[1] 39.613\n\n\n[1] 82.603\n\nmin(gdp_data$gdpPercap); max(gdp_data$gdpPercap)\n\n[1] 277.5519\n\n\n[1] 49357.19\n\n\nWe now want to add an additional layer with data points on our plot. Poits are so called geom: a certain geometrical object representing data points. The function to add points is called ggplot2::geom_point() amd we literally just add it to our plot:\n\ngdp_plot &lt;- gdp_plot + geom_point()\ngdp_plot\n\n\n\n\n\n\n\n\nThis already reveals much of the general workflow involved in creating a plot: define a raw object and add and refine layers. Looking at the plot above, one thing that is missing is that the dots are filled in different colors, representing the continents of the countries, and the size of the dots represent the population size of the countries.\nTo achieve this we need to map the variable continent from the data set to the aesthetic color in the plot, and the variable pop to the aesthetic size:\n\ngdp_plot &lt;- ggplot2::ggplot(\n  data = gdp_data, \n  mapping = ggplot2::aes(\n    x = gdpPercap,\n    y = lifeExp,\n    size = pop, \n    color = continent\n    )\n  ) +\n  ggplot2::geom_point()\ngdp_plot\n\n\n\n\n\n\n\n\nWhat is not so nice is that the points are partly overlapping and bigger points might conceal smaller points below them. To address this problem we might make the plots a bit transparent. Since this is not a mapping from a variable from the data set to an aesthetic, but a general setting that should apply to all points equally, we do not specify it via the argument aes, but via the parameter responsible for transparency directly. This parameter is called alpha and we can set it for the affected geom directly:\n\ngdp_plot &lt;- ggplot2::ggplot(\n  data = gdp_data, \n  mapping = ggplot2::aes(\n    x = gdpPercap,\n    y = lifeExp,\n    size = pop, \n    color = continent\n    )\n  ) +\n  ggplot2::geom_point(alpha=0.5)\ngdp_plot\n\n\n\n\n\n\n\n\nBut now there is the danger for points to ‘melt into each other’. Better have their circle in black, and only color their interior. We can do so by replacing color in the aesthetics with fill, and set the color explicitly to 'black'. However, this distinction between circle color and fill color is not available for all kind of point shapes. You need to search the internet for a shape that supports this distinction. If you looked, for instance, here you found that they shape with index 21 allows this:\n\ngdp_plot &lt;- ggplot2::ggplot(\n  data = gdp_data, \n  mapping = ggplot2::aes(\n    x = gdpPercap,\n    y = lifeExp,\n    size = pop, \n    fill = continent\n    )\n  ) +\n  ggplot2::geom_point(\n    shape=21, color=\"black\", alpha=0.5)\ngdp_plot\n\n\n\n\n\n\n\n\nProgress cannot be denied! Now lets fix the labels and annotations of the plot. Here, the function ggplot2::labs() comes in handy. It accepts arguments such as title, subtitle, captio, and several more. The help() function gives further information about the possibilities.\nIn our case we want to add a title, specify the x and y axis, and add a caption:\n\ngdp_plot &lt;- gdp_plot +\n  ggplot2::labs(\n    title = \"Life expectancy and income per capita\", \n    caption = \"Note: size of bubbles represents population. Data: Gapminder\",\n    x = \"GDP per capita (int. Dollar)\",\n    y = \"Life expectancy in years\"\n  )\ngdp_plot\n\n\n\n\n\n\n\n\nSo far, so good. The x-axis is a bit clumsy, though. It would be better to scale the number down so that it shows 1000 dollars. The scale properties of the axes can be defined by the functions scale_*_**(), where the first * should be replaced by the aesthetic we want to adjust, and the second by a keyword indicating whether the variable is discrete or continuous, or whether we want to provide fully manual specifications. In our case we are interested in changing the x-axis, which represents a continuous variable (GDP). Thus we call scale_x_continuous(). Since we want to change the labels on the axis we specify the argument labels. To scale the labels we make use of a function from the scales-package: scales::number_format(). And to make this clear on the axis we add the suffix ‘k’:\n\ngdp_plot &lt;- gdp_plot +\n  ggplot2::scale_x_continuous(\n    labels = scales::number_format(scale = 0.001, suffix = \"k\")\n    )\ngdp_plot\n\n\n\n\n\n\n\n\nNow lets turn to the legends. First of all we want to remove the legend for the bubble size since, first, the mapping of the bubble size is not straightforward to understand and, second, we already indicated that the bubble size represents population in the caption of the plot. There are several ways to to this: either we use the scale_*_*() function we already encountered with the argument guide=\"none\":\n\ngdp_plot + ggplot2::scale_size_continuous(guide = \"none\")\n\nOr we use a function that allows us to specify all kinds of legend properties: ggplot2::guides(). Here we take the aesthetic name as an argument and set it to ´“none”`:\n\ngdp_plot &lt;- gdp_plot + ggplot2::guides(size = \"none\")\ngdp_plot\n\n\n\n\n\n\n\n\nThe advantage of using ggplot2::scale_size_continuous() would be that we could strech the limits a bit to make the differences more straightforward to see:\n\ngdp_plot &lt;- gdp_plot + \n  ggplot2::scale_size_continuous(\n    guide = \"none\", \n    range = c(0.1, 24)\n    )\n\nNow we want to put the remaining legend to the bottom of the plot. Again, there are several ways to achieve this, but for such specific changes the function ggplot2::theme() is usually a good option. It allows us to change almost everything on a plot. The argument to place legends at the bottom is legend.position and already hints at the internal logic of theme(), which you might explore through the help() function yourself:\n\ngdp_plot &lt;- gdp_plot +\n  ggplot2::theme(legend.position = \"bottom\")\ngdp_plot\n\n\n\n\n\n\n\n\nSince the theme() function is so extensive there are also many pre-defined themes for plots, which are best explored in the internet. A good default one is the black-and-white theme, which we can use via ggplot2::theme_bw():\n\ngdp_plot &lt;- gdp_plot +\n  ggplot2::theme_bw()\ngdp_plot\n\n\n\n\n\n\n\n\nOups, while everything looks nicer, some of our previous changes, such as moving the legend to the bottom and removing its title were overwritten! It, thus, makes always sense to first call the default theme, and then make further changes via ggplot::theme().\nOf course, we can then also make further adjustments to the theme, e.g.  by removing the panel of the plot. Removing elements of the plot via ggplot2::theme() requires us to set these elements via the function ggplot2::element_blank():\n\ngdp_plot &lt;- gdp_plot +\n  ggplot2::theme_bw() +\n  theme(\n    legend.position = \"bottom\",\n    panel.border = ggplot2::element_blank()\n  )\ngdp_plot\n\n\n\n\n\n\n\n\nHm, but it would indeed be a bit nicer to keep the axis lines of the x- and y-axis. Lets do this by specifying them explicitly via ggplot2::element_line(), which again allows for endless specification details:\n\ngdp_plot &lt;- gdp_plot +\n  ggplot2::theme(\n    axis.line = ggplot2::element_line(colour = \"grey\"))\ngdp_plot\n\n\n\n\n\n\n\n\nIts time to get picky! The ticks of the values should have the same color as the axis lines!!!\n\ngdp_plot &lt;- gdp_plot +\n  ggplot2::theme(\n    axis.ticks = ggplot2::element_line(colour = \"grey\"))\ngdp_plot\n\n\n\n\n\n\n\n\nOkay, you should get the general idea. What is more worrisome, to be honest, is the ugly title of the legend. Away with it!\n\ngdp_plot &lt;- gdp_plot +\n  ggplot2::theme(legend.title = ggplot2::element_blank())\ngdp_plot\n\n\n\n\n\n\n\n\nSo, the only thing that distinguishes our plot from the initial example is the color pallette. There are many different pallettes available, you can search for your favorite one in the internet. Here we use one provided by the package RColorBrewer, which can be used for the fill-aesthetic direclty:\n\ngdp_plot &lt;- gdp_plot +\n  ggplot2::scale_fill_brewer(palette = \"Dark2\")\ngdp_plot\n\n\n\n\n\n\n\n\nThats it! This was, of course, only a tiny glimpse on what you can achieve using ggplot2, but it should suffice for the start. Moreover, what is more important, you learned about the general workflow when developing a plot: start with creating a list with ´ggplot2::ggplot()` and then adjust your plot layer by layer until you are satisfied.\nHere is the whole code we used for the figure:\n\ngdp_plot &lt;- ggplot2::ggplot(\n  data = gdp_data, \n  mapping = ggplot2::aes(\n    x = gdpPercap,\n    y = lifeExp,\n    size = pop, \n    fill = continent\n  )\n) +\n  ggplot2::geom_point(\n    shape=21, color=\"black\", alpha=0.5) +\n  ggplot2::labs(\n    title = \"Life expectancy and income per capita\", \n    caption = \"Note: size of bubbles represents population. Data: Gapminder\",\n    x = \"GDP per capita (int. Dollar)\",\n    y = \"Life expectancy in years\"\n  ) +\n  ggplot2::scale_x_continuous(\n    labels = scales::number_format(scale = 0.001, suffix = \"k\")\n  ) + \n  ggplot2::scale_size_continuous(\n    guide = \"none\", \n    range = c(0.1, 24)\n  ) +\n  ggplot2::scale_fill_brewer(\n    palette = \"Dark2\"\n    ) +\n  ggplot2::theme_bw() +\n  ggplot2::theme(\n    legend.position = \"bottom\",\n    legend.title = ggplot2::element_blank(),\n    panel.border = ggplot2::element_blank(),\n    axis.line = ggplot2::element_line(colour = \"grey\"),\n    axis.ticks = ggplot2::element_line(colour = \"grey\")\n  )\n\nOf course, for simple exploratory analysis, you do not need so many details as we just did, but for publication purposes its good to know how far you can get!\nAnother great thing is that the syntax remains largely the same, no matter whether you want to make a scatter plot as above, or a line graph or a histogram. All that changes is the particular geom_*() function used.\n\n\nAn alternativ line plot\nTo illustrate the similarities of the code used for a different plot type, we will now use a data set that is very similar to the one used previously, only this time we have observations for GDP per capita and life expectancy for several years, aggregated for the different continents. The data set is gain made available via the package DataScienceExercises:\n\ngdp_data_agg &lt;- DataScienceExercises::aggGDPlifexp\n\nAgain, we first inspect the data to get a feeling about the variables that are present:\n\nhead(gdp_data_agg, 3)\n\n# A tibble: 3 × 5\n  continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Africa     1952    39.1 4570010.     1253.\n2 Africa     1957    41.3 5093033.     1385.\n3 Africa     1962    43.3 5702247.     1598.\n\n\nLets plot the dynamics of GDP per capita over time for the different continents. We can now simply copy-paste a lot of the code we have used before. Lets start with the uncontroversial beginning and just replace the name of the data set and the variable names:\n\ngdp_dyn_plot &lt;- ggplot2::ggplot(\n  data = gdp_data_agg, # &lt;- Replaced \n  mapping = ggplot2::aes(\n    x = year, # &lt;- Replaced \n    y = gdpPercap, # &lt;- Replaced \n    color = continent#, \n    #fill = continent # &lt;- Not necessary \n  )\n) +\n  ggplot2::geom_point() \ngdp_dyn_plot\n\n\n\n\n\n\n\n\nThis is not so bad! But it would be nice to add an additional geom that connects the dots with lines. No problem, simply add ggplot2::geom_line() to the plot:\n\ngdp_dyn_plot &lt;- gdp_dyn_plot +\n  geom_line()\ngdp_dyn_plot\n\n\n\n\n\n\n\n\nMuch of the code above only requires slight adjustments: the scaling of the x-axis should now be applied to the y-axis so we change ggplot2::scale_x_continuous() into ggplot2::scale_y_continuous(). Moreover, colors should change not for the fill but the color aesthetic, so ggplot2::scale_fill_brewer() becomes ggplot2::scale_color_brewer():\n\ngdp_dyn_plot &lt;- gdp_dyn_plot +\n  ggplot2::scale_y_continuous(\n    labels = scales::number_format(scale = 0.001, suffix = \"k\")\n  ) + \n  ggplot2::scale_color_brewer(\n    palette = \"Dark2\"\n    )\ngdp_dyn_plot\n\n\n\n\n\n\n\n\nAside from this, we can pretty much re-use almost the entire code from above with which we adjusted the legend, the labels, as well as the overall theme, only we can be so bold to remove the title of the x-axis via axis.title.x = ggplot2::element_blank(). Moreover, since we do not map the population size, ggplot2::scale_size_continuous() can now be removed, resulting in:\n\ngdp_dyn_plot &lt;- gdp_dyn_plot +\n  labs(\n    title = \"The divergence of income per capita\", \n    caption = \"Note: country data averaged over continants. Data: Gapminder\",\n    y = \"GDP per capita (int. Dollar)\"\n  ) +\n  ggplot2::theme_bw() +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = ggplot2::element_blank(),\n    panel.border = ggplot2::element_blank(),\n    axis.line = ggplot2::element_line(colour = \"grey\"),\n    axis.ticks = ggplot2::element_line(colour = \"grey\"),\n    axis.title.x = ggplot2::element_blank()\n  )\ngdp_dyn_plot\n\n\n\n\n\n\n\n\nAgain, a very nice plot - and much faster to complete than the first one, thanks to the amazingly consistent syntax of ggplot2:)\n\n\nSaving your plot\nYou can save your plot using the function ggplot2::ggsave(). The function saves, by default, the last plot you created, but it is better to specify the plot you want to save directly. Other important arguments are the file name (which also determines the format), and the size:\n\nggplot2::ggsave(\n  plot = gdp_plot, \n  filename = \"gdp_plot.pdf\", \n  width = 6, height = 4.2)"
  },
  {
    "objectID": "content/exercises/obj-types-exercises/index.html",
    "href": "content/exercises/obj-types-exercises/index.html",
    "title": "Basic object types: exercises",
    "section": "",
    "text": "Task 1\nCreate a vector containing the numbers 2, 5, 2.4 and 11.\n\nWhat is the type of this vector?\nReplace the second element with 5.9.\nAdd the elements 3 and 1 to the beginning, and the elements \"8.0\" and \"9.2\" to the end of the vector.\nTransform this vector into the type integer. What happens?\n\n\n\nTask 2\n\nWhat type is the following vector: \"2\", \"Hello\", 4.0, and TRUE\nWhat hierarchy is underlying this?\n\n\n\nTask 3\n\nCreate a vector with the numbers from -8 to 9 (step size: 0.5)\nCompute the square root of each element of the first vector using vectorisation. Anything that draws your attention?\n\n\n\nTask 4\nCreate a list that has three named elements: \"A\", \"B\", and \"C\"\n\nThe element \"A\" should contain the square root of the numbers form -2 to 8 (step size: 1)\nThe element \"B\" should contain the log of numbers between 2 and 4 (step size: 0.5)\nThe element \"C\" should contain letters from a1 to g7 (hint: use the pre-defined vector letters and the function paste())\n\nLink to the solutions"
  },
  {
    "objectID": "content/statrecap/ConceptualFoundations/index.html",
    "href": "content/statrecap/ConceptualFoundations/index.html",
    "title": "1: What is Statistics all about? Conceptual Foundations",
    "section": "",
    "text": "Packages used for R examples\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(kableExtra)\n\n\n\nIntroduction: Why Statistics Might Feel Intimidating (And Why It Shouldn’t)\nIf you’re reading this with a slight sense of dread, you’re not alone. Many master’s students in management and business fields approach statistics with apprehension, often because they’ve had negative experiences with mathematics in the past or because statistics feels abstract and removed from practical business applications. Let’s start by addressing this directly: statistics is not about complex mathematical formulas that only mathematicians can understand. Instead, it’s a powerful toolkit for making sense of uncertainty and making better decisions in an uncertain world.\nThink of statistics as a language—a way of communicating with data and extracting meaningful insights from the noise of everyday business and research activities. Just as you wouldn’t expect to become fluent in a foreign language overnight, becoming comfortable with statistical thinking takes time and practice. The goal isn’t to become a mathematician; it’s to develop a mindset that helps you navigate uncertainty with confidence.\n\n\nWhat is Statistics and Why Do We Need It?\nAt its core, statistics is the science of learning from data. More specifically, it’s a collection of methods and principles that help us collect, organize, analyze, and interpret information to answer questions and solve problems. But why do we need this formal approach? Why can’t we just look at data and draw conclusions intuitively?\n\nConsider a simple business scenario: You’re the marketing manager for a company that recently launched a new advertising campaign. After three months, you notice that sales have increased by over 56% if compared to the average sales before the campaign. This sounds like good news, but several questions immediately arise: Is this increase actually due to your campaign, or could it be caused by seasonal trends, competitor actions, or random fluctuations? How confident can you be that this trend will continue? Is this increase significant enough to justify the campaign’s cost? Figure 1 gives a first idea of why the answer to these questions is not as easy as it might appear and requires thorough statistical reasoning and computation skills!\n\n\n\nCreating the dataset used in example\n# Set seed for reproducibility\nset.seed(123)\n\n# Create monthly dates for a 2-year period\ndates &lt;- seq(as.Date(\"2023-01-01\"), as.Date(\"2024-12-01\"), by = \"month\")\nmonth &lt;- format(dates, \"%m\")\nyear &lt;- format(dates, \"%Y\")\n\n# Create a seasonal pattern (higher in Q4, lower in Q1)\nseasonal_factor &lt;- c(0.8, 0.7, 0.8, 0.9, 1.0, 1.0, 0.9, 0.9, 1.1, 1.3, 1.4, 1.6)\nmonthly_effect &lt;- seasonal_factor[as.numeric(month)]\n\n# Base sales with year-over-year growth and seasonal effects\nbase_sales &lt;- 100000 * (1 + 0.03 * (as.numeric(year) - 2023)) * monthly_effect\n# Add random noise\nsales &lt;- base_sales * rnorm(length(dates), mean = 1, sd = 0.04)\n\n# Create campaign effect (after September 2024)\n# campaign effect is  5%, starts right before seasonal upswing\ncampaign_date &lt;- as.Date(\"2024-09-01\")\ncampaign_effect &lt;- ifelse(dates &gt;= campaign_date, 1.05, 1)\n\n# Apply campaign effect to get final sales\nfinal_sales &lt;- sales * campaign_effect\n\n# Create data frame\nsales_data &lt;- data.frame(\n  date = dates,\n  month = month,\n  year = year,\n  sales = round(final_sales),\n  campaign = dates &gt;= campaign_date\n)\n\n\n\n\nR code for the visualization\nblue_col &lt;- \"#3498db\"\nred_col &lt;- \"#e74c3c\"\n\n# Get only 2024 data for the first plot\nsales_data_2024 &lt;- filter(sales_data, year == \"2024\")\n\n# Calculate misleading metrics someone might report (2024 only)\nmean2024_before_campaign&lt;- mean(filter(sales_data_2024, !campaign)$sales)\nmean2024_after_campaign &lt;- mean(filter(sales_data_2024, campaign)$sales)\nnaive_increase_2024 &lt;- (\n  mean2024_after_campaign / mean2024_before_campaign - 1) * 100\ncorrected_increase &lt;- sales_data |&gt;\n  filter(month %in% filter(sales_data_2024, campaign)$month) |&gt;\n  summarise(avg_sales=mean(sales), .by = year) |&gt;\n  pivot_wider(names_from = \"year\", values_from = \"avg_sales\") |&gt;\n  mutate(increase=(`2024` / `2023` - 1) *100) |&gt;\n  pull(\"increase\")\n\n# Plot 1: 2024-only view\nplot_2024 &lt;- ggplot(sales_data_2024, aes(x = date, y = sales)) +\n  geom_line(alpha = 0.6, linewidth = 0.8) +\n  geom_point(aes(color = campaign), alpha = 0.8, size = 2.5) +\n  geom_vline(\n    xintercept = as.Date(\"2024-09-01\"),\n    linetype = \"dashed\", color = \"darkred\"\n  ) +\n  # Add annotation for campaign launch\n  annotate(\"text\",\n    x = as.Date(\"2024-09-01\") - 15, y = max(sales_data_2024$sales) * 0.95,\n    label = \"Campaign Launch\", hjust = 1, color = \"darkred\"\n  ) +\n  # Add horizontal line for average before\n  geom_hline(\n    yintercept = mean2024_before_campaign, linetype = \"dotted\", color = blue_col\n    ) +\n  annotate(\"text\",\n    x = as.Date(\"2024-03-15\"), y = mean2024_before_campaign * 1.03,\n    label = paste0(\"Avg Before: \", \n                   format(round(mean2024_before_campaign), \n                          big.mark = \".\", \n                          decimal.mark = \",\")\n                   ),\n    color = blue_col\n  ) +\n  # Add horizontal line for average after\n  geom_hline(\n    yintercept = mean2024_after_campaign, linetype = \"dotted\", color = red_col\n    ) +\n  annotate(\"text\",\n    x = as.Date(\"2024-03-15\"), y = mean2024_after_campaign * 1.03,\n    label = paste0(\"Avg After: \", \n                   format(round(mean2024_after_campaign), \n                          big.mark = \".\", \n                          decimal.mark = \",\")),\n    color = red_col\n  ) +\n  # Add title and labels\n  labs(\n    title = \"2024 Sales Before and After Marketing Campaign\",\n    subtitle = paste0(\n      \"It appears the campaign increased sales by \",\n      round(naive_increase_2024, 1), \"%!\"\n    ),\n    x = \"Month (2024)\",\n    y = \"Sales\",\n    color = \"After Campaign Launch\"\n  ) +\n  # Styling options\n  scale_color_manual(\n    values = c(\"FALSE\" = blue_col, \"TRUE\" = red_col)\n    ) +\n  scale_y_continuous(\n    labels = scales::number_format(scale = 0.001, suffix = \"k €\")\n    ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.title.x = element_blank())\n\n# Plot 2: The full context with both years\nplot_full &lt;- ggplot(sales_data, aes(x = date, y = sales)) +\n  geom_line(alpha = 0.5) +\n  geom_point(aes(color = campaign), alpha = 0.7, size = 2) +\n  geom_vline(\n    xintercept = as.Date(\"2024-09-01\"),\n    linetype = \"dashed\", color = \"darkred\"\n  ) +\n  geom_vline(\n    xintercept = as.Date(\"2024-01-01\"),\n    linetype = \"dotted\", color = \"gray50\"\n  ) +\n  annotate(\"text\",\n    x = as.Date(\"2024-09-01\") - 15, y = max(sales_data$sales) * 0.9,\n    label = \"Campaign Launch\", hjust = 1, color = \"darkred\"\n  ) +\n  annotate(\"rect\",\n    xmin = as.Date(\"2023-09-01\"), xmax = as.Date(\"2023-12-31\"),\n    ymin = min(sales_data$sales) * 0.95, ymax = max(sales_data$sales),\n    alpha = 0.1, fill = \"darkblue\"\n  ) +\n  annotate(\"rect\",\n    xmin = as.Date(\"2024-09-01\"), xmax = as.Date(\"2024-12-31\"),\n    ymin = min(sales_data$sales) * 0.95, ymax = max(sales_data$sales),\n    alpha = 0.1, fill = \"darkred\"\n  ) +\n  annotate(\"text\",\n    x = as.Date(\"2023-06-01\"), y = max(sales_data$sales) * 0.85,\n    label = paste0(\"True year-over-year increase: \", \n                   round(corrected_increase, 1)  , \"%\"),\n    color = \"black\", size = 4\n  ) +\n  annotate(\"text\",\n    x = as.Date(\"2023-06-01\"), y = max(sales_data$sales) * 0.8,\n    label = \"Actual campaign effect: 5%\",\n    color = \"black\", size = 4\n  ) +\n  labs(\n    title = \"Full Context: Sales Data for 2023-2024\",\n    subtitle = \"Most of the increase is due to seasonal patterns\",\n    x = \"Month\",\n    y = \"Sales\",\n    color = \"After Campaign Launch\"\n  ) +\n  # Styling options\n  scale_color_manual(\n    values = c(\"FALSE\" = blue_col, \"TRUE\" = red_col)\n    ) +\n  scale_y_continuous(\n    labels = scales::number_format(scale = 0.001, suffix = \"k €\")\n    ) +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.title.x = element_blank())\n\n# Combine the plots\ncombined_plot &lt;- ggarrange(plot_2024, plot_full,\n  labels = c(\"A\", \"B\"),\n  ncol = 1, nrow = 2,\n  legend = \"none\"\n)\n\n# Add an overall title\ncombined_plot &lt;- annotate_figure(\n  combined_plot,\n  top = text_grob(\"The Importance of Sound Data Analysis\",\n    face = \"bold\", size = 14\n  )\n)\ncombined_plot\n\n\n\n\n\n\n\n\nFigure 1: The apparent effect of a marketing campaign!\n\n\n\n\n\nThese questions illustrate why we need statistics. Our intuition, while valuable, is often inadequate for making sense of complex data patterns. Humans are naturally prone to seeing patterns where none exist (we call this “apophenia”) and tend to overinterpret small samples or unusual events. Statistics provides us with rigorous methods to distinguish between real patterns and random noise, to quantify our uncertainty, and to make informed decisions despite incomplete information.\nStatistics serves several crucial functions in business and research contexts. First, it helps us describe and summarize large amounts of data in meaningful ways. When faced with thousands of customer transactions, we can use statistical measures to understand central tendencies, variability, and patterns that would be impossible to grasp by examining individual data points.\nSecond, statistics enables us to make inferences about populations based on samples. This is particularly valuable in business, where it’s often impractical or impossible to survey every customer or test every possible scenario. By studying a representative sample, we can draw conclusions about the broader population with a known degree of confidence.\n\nFor instance, if you want to understand customer satisfaction across your company’s 50,000 customers, you don’t need to survey all 50,000. A properly designed survey of 1,000 randomly selected customers can give you reliable insights about the entire customer base, saving time and resources while still providing actionable information.\n\nThird, statistics helps us test hypotheses and evaluate claims. When someone claims that a new training program improves employee productivity, statistics provides the framework for testing whether this claim is supported by evidence or whether observed differences could reasonably be attributed to chance.\nFinally, statistics enables prediction and forecasting. While we cannot predict the future with certainty, statistical models help us understand relationships between variables and make informed projections about likely outcomes under different scenarios.\n\n\nWhat is Probability Theory and Why Do We Need It?\nProbability theory might sound abstract, but it serves a crucial practical purpose: it provides the reference point we need to make sense of what we observe in data. Without probability theory, we cannot determine whether our observations are surprising, expected, or somewhere in between.\n\nImagine you’re recruiting for a basketball team and a candidate tells you he was 200 cm tall. Is this unusually tall or fairly normal? You can’t answer this question just by looking at this single number. You need a reference point - specifically, you need to know how height is distributed in the general population. Probability theory tells us that height of men in Germany follows a roughly normal distribution with an average around 178.9 cm and a certain spread around that average. With this reference, we can determine that 200 cm is indeed rare - occurring in perhaps 1 in 520 people or fewer (see Figure 2).\n\n\n\nR code to compute change for being 200cm or higher\nmean_height &lt;- 178.9 # Mean height of German men\nsd_height &lt;- 7.3  # Standard deviation\n\n# Create a data frame for the distribution\nheight_range &lt;- seq(150, 230, by = 0.1)\nheight_density &lt;- dnorm(height_range, mean = mean_height, sd = sd_height)\n\nheight_df &lt;- data.frame(\n  height = height_range,\n  density = height_density\n)\n\n# Calculate the probability of being 200 cm or taller\nprob_200_plus &lt;- pnorm(\n  q = 200, mean = mean_height, sd = sd_height, lower.tail = FALSE)\napprox_odds &lt;- round(1 / prob_200_plus)\n\n\n\n\nR code for the visualization\nggplot(height_df, aes(x = height, y = density)) +\n  geom_line(size = 1.2, color = \"royalblue\") +\n  geom_area(fill = \"royalblue\", alpha = 0.3) +\n  \n  geom_vline(xintercept = mean_height, linetype = \"dashed\", color = \"darkblue\") +\n  annotate(\"text\", x = mean_height + 4, y = max(height_density) * 0.95, \n           label = paste0(\"Mean = \", mean_height, \" cm\"), \n           hjust = 0, color = \"darkblue\") +\n  \n  geom_vline(xintercept = 200, linetype = \"dashed\", color = \"red\") +\n  \n  geom_area(data = subset(height_df, height &gt;= 200),\n            fill = \"red\", alpha = 0.4) +\n  \n  # Add probability annotation\n  annotate(\"text\", x = 205, y = max(height_density) * 0.6,\n           label = paste0(\"Probability ≈ 1 in \", format(approx_odds, big.mark=\",\")),\n           color = \"red\", hjust = 0.0) +\n  \n  # Add standard deviation markers\n  geom_segment(aes(x = mean_height + sd_height, xend = mean_height + sd_height, \n                  y = 0, yend = dnorm(mean_height + sd_height, mean = mean_height, sd = sd_height)),\n              linetype = \"dotted\", color = \"gray30\") +\n  geom_segment(aes(x = mean_height + 2*sd_height, xend = mean_height + 2*sd_height, \n                  y = 0, yend = dnorm(mean_height + 2*sd_height, mean = mean_height, sd = sd_height)),\n              linetype = \"dotted\", color = \"gray30\") +\n  geom_segment(aes(x = mean_height + 3*sd_height, xend = mean_height + 3*sd_height, \n                  y = 0, yend = dnorm(mean_height + 3*sd_height, mean = mean_height, sd = sd_height)),\n              linetype = \"dotted\", color = \"gray30\") +\n  \n  # Add titles and labels\n  labs(\n    title = \"Distribution of Adult Male Height\",\n    subtitle = paste0(\"200 cm is an extreme value (approximately 1 in \", \n                     format(approx_odds, big.mark=\",\"), \" people)\"),\n    x = \"Height (cm)\",\n    y = \"Probability Density\",\n    caption = \"Note: Based on average adult male height distribution for Germany.\"\n  ) +\n  \n  # Set theme and appearance\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(color = \"darkred\", size = 12),\n    axis.title = element_text(size = 12),\n    panel.grid.minor = element_blank()\n  ) +\n  \n  # Set axis limits to focus on the relevant part of the distribution\n  coord_cartesian(xlim = c(150, 230), ylim = c(0, max(height_density) * 1.05))\n\n\n\n\n\n\n\n\nFigure 2: The apparent effect of a marketing campaign!\n\n\n\n\n\nThis example illustrates the fundamental role of probability theory: it establishes baselines against which we can evaluate our observations. In business contexts, this principle applies constantly. Is a 15% increase in sales after a marketing campaign impressive? We can only answer this by comparing it to the typical variation in sales we’d expect to see without any intervention.\nProbability theory provides us with mathematical models that describe what we should expect to see if only random variation is at play. These models serve as our null hypothesis - our baseline assumption of “nothing special happening.” When our actual observations differ substantially from what these probability models predict, we have evidence that something interesting might be occurring.\n\nConsider quality control in manufacturing. Suppose your process typically produces 2% defective items. Probability theory can tell you what to expect in a batch of 100 items: most batches will have 1-3 defective items, occasionally you might see 0 or 4-5, and very rarely you might see 6 or more (see Figure 3). If you test a batch and find 8 defective items, probability theory helps you recognize this as highly unusual - suggesting something may have gone wrong with your process (see (tab-defects?)).\n\n\n\nR code to compute expected defects\n# Define parameters\nbatch_size &lt;- 100     # Number of items in a batch\ndefect_rate &lt;- 0.02   # Probability of a defective item (2%)\nobserved_defects &lt;- 8 # Number of defects found in a batch\n\n# Create a table of probabilities for different numbers of defects\ndefect_probs &lt;- tibble(\n  defects = 0:15,  # Range of possible defects to consider\n  \n  # Calculate exact probability for each number of defects\n  exact_prob = dbinom(defects, size = batch_size, prob = defect_rate),\n  \n  # Calculate probability of seeing AT LEAST this many defects\n  cumulative_prob = pbinom(defects - 1, size = batch_size, prob = defect_rate, \n                           lower.tail = FALSE),\n  \n  # Format probabilities as percentages\n  exact_prob_pct = scales::percent(exact_prob, accuracy = 0.001),\n  cumulative_prob_pct = scales::percent(cumulative_prob, accuracy = 0.001),\n  \n  # Calculate the odds (1 in X) for at least this many defects\n  odds = ifelse(cumulative_prob &gt; 0, round(1/cumulative_prob), Inf)\n)\n\n# Table showing key probabilities (first 11 rows)\ndefect_table &lt;- defect_probs %&gt;%\n  filter(defects &lt;= 10) %&gt;%\n  select(defects, exact_prob_pct, cumulative_prob_pct, odds) %&gt;%\n  rename(\n    \"Defects\" = defects,\n    \"Exact Probability\" = exact_prob_pct,\n    \"P(X ≥ Defects)\" = cumulative_prob_pct,\n    \"Odds (1 in X)\" = odds\n  )\n\n# Display the table\nkable(defect_table)\n\n\n\n\n\nDefects\nExact Probability\nP(X ≥ Defects)\nOdds (1 in X)\n\n\n\n\n0\n13.262%\n100.000%\n1\n\n\n1\n27.065%\n86.738%\n1\n\n\n2\n27.341%\n59.673%\n2\n\n\n3\n18.228%\n32.331%\n3\n\n\n4\n9.021%\n14.104%\n7\n\n\n5\n3.535%\n5.083%\n20\n\n\n6\n1.142%\n1.548%\n65\n\n\n7\n0.313%\n0.406%\n246\n\n\n8\n0.074%\n0.093%\n1073\n\n\n9\n0.015%\n0.019%\n5282\n\n\n10\n0.003%\n0.003%\n29056\n\n\n\nThe likelihoods of seeing different numbers of defect items.\n\n\n\n\nR code for the visualization\n# Create visualization of the probability distribution\nggplot(defect_probs %&gt;% filter(defects &lt;= 15), \n       aes(x = defects, y = cumulative_prob)) +\n  # Add line for cumulative probability\n  geom_line(color = \"darkblue\", size = 1) +\n  \n  # Add points\n  geom_point(color = \"darkblue\", size = 3) +\n  \n  # Highlight the observed value (8 defects)\n  geom_point(data = defect_probs %&gt;% filter(defects == 8), \n             color = \"red\", size = 4) +\n  \n  # Add shading for area of interest (≥ 8 defects)\n  geom_area(data = defect_probs %&gt;% filter(defects &gt;= 8), \n            fill = \"red\", alpha = 0.3) +\n  \n  # Add annotation for probability\n  annotate(\"text\", x = 10, y = 0.1,\n           label = paste0(\"P(X ≥ 8) = \", \n                          defect_probs$cumulative_prob_pct[defect_probs$defects == 8]),\n           color = \"darkred\") +\n  \n  # Add titles and labels\n  labs(\n    title = \"Probability of Finding X or More Defects\",\n    subtitle = paste0(\"Finding 8+ defects when defect rate is 2% happens in only 1 in \", \n                      defect_probs$odds[defect_probs$defects == 8], \" batches\"),\n    x = \"Number of Defective Items (X)\",\n    y = \"Probability of X or More Defects\",\n    caption = \"Based on binomial distribution B(100, 0.02)\"\n  ) +\n  \n  # Customize the appearance\n  theme_minimal() +\n  \n  # Set y-axis as percentage\n  scale_y_continuous(labels = scales::percent_format()) +\n  \n  # Set x-axis to show only whole numbers of defects\n  scale_x_continuous(breaks = 0:15)\n\n\n\n\n\n\n\n\nFigure 3: The likelihoods of seeing different numbers of defect items.\n\n\n\n\n\nThe key insight is that probability theory doesn’t require us to know exactly what will happen - it tells us about the range of possibilities and how likely each one is. This framework transforms our question from “What will happen?” to “How unusual is what we observed?” This shift is fundamental to statistical thinking.\nProbability theory also helps us understand that unusual events do occur naturally through random variation. Just as you might occasionally flip a coin and get five heads in a row purely by chance, business processes will sometimes produce unusual results even when operating normally. Probability theory helps us distinguish between these natural anomalies and genuine signals that require attention.\n\n\nHow Do Probability and Statistics Work Together?\nThe relationship between probability and statistics represents one of the most elegant partnerships in scientific thinking. They work together to bridge the gap between theoretical expectations and real-world observations, creating a powerful cycle of reasoning that drives both business decision-making and scientific discovery.\nProbability provides the theoretical framework - it tells us what patterns we should expect to see if certain assumptions are true. Statistics then examines real data to see whether these expected patterns actually appear, allowing us to evaluate our assumptions and refine our understanding.\n\nThink of a retail company testing whether a new store layout increases customer spending. Probability theory might suggest that if the layout truly has no effect, we should expect to see the same average spending as before, with purchases varying randomly around that average. If the new layout does have an effect, we should see a systematic shift in the spending pattern. Statistics then analyzes actual customer data to determine whether the observed spending pattern looks more like the “no effect” scenario or the “positive effect” scenario. Figure Figure 4 shows the two crucial steps: you first use probability theory to construct a reference case, i.e. what you would expect to see of the new layout had no effect. Then you collect data and compare the collected data against the theoretical prediction and use tools from inferential statistics to make a rational conclusion.\n\n\n\nR code for the visualization\n# Set seed for reproducibility\nset.seed(123)\n\n# Define parameters\nold_mean &lt;- 65             # Mean spending with old layout (€)\nold_sd &lt;- 12               # Standard deviation with old layout (€)\nn_customers &lt;- 40          # Sample size for new layout test\n\n# Generate spending range for theoretical distributions\nspending_range &lt;- seq(30, 120, by = 0.5)\n\n# Calculate standard error for the sample mean\nse &lt;- old_sd / sqrt(n_customers)\n\n# Create a data frame for theoretical distributions\ntheory_df &lt;- tibble(\n  spending = spending_range,\n  # Distribution of individual customer spending\n  null_density = dnorm(spending_range, mean = old_mean, sd = old_sd),\n  # Sampling distribution of the mean\n  sampling_density = dnorm(spending_range, mean = old_mean, sd = se)\n)\n\n# Create Panel A: Probability theory perspective\ntheory_plot &lt;- ggplot(theory_df) +\n  # Add the null hypothesis distribution (individual customers)\n  geom_line(aes(x = spending, y = null_density), \n            color = \"darkblue\", size = 1.2) +\n  geom_area(aes(x = spending, y = null_density), \n            fill = \"darkblue\", alpha = 0.2) +\n  \n  # Add the sampling distribution of the mean (what we'd expect if H0 is true)\n  geom_line(aes(x = spending, y = sampling_density), \n            color = \"purple\", size = 1.2, linetype = \"dashed\") +\n  \n  # Add vertical line for baseline (old layout) mean\n  geom_vline(xintercept = old_mean, linetype = \"solid\", color = \"darkblue\") +\n\n  # Add labels and title\n  labs(\n    title = \"Probability Theory Perspective\",\n    subtitle = \"What we would expect to see if the new layout had no effect\",\n    x = \"Customer Spending (EUR)\",\n    y = \"Probability Density\"\n  ) +\n  \n  # Add annotations explaining the distributions\n  annotate(\"text\", x = 40, y = max(theory_df$null_density) * 1.5,\n           label = \"Individual customer\\nspending distribution\\nunder old layout\",\n           color = \"darkblue\", hjust = 0) +\n  annotate(\"text\", x = 75, y = max(theory_df$sampling_density) * 0.7,\n           label = \"Sampling distribution\\nof the mean\\n(if null hypothesis is true)\",\n           color = \"purple\", hjust = 0) +\n  \n  # Customize theme\n  theme_minimal()\n\n# Statistics perspective:\n\n# Create the actual observed data (simulated for this example)\n# We'll assume the new layout has an actual effect of +8€\nactual_effect &lt;- 8\nnew_layout_data &lt;- tibble(\n  spending = rnorm(n_customers, mean = old_mean + actual_effect, sd = old_sd),\n  layout = \"New Layout Test\"\n)\n\n# Calculate observed sample mean from new layout test\nobserved_mean &lt;- mean(new_layout_data$spending)\n\n# Calculate z-score for the observed mean\nz_score &lt;- (observed_mean - old_mean) / se\n\n# Calculate p-value (one-tailed test)\np_value &lt;- pnorm(z_score, lower.tail = FALSE)\n\n# Create Panel B: Statistical analysis of the actual data\ndata_plot &lt;- ggplot() +\n  # Add sampling distribution under null hypothesis\n  geom_line(data = theory_df, aes(x = spending, y = sampling_density), \n            color = \"purple\", size = 1) +\n  geom_area(data = theory_df, aes(x = spending, y = sampling_density), \n            fill = \"purple\", alpha = 0.1) +\n  \n  # Add vertical line for null hypothesis (old layout mean)\n  geom_vline(xintercept = old_mean, linetype = \"solid\", color = \"darkblue\") +\n  annotate(\"text\", x = old_mean - 1, y = max(theory_df$sampling_density) * 0.9, \n           label = \"Old Layout Mean\\n(null hypothesis)\", hjust = 1, color = \"darkblue\") +\n  \n  # Add critical value line\n  geom_vline(xintercept = old_mean + qnorm(0.95) * se, \n             linetype = \"dotted\", color = \"red\") +\n  \n  # Add the observed mean from our new layout test\n  geom_vline(xintercept = observed_mean, linetype = \"dashed\", \n             color = \"forestgreen\", size = 1) +\n  annotate(\"text\", x = observed_mean + 1, y = max(theory_df$sampling_density) * 0.9, \n           label = paste0(\"New Layout\\nObserved Mean: \", round(observed_mean, 1), \"€\"), \n           hjust = 0, color = \"forestgreen\") +\n  \n  # Add individual data points at the bottom for visual context\n  geom_jitter(data = new_layout_data, aes(x = spending, y = 0), \n              height = 0.0005, color = \"forestgreen\", alpha = 0.7) +\n  \n  geom_density(data = new_layout_data, aes(x = spending), \n             color = \"forestgreen\", fill = \"forestgreen\", alpha = 0.2, adjust = 1.5) +\n  \n  # Shade the p-value region\n  geom_area(data = filter(theory_df, spending &gt;= observed_mean), \n            aes(x = spending, y = sampling_density), fill = \"red\", alpha = 0.3) +\n  \n  # Add annotation for p-value\n  annotate(\"text\", x = 100, y = max(theory_df$sampling_density) * 0.7,\n           label = paste0(\"p-value = \", round(p_value, 4), \"\\n\",\n                          \"z-score = \", round(z_score, 2)),\n           color = \"red\", hjust = 0.5) +\n  \n  # Add title and labels\n  labs(\n    title = \"Statistical Analysis of New Layout Test\",\n    subtitle = paste0(\"Evaluating evidence against the null hypothesis\"),\n    x = \"Customer Spending (€)\",\n    y = \"Probability Density\"\n  ) +\n  \n  # Customize theme\n  theme_minimal()\n\n# Combine the plots\nggarrange(\n  theory_plot, data_plot,\n  ncol = 1, nrow = 2,\n  labels = c(\"A\", \"B\"),\n  heights = c(1, 1.2)\n)\n\n\n\n\n\n\n\n\nFigure 4: How to use probability theory and statistics in conjunction.\n\n\n\n\n\nThis partnership creates several important capabilities. First, it allows us to move beyond simple description to meaningful inference. We don’t just observe that sales increased by 15% - we can assess whether this increase is likely due to our intervention or could reasonably be explained by normal business fluctuations.\nSecond, the probability-statistics partnership helps us calibrate our confidence in conclusions. When we observe an effect in our data, probability theory helps us calculate how likely we would be to see such an effect if nothing real were happening. This gives us a principled way to decide how much weight to place on our findings.\n\nFor example, if probability calculations show that we’d see a 15% sales increase less than 5% of the time purely by chance, we can be fairly confident that our marketing campaign contributed to the improvement. If such an increase would happen 40% of the time by chance alone, we should be much more cautious about claiming success.\n\nThird, this relationship enables prediction and planning. By understanding both the underlying probability patterns and how to extract information from data, we can make informed projections about future outcomes and assess the reliability of those projections.\nThe interplay between probability and statistics also illuminates why statistical reasoning requires both theoretical understanding and practical experience with data. Without probability theory, we might misinterpret random fluctuations as meaningful trends. Without statistical methods for analyzing real data, probability remains purely academic.\n\n\nThe Role of Uncertainty in Business and Research\nUnderstanding uncertainty represents perhaps the most crucial mindset shift for students approaching statistics. In many academic disciplines, we’re taught to seek definitive answers - to prove or disprove propositions decisively. Statistics operates differently. Rather than eliminating uncertainty, it teaches us to acknowledge uncertainty as an inherent feature of complex systems and to make rational decisions despite incomplete information.\nIn business contexts, uncertainty permeates every decision. Market conditions shift unpredictably, consumer preferences evolve, competitors make unexpected moves, and economic conditions fluctuate. Even within organizations, employee performance varies, operational processes contain natural variation, and strategic outcomes depend on countless unpredictable factors.\n\nConsider a product manager deciding how many units to manufacture for the upcoming holiday season. They might estimate demand at 10,000 units based on historical data and market research. However, actual demand could easily range from 7,000 to 13,000 units depending on economic conditions, competitor actions, weather, and countless other factors. Statistics doesn’t eliminate this uncertainty, but it helps the manager understand the range of possibilities and make informed decisions about production levels, inventory management, and risk mitigation.\n\nStatistics teaches us that uncertainty isn’t a flaw in our analysis - it’s a fundamental characteristic of the world we must incorporate into our decision-making processes. This represents a mature, sophisticated approach to management and research. Instead of pretending we can predict everything precisely, we learn to work productively within uncertainty.\nThis perspective affects how we interpret findings and draw conclusions. When research suggests that a new management technique improves productivity, we don’t just want to know the average improvement - we want to understand the range of outcomes we might reasonably expect and the factors that influence this variation.\n\nA training program might show an average productivity increase of 12%, but this could mean that most employees see improvements between 8-16%, while a few see dramatic gains and others see little change. Understanding this variability helps managers set realistic expectations and tailor implementation strategies.\n\nEmbracing uncertainty also cultivates intellectual humility. Statistical thinking encourages us to be appropriately cautious about our conclusions, to acknowledge the limitations of our data and methods, and to update our beliefs when presented with new evidence. In a business world that often rewards confident assertions and decisive action, this nuanced approach can initially feel uncomfortable but ultimately leads to more robust strategies and better long-term outcomes.\nThis mindset connects statistics to critical thinking more broadly. Just as statistics teaches us to distinguish between correlation and causation, to recognize the limitations of small samples, and to account for various sources of bias, it cultivates careful, evidence-based reasoning that extends far beyond purely quantitative contexts.\n\n\nConclusion: Building Statistical Intuition\nAs we conclude this foundational overview, it’s worth reflecting on what we’re really trying to accomplish in developing statistical understanding. We’re not just learning tools and techniques - we’re cultivating a way of thinking about evidence, uncertainty, and decision-making that will serve you throughout your careers in management and research.\nStatistical thinking involves several key habits of mind. It means being curious about data and asking probing questions about what patterns might mean and what factors might explain them. It means being appropriately skeptical of simple explanations for complex phenomena while remaining open to evidence. Most importantly, it means being comfortable making decisions under uncertainty while acknowledging the limits of our knowledge.\n\nThink of statistical reasoning as developing a new form of professional judgment. Just as an experienced manager learns to read market signals, assess team dynamics, and anticipate potential problems, statistical thinking provides systematic methods for evaluating evidence and making informed decisions when complete information isn’t available.\n\nAs you progress through this course, remember that developing statistical intuition is a gradual process. The concepts we’ve introduced here - the need for reference points to interpret data, the partnership between probability and statistics, the reality of uncertainty in business decisions - will become more concrete and intuitive as you work with real data and tackle practical problems.\nThe investment in developing this statistical mindset will pay dividends far beyond any single course or project. In an increasingly data-rich business environment, the ability to think clearly about uncertainty, extract meaningful insights from complex information, and communicate these insights effectively has become an essential management skill.\nYour journey into statistical thinking begins with recognizing that uncertainty is not the enemy of good decision-making - it’s simply the context within which all important decisions must be made. Statistics provides the tools and frameworks to make those decisions as wisely as possible, given the information available. This is both the challenge and the power of statistical reasoning: finding clarity and direction amid the inherent uncertainty of business and research endeavors.\n\n\nNext steps\n\nBack to the recap overview\nNext topic: Data fundamentals"
  },
  {
    "objectID": "content/statrecap/DescriptiveStatistics/index.html",
    "href": "content/statrecap/DescriptiveStatistics/index.html",
    "title": "3: Descriptive Statistics",
    "section": "",
    "text": "Packages used in R examples\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(knitr)"
  },
  {
    "objectID": "content/statrecap/DescriptiveStatistics/index.html#correlation-matrices",
    "href": "content/statrecap/DescriptiveStatistics/index.html#correlation-matrices",
    "title": "3: Descriptive Statistics",
    "section": "Correlation Matrices",
    "text": "Correlation Matrices\nWhen working with multiple variables simultaneously, correlation matrices become invaluable tools in business analysis. A correlation matrix shows the correlation between every pair of variables in your dataset. Reading a correlation matrix is like reading a multiplication table, but for relationships between business metrics.\nThe diagonal of a correlation matrix always contains 1s because every variable is perfectly correlated with itself. The matrix is symmetric because the correlation between X and Y is the same as the correlation between Y and X.\nIn a correlation matrix examining business performance metrics, you might find that “customer satisfaction” correlates strongly with “repeat purchase rate” (0.82) but weakly with “marketing spend” (0.15), while “employee satisfaction” might have a moderate positive correlation with “customer satisfaction” (0.58).\nWhen interpreting correlation matrices in business contexts, look for:\n\nStrong positive correlations (close to +1) that suggest complementary metrics\nStrong negative correlations (close to -1) that might indicate trade-offs\nWeak correlations (close to 0) suggesting independent factors\n\n\n\nCreate artificial data ‘business_metrics’\nlibrary(dplyr)\n# Creating a realistic business dataset\nset.seed(123)\nbusiness_metrics &lt;- data.frame(\n  advertising_spend = rnorm(100, mean = 50, sd = 15),  # thousands of euros\n  customer_satisfaction = rnorm(100, mean = 3.8, sd = 0.5),  # 1-5 scale\n  employee_satisfaction = rnorm(100, mean = 3.5, sd = 0.6),  # 1-5 scale\n  training_hours = rnorm(100, mean = 25, sd = 8)  # hours per quarter\n)\n\n# Create interdependent business metrics\nbusiness_metrics &lt;- business_metrics %&gt;%\n  mutate(\n    # Sales influenced by advertising and customer satisfaction\n    sales_revenue = 100 + \n      1.2 * advertising_spend + \n      30 * customer_satisfaction + \n      rnorm(100, 0, 20),\n    \n    # Customer satisfaction influenced by employee satisfaction\n    customer_satisfaction = customer_satisfaction + \n      0.3 * employee_satisfaction + \n      rnorm(100, 0, 0.2),\n    \n    # Employee satisfaction influenced by training\n    employee_satisfaction = employee_satisfaction + \n      0.02 * training_hours + \n      rnorm(100, 0, 0.15),\n    \n    # Profit margin influenced by efficiency (inverse of spending per revenue)\n    profit_margin = 15 + \n      sales_revenue * 0.1 - \n      advertising_spend * 0.5 + \n      customer_satisfaction * 2 + \n      rnorm(100, 0, 5)\n  )\n\n# Clean up the correlations by ensuring realistic ranges\nbusiness_metrics$customer_satisfaction &lt;- pmax(\n  1, pmin(5, business_metrics$customer_satisfaction))\nbusiness_metrics$employee_satisfaction &lt;- pmax(\n  1, pmin(5, business_metrics$employee_satisfaction))\n\n\n\n\nCreate correlation matrix in R\ncorrelation_matrix &lt;- cor(business_metrics)\n\n\n\n\nVisualize correlation matrix\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Assuming your correlation matrix is stored in 'correlation_matrix'\n# First, let's understand what we're working with\ncorrelation_visualization &lt;- correlation_matrix %&gt;%\n  # Convert the matrix to a data frame if it isn't already\n  as.data.frame() %&gt;%\n  # Add row names as a column (these are our first variable names)\n  mutate(var1 = rownames(.)) %&gt;%\n  # Reshape from wide to long format\n  pivot_longer(cols = -var1,           \n               names_to = \"var2\", \n               values_to = \"correlation\") %&gt;%\n  # Create a more readable format for variable names\n  mutate(\n    var1_clean = case_when(\n      var1 == \"advertising_spend\" ~ \"Advertising Spend\",\n      var1 == \"customer_satisfaction\" ~ \"Customer Satisfaction\", \n      var1 == \"employee_satisfaction\" ~ \"Employee Satisfaction\",\n      var1 == \"training_hours\" ~ \"Training Hours\",\n      var1 == \"sales_revenue\" ~ \"Sales Revenue\",\n      var1 == \"profit_margin\" ~ \"Profit Margin\",\n      TRUE ~ var1\n    ),\n    var2_clean = case_when(\n      var2 == \"advertising_spend\" ~ \"Advertising Spend\",\n      var2 == \"customer_satisfaction\" ~ \"Customer Satisfaction\",\n      var2 == \"employee_satisfaction\" ~ \"Employee Satisfaction\", \n      var2 == \"training_hours\" ~ \"Training Hours\",\n      var2 == \"sales_revenue\" ~ \"Sales Revenue\",\n      var2 == \"profit_margin\" ~ \"Profit Margin\",\n      TRUE ~ var2\n    )\n  )\n\n# Create the visualization\ncorrelation_heatmap &lt;- correlation_visualization %&gt;%\n  ggplot(aes(x = var2_clean, y = var1_clean, fill = correlation)) +\n  geom_tile(color = \"white\", linewidth = 0.3) +\n  geom_text(aes(label = round(correlation, 2)), \n            color = \"black\", size = 3) +\n  scale_fill_gradient2(low = \"darkred\", mid = \"white\", high = \"darkblue\",\n                       midpoint = 0, limit = c(-1, 1),\n                       name = \"Correlation\\nCoefficient\") +\n  # Rotate x-axis labels for better readability\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.title = element_blank(),\n        panel.grid = element_blank()) +\n  labs(title = \"Business Metrics Correlation Matrix\")"
  },
  {
    "objectID": "content/statrecap/DescriptiveStatistics/index.html#regression",
    "href": "content/statrecap/DescriptiveStatistics/index.html#regression",
    "title": "3: Descriptive Statistics",
    "section": "Regression",
    "text": "Regression\nWhile often associated with predictive modeling, regression also serves as a powerful descriptive tool that goes beyond correlation. Think of regression as drawing the “best-fit line” through scattered data points, providing a mathematical equation that summarizes the relationship between (two or more) variables. The most common form, linear regression, is typically written using standardized notation:\n\\[y=\\beta_0 + \\beta_1 \\boldsymbol{x}_1\\]\nwhere \\(\\beta_0\\) represents the y-intercept and \\(\\beta_1\\) represents the slope coefficient. For multiple regression with several predictors, the formula extends to\n\\[y=\\beta_0 + \\beta_1  \\boldsymbol{x}_1 + \\beta_2  \\boldsymbol{x}_2 ... + \\beta_k \\boldsymbol{x}_k \\]\nwith each \\(\\beta_1\\) coefficient describing how \\(\\boldsymbol{y}\\) changes when the corresponding predictor variable changes by one unit, holding all other variables constant. Unlike correlation, which only indicates strength and direction, regression coefficients provide interpretable measures of how variables actually relate to each other in your data.\nFor instance, when \\(y\\) is total sales in 1000 EUR, \\(\\boldsymbol{x}_2\\) the total distance of your shop to the city centre in km, and \\(\\hat{\\beta}_2=-0.5\\) this means that, according to your model, if your shop was moving away from the city centre by 1 km, this is associated, on average, by reducing sales by 500 EUR.\nWhen examining relationships between variables, a regression line offers both a visual and mathematical summary that complements other measures of association.\n\nExample: Consider a marketing manager analyzing the relationship between monthly advertising spend and sales revenue. The manager collects data from the past 12 months to understand how these variables are related. Figure 2 shows a regression that is based on the equation \\[sales = \\beta_0 + \\beta_1 \\cdot advertising\\] The coefficient \\(\\beta_1\\) (approximately 2.5) provides a clear descriptive measure: in the data, for each additional thousand euros spent on advertising, monthly sales revenue increase on average by about 2500 EUR. The intercept (\\(\\beta_0\\)) of around 50 represents the theoretical sales revenue when advertising spend is zero, though this value often has less practical interpretation.\nThe visualization highlights both the overall trend and how individual data points deviate from it. The dotted lines — representing residuals — show the “misses” between what our regression line predicts (the ‘fitted values’) and the actual sales values. Some months performed better than the regression predicted (points above the line), while others performed worse (points below the line). These patterns themselves are descriptive insights that might prompt further investigation: What happened in months with large positive residuals that might explain their better-than-expected performance?\nAs a descriptive tool, this regression cannot claim that advertising directly causes sales increases or predict future performance. It simply summarizes the historical relationship between these variables in a quantifiable way that goes beyond what correlation alone could tell us. Everything that goes beyond that is in the area of inferential statistics.\n\nas you can see in .\n\n\nCreate data and conduct regression\n# Create a simple business dataset\n\nset.seed(123) \n\n # Monthly ad expenditures in thousands of euros:\nadvertising_spend &lt;- seq(15, 37, by = 2) \n# Sales in thousands with noise:\nsales_revenue &lt;- 50 + 2.5 * advertising_spend + rnorm(12, 0, 5)  \n\n# Combine into a data frame\nmarketing_data &lt;- data.frame(\n  month = month.abb[1:12],\n  advertising_spend = advertising_spend,\n  sales_revenue = sales_revenue\n)\n\n# Run a simple linear regression\nmodel &lt;- lm(sales_revenue ~ advertising_spend, data = marketing_data)\n\n# Extract coefficients for our discussion\nbeta0 &lt;- coef(model)[1]  # Intercept\nbeta1 &lt;- coef(model)[2]  # Slope\n\n# Create data with regression information for plotting\nmarketing_data$fitted &lt;- fitted(model)  # Predicted values from regression\nmarketing_data$residuals &lt;- residuals(model)  # Differences between actual and predicted\n\n\n\n\nR code for the visualization\nggplot(marketing_data, aes(x = advertising_spend, y = sales_revenue)) +\n  geom_point(size = 3, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\") +\n  geom_segment(aes(xend = advertising_spend, yend = fitted), \n               linetype = \"dotted\", color = \"gray30\") +\n  labs(\n    title = \"Advertising Expenditure and Sales Revenue\",\n    subtitle = paste0(\"y = \", round(beta0, 1), \" + \", round(beta1, 2), \"x\"),\n    x = \"Monthly Advertising Spend (thousands €)\",\n    y = \"Monthly Sales Revenue (thousands €)\",\n    caption = \"Dotted lines represent residuals - the difference between\\nactual sales and what the regression line predicts\"\n  ) +\n  annotate(\"text\", x = 30, y = 110, \n           label = \"Residuals show how much\\nactual sales differ from\\nwhat our line predicts\", \n           size = 3, hjust = 0) +\n  annotate(\"curve\", x = 29, y = 110, xend = 27, yend = 120,\n           arrow = arrow(length = unit(0.2, \"cm\")), curvature = -0.3) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(face = \"italic\")\n  )\n\n\n\n\n\n\n\n\nFigure 2: The regression associated with the marketing example."
  }
]